{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd;\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras_tuner as kt\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "import shutil\n",
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>volume</th>\n",
       "      <th>area</th>\n",
       "      <th>n_triangles</th>\n",
       "      <th>x_scale</th>\n",
       "      <th>y_scale</th>\n",
       "      <th>z_scale</th>\n",
       "      <th>area_ratio_xy</th>\n",
       "      <th>area_ratio_xz</th>\n",
       "      <th>area_ratio_yz</th>\n",
       "      <th>build_time</th>\n",
       "      <th>model_material</th>\n",
       "      <th>area_ground</th>\n",
       "      <th>box_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98.344215</td>\n",
       "      <td>420.096211</td>\n",
       "      <td>652</td>\n",
       "      <td>9.144006</td>\n",
       "      <td>0.50800</td>\n",
       "      <td>22.013333</td>\n",
       "      <td>2.398148</td>\n",
       "      <td>1.923505</td>\n",
       "      <td>1.999995</td>\n",
       "      <td>435</td>\n",
       "      <td>109.838</td>\n",
       "      <td>4.645153</td>\n",
       "      <td>102.255309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.726959</td>\n",
       "      <td>26.368342</td>\n",
       "      <td>564</td>\n",
       "      <td>6.032647</td>\n",
       "      <td>1.15000</td>\n",
       "      <td>1.234622</td>\n",
       "      <td>1.216648</td>\n",
       "      <td>1.991790</td>\n",
       "      <td>2.678573</td>\n",
       "      <td>30</td>\n",
       "      <td>5.057</td>\n",
       "      <td>6.937544</td>\n",
       "      <td>8.565246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.315050</td>\n",
       "      <td>11.516920</td>\n",
       "      <td>2454</td>\n",
       "      <td>1.650000</td>\n",
       "      <td>1.65000</td>\n",
       "      <td>1.390008</td>\n",
       "      <td>1.028467</td>\n",
       "      <td>2.060714</td>\n",
       "      <td>2.113780</td>\n",
       "      <td>28</td>\n",
       "      <td>2.563</td>\n",
       "      <td>2.722500</td>\n",
       "      <td>3.784297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.134664</td>\n",
       "      <td>20.899255</td>\n",
       "      <td>1152</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.25849</td>\n",
       "      <td>1.650000</td>\n",
       "      <td>2.081730</td>\n",
       "      <td>1.906478</td>\n",
       "      <td>1.953626</td>\n",
       "      <td>29</td>\n",
       "      <td>5.198</td>\n",
       "      <td>4.516980</td>\n",
       "      <td>7.453017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>927.755856</td>\n",
       "      <td>1253.666592</td>\n",
       "      <td>31880</td>\n",
       "      <td>25.400000</td>\n",
       "      <td>25.40000</td>\n",
       "      <td>8.375590</td>\n",
       "      <td>0.727910</td>\n",
       "      <td>3.029799</td>\n",
       "      <td>3.045351</td>\n",
       "      <td>766</td>\n",
       "      <td>874.206</td>\n",
       "      <td>645.160000</td>\n",
       "      <td>5403.595894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       volume         area  n_triangles    x_scale   y_scale    z_scale  \\\n",
       "0   98.344215   420.096211          652   9.144006   0.50800  22.013333   \n",
       "1    3.726959    26.368342          564   6.032647   1.15000   1.234622   \n",
       "2    1.315050    11.516920         2454   1.650000   1.65000   1.390008   \n",
       "3    4.134664    20.899255         1152   2.000000   2.25849   1.650000   \n",
       "4  927.755856  1253.666592        31880  25.400000  25.40000   8.375590   \n",
       "\n",
       "   area_ratio_xy  area_ratio_xz  area_ratio_yz  build_time  model_material  \\\n",
       "0       2.398148       1.923505       1.999995         435         109.838   \n",
       "1       1.216648       1.991790       2.678573          30           5.057   \n",
       "2       1.028467       2.060714       2.113780          28           2.563   \n",
       "3       2.081730       1.906478       1.953626          29           5.198   \n",
       "4       0.727910       3.029799       3.045351         766         874.206   \n",
       "\n",
       "   area_ground   box_volume  \n",
       "0     4.645153   102.255309  \n",
       "1     6.937544     8.565246  \n",
       "2     2.722500     3.784297  \n",
       "3     4.516980     7.453017  \n",
       "4   645.160000  5403.595894  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read data, drop unnecessary\n",
    "data = pd.read_csv('thingi10k_data.csv', index_col=0)\n",
    "\n",
    "df = data.copy()\n",
    "df.drop(\"source\", axis=1, inplace=True)\n",
    "df.drop(\"model_name\", axis=1, inplace=True)\n",
    "df.drop(\"support_material\", axis=1, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    435\n",
       "1     30\n",
       "2     28\n",
       "3     29\n",
       "4    766\n",
       "Name: build_time, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract build times\n",
    "dfcopy = df.copy()\n",
    "build_times = df[\"build_time\"]\n",
    "df.drop(\"build_time\", axis=1,inplace=True)\n",
    "build_times.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train/test split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(df, build_times, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make pipeline\n",
    "#https://joaquinamatrodrigo.github.io/skforecast/0.5.1/user_guides/sklearn-transformers-and-pipeline.html\n",
    "\n",
    "def log_transform(x):\n",
    "    return np.log(x + 1)\n",
    "\n",
    "def inv_log_transform(x):\n",
    "    return np.exp(x) - 1 \n",
    "\n",
    "\n",
    "logtransformer = FunctionTransformer(func=log_transform, inverse_func=inv_log_transform, check_inverse=False)\n",
    "pipe = Pipeline(steps=[ ('logtransformer', logtransformer)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         volume        area  n_triangles    x_scale    y_scale   z_scale  \\\n",
      "46    16.386163  125.024258         2220   2.634412   2.623847  8.098779   \n",
      "1701  63.348636  433.763828       345996  16.694638   7.911137  4.208379   \n",
      "3423  91.390815  248.459609          868   4.069800  20.499802  1.240000   \n",
      "1699   8.526674   80.814419           36   2.377641   2.261271  5.700000   \n",
      "362   46.657899  179.674844         1752   6.967062   6.947266  4.295972   \n",
      "\n",
      "      area_ratio_xy  area_ratio_xz  area_ratio_yz  model_material  \\\n",
      "46         0.812219       3.573083       3.568259          17.206   \n",
      "1701       1.920012       3.173722       4.602365          65.060   \n",
      "3423       1.922067       2.398057       3.279922          88.089   \n",
      "1699       1.381966       3.504528       3.504528           8.071   \n",
      "362        1.162822       2.795995       2.714557          49.768   \n",
      "\n",
      "      area_ground  box_volume  \n",
      "46       6.912293   55.981136  \n",
      "1701   132.073563  555.815605  \n",
      "3423    83.430097  103.453324  \n",
      "1699     5.376492   30.646004  \n",
      "362     48.402033  207.933763  \n",
      "        volume      area  n_triangles   x_scale   y_scale   z_scale  \\\n",
      "46    2.855675  4.836474     7.705713  1.290447  1.287536  2.208140   \n",
      "1701  4.164316  6.074803    12.754185  2.873262  2.187302  1.650269   \n",
      "3423  4.526028  5.519297     6.767343  1.623301  3.068044  0.806476   \n",
      "1699  2.254096  4.404453     3.610918  1.217178  1.182117  1.902108   \n",
      "362   3.864048  5.196699     7.469084  2.075316  2.072828  1.666946   \n",
      "\n",
      "      area_ratio_xy  area_ratio_xz  area_ratio_yz  model_material  \\\n",
      "46         0.594552       1.520188       1.519132        2.901751   \n",
      "1701       1.071588       1.428808       1.723189        4.190563   \n",
      "3423       1.072291       1.223204       1.453935        4.489636   \n",
      "1699       0.867926       1.505083       1.505083        2.205083   \n",
      "362        0.771414       1.333947       1.312259        3.927266   \n",
      "\n",
      "      area_ground  box_volume  \n",
      "46       2.068418    4.042720  \n",
      "1701     4.890902    6.322234  \n",
      "3423     4.435924    4.648740  \n",
      "1699     1.852618    3.454612  \n",
      "362      3.899992    5.342017  \n"
     ]
    }
   ],
   "source": [
    "#Xdata pipe fit\n",
    "xlabels = xtrain.columns.values\n",
    "\n",
    "xtrain = pipe.fit_transform(xtrain)\n",
    "xtrain = pd.DataFrame(xtrain)\n",
    "xtrain.columns = xlabels\n",
    "\n",
    "print(xtest.head())\n",
    "xtest = pipe.transform(xtest)\n",
    "xtest = pd.DataFrame(xtest)\n",
    "xtest.columns = xlabels\n",
    "print(xtest.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to tensors\n",
    "xtraint = tf.convert_to_tensor(xtrain)\n",
    "ytraint = tf.convert_to_tensor(ytrain)\n",
    "xtestt = tf.convert_to_tensor(xtest)\n",
    "ytestt = tf.convert_to_tensor(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define callbacks\n",
    "def get_callbacks(weights_file, patience, lr_factor):\n",
    "  \n",
    "  return [\n",
    "      # Only save the weights that correspond to the minimum mape.\n",
    "      ModelCheckpoint(filepath= weights_file,\n",
    "                      monitor=\"val_mape\", \n",
    "                      mode=\"min\",\n",
    "                      save_best_only=True, \n",
    "                      save_weights_only=False),\n",
    "      # If val_loss doesn't improve for a number of epochs set with 'patience' var \n",
    "      # training will stop to avoid overfitting.    \n",
    "      EarlyStopping(monitor=\"val_loss\",\n",
    "                    mode=\"min\",\n",
    "                    patience = patience,\n",
    "                    verbose=1),\n",
    "      # Learning rate is reduced by 'lr_factor' if val_loss stagnates\n",
    "      # for a number of epochs set with 'patience/2' var.     \n",
    "      ReduceLROnPlateau(monitor=\"val_loss\", mode=\"min\",\n",
    "                        factor=lr_factor, min_lr=1e-8, patience=patience//2, verbose=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: single_layer_tuning - Het systeem kan het opgegeven pad niet vinden.\n",
      "Search space summary\n",
      "Default search space size: 2\n",
      "units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': None}\n",
      "learning_rate (Choice)\n",
      "{'default': 0.01, 'conditions': [], 'values': [0.01, 0.001, 0.0001], 'ordered': True}\n"
     ]
    }
   ],
   "source": [
    "#Single layer tuner\n",
    "#https://www.analyticsvidhya.com/blog/2021/06/keras-tuner-auto-neural-network-architecture-selection/\n",
    "\n",
    "#Remove old search\n",
    "try:\n",
    "    shutil.rmtree(\"single_layer_tuning\")\n",
    "except OSError as e:\n",
    "    print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "\n",
    "def model_builder(hp):\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(tf.keras.layers.Input(13,))\n",
    "\n",
    "  hp_units = hp.Int(\"units\", min_value=32, max_value=512, step=32)\n",
    "  model.add(tf.keras.layers.Dense(units=hp_units, activation='relu'))\n",
    "  \n",
    "  model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss = 'mae', metrics = ['mape'])\n",
    "  return model\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    model_builder,\n",
    "    objective = 'val_mae',\n",
    "    max_trials = 10,\n",
    "    executions_per_trial = 3,\n",
    "    project_name = \"single_layer_tuning\"\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 09s]\n",
      "val_accuracy: 0.0\n",
      "\n",
      "Best val_accuracy So Far: 0.0\n",
      "Total elapsed time: 00h 01m 37s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "#Search\n",
    "tuner.search(xtraint, ytraint, epochs=5, validation_data=(xtestt, ytestt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in .\\single_layer_tuning\n",
      "Showing 1 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x00000202CAA21780>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 128\n",
      "learning_rate: 0.0001\n",
      "Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "#Results\n",
    "tuner.results_summary(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 3\n",
      "layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 5, 'step': 1, 'sampling': None}\n",
      "units_0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 8, 'max_value': 256, 'step': 8, 'sampling': None}\n",
      "learning_rate (Choice)\n",
      "{'default': 0.01, 'conditions': [], 'values': [0.01, 0.001, 0.0001], 'ordered': True}\n"
     ]
    }
   ],
   "source": [
    "#Multiple layer tuner\n",
    "#https://www.analyticsvidhya.com/blog/2021/06/keras-tuner-auto-neural-network-architecture-selection/\n",
    "#https://pub.towardsai.net/keras-tuner-tutorial-hyperparameter-optimization-tensorflow-keras-computer-vision-example-c9abbdad9887\n",
    "\n",
    "#Remove old search\n",
    "try:\n",
    "    shutil.rmtree(\"multiple_layers_tuning\")\n",
    "except OSError as e:\n",
    "    print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "\n",
    "\n",
    "def model_builder(hp):\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(tf.keras.layers.Input(12,))\n",
    "\n",
    "  hp_layers = hp.Int(\"layers\", min_value=1, max_value=5, step=1)\n",
    "  \n",
    "  for i in range(hp_layers):\n",
    "    model.add(tf.keras.layers.Dense(units=hp.Int(f\"units_{i}\", min_value=8, max_value=256, step=8), activation='relu'))\n",
    "\n",
    "  \n",
    "  model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss = 'mape', metrics = ['mae', 'mape'])\n",
    "  return model\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    model_builder,\n",
    "    objective = 'val_mape',\n",
    "    max_trials = 10,\n",
    "    executions_per_trial = 1,\n",
    "    project_name = \"multiple_layers_tuning\"\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 251 Complete [00h 03m 20s]\n",
      "val_mape: 28.006498336791992\n",
      "\n",
      "Best val_mape So Far: 11.070463180541992\n",
      "Total elapsed time: 01h 25m 15s\n",
      "\n",
      "Search: Running Trial #252\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "4                 |5                 |layers\n",
      "88                |248               |units_0\n",
      "0.0001            |0.001             |learning_rate\n",
      "40                |128               |units_1\n",
      "128               |232               |units_2\n",
      "112               |168               |units_3\n",
      "88                |208               |units_4\n",
      "200               |200               |tuner/epochs\n",
      "0                 |67                |tuner/initial_epoch\n",
      "0                 |2                 |tuner/bracket\n",
      "0                 |2                 |tuner/round\n",
      "\n",
      "Epoch 1/200\n",
      "84/92 [==========================>...] - ETA: 0s - loss: 91.7148 - mae: 263.7819 - mape: 91.7148INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 90.5995 - mae: 254.3487 - mape: 90.5995 - val_loss: 75.0074 - val_mae: 269.6422 - val_mape: 75.0074 - lr: 1.0000e-04\n",
      "Epoch 2/200\n",
      "83/92 [==========================>...] - ETA: 0s - loss: 67.9058 - mae: 245.2862 - mape: 67.9058INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 67.5424 - mae: 244.3501 - mape: 67.5424 - val_loss: 65.0680 - val_mae: 260.8683 - val_mape: 65.0680 - lr: 1.0000e-04\n",
      "Epoch 3/200\n",
      "82/92 [=========================>....] - ETA: 0s - loss: 63.0392 - mae: 243.7198 - mape: 63.0392INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 62.8502 - mae: 239.9983 - mape: 62.8502 - val_loss: 61.7486 - val_mae: 258.6960 - val_mape: 61.7486 - lr: 1.0000e-04\n",
      "Epoch 4/200\n",
      "84/92 [==========================>...] - ETA: 0s - loss: 59.2648 - mae: 232.9029 - mape: 59.2648INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 59.1039 - mae: 237.4833 - mape: 59.1039 - val_loss: 57.4050 - val_mae: 256.3853 - val_mape: 57.4050 - lr: 1.0000e-04\n",
      "Epoch 5/200\n",
      "91/92 [============================>.] - ETA: 0s - loss: 55.2334 - mae: 235.4469 - mape: 55.2334INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 55.1872 - mae: 234.3897 - mape: 55.1872 - val_loss: 54.0031 - val_mae: 252.7497 - val_mape: 54.0031 - lr: 1.0000e-04\n",
      "Epoch 6/200\n",
      "80/92 [=========================>....] - ETA: 0s - loss: 52.7443 - mae: 235.5541 - mape: 52.7443INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 52.7342 - mae: 231.2807 - mape: 52.7342 - val_loss: 52.3246 - val_mae: 249.8293 - val_mape: 52.3246 - lr: 1.0000e-04\n",
      "Epoch 7/200\n",
      "91/92 [============================>.] - ETA: 0s - loss: 51.5737 - mae: 228.5821 - mape: 51.5737INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 51.6082 - mae: 229.1278 - mape: 51.6082 - val_loss: 51.2862 - val_mae: 248.1049 - val_mape: 51.2862 - lr: 1.0000e-04\n",
      "Epoch 8/200\n",
      "85/92 [==========================>...] - ETA: 0s - loss: 50.6209 - mae: 223.7368 - mape: 50.6209INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 50.7677 - mae: 227.6464 - mape: 50.7677 - val_loss: 51.0433 - val_mae: 245.8264 - val_mape: 51.0433 - lr: 1.0000e-04\n",
      "Epoch 9/200\n",
      "86/92 [===========================>..] - ETA: 0s - loss: 50.2313 - mae: 228.3743 - mape: 50.2313INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 50.3227 - mae: 226.1933 - mape: 50.3227 - val_loss: 50.0280 - val_mae: 245.5048 - val_mape: 50.0280 - lr: 1.0000e-04\n",
      "Epoch 10/200\n",
      "83/92 [==========================>...] - ETA: 0s - loss: 50.0353 - mae: 226.1377 - mape: 50.0353INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 49.7123 - mae: 225.0140 - mape: 49.7123 - val_loss: 49.2646 - val_mae: 245.1767 - val_mape: 49.2646 - lr: 1.0000e-04\n",
      "Epoch 11/200\n",
      "89/92 [============================>.] - ETA: 0s - loss: 49.3459 - mae: 221.1059 - mape: 49.3459INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 49.2313 - mae: 224.0843 - mape: 49.2313 - val_loss: 48.8443 - val_mae: 244.3418 - val_mape: 48.8443 - lr: 1.0000e-04\n",
      "Epoch 12/200\n",
      "90/92 [============================>.] - ETA: 0s - loss: 48.7548 - mae: 223.4386 - mape: 48.7548INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 48.8393 - mae: 223.5309 - mape: 48.8393 - val_loss: 48.5367 - val_mae: 242.7332 - val_mape: 48.5367 - lr: 1.0000e-04\n",
      "Epoch 13/200\n",
      "85/92 [==========================>...] - ETA: 0s - loss: 48.5973 - mae: 220.7436 - mape: 48.5973INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 48.4451 - mae: 222.5865 - mape: 48.4451 - val_loss: 48.2187 - val_mae: 241.5296 - val_mape: 48.2187 - lr: 1.0000e-04\n",
      "Epoch 14/200\n",
      "84/92 [==========================>...] - ETA: 0s - loss: 47.9986 - mae: 225.7428 - mape: 47.9986INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 48.0390 - mae: 221.6567 - mape: 48.0390 - val_loss: 47.7384 - val_mae: 241.3676 - val_mape: 47.7384 - lr: 1.0000e-04\n",
      "Epoch 15/200\n",
      "86/92 [===========================>..] - ETA: 0s - loss: 47.8805 - mae: 222.5884 - mape: 47.8805INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 47.8039 - mae: 221.1311 - mape: 47.8039 - val_loss: 47.6546 - val_mae: 239.9962 - val_mape: 47.6546 - lr: 1.0000e-04\n",
      "Epoch 16/200\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 47.4146 - mae: 219.4165 - mape: 47.4146INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 47.4592 - mae: 220.4439 - mape: 47.4592 - val_loss: 47.1341 - val_mae: 240.2498 - val_mape: 47.1341 - lr: 1.0000e-04\n",
      "Epoch 17/200\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 47.3292 - mae: 219.7147 - mape: 47.3292INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 47.2607 - mae: 220.2095 - mape: 47.2607 - val_loss: 47.1330 - val_mae: 238.9944 - val_mape: 47.1330 - lr: 1.0000e-04\n",
      "Epoch 18/200\n",
      "91/92 [============================>.] - ETA: 0s - loss: 46.9716 - mae: 220.4976 - mape: 46.9716INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 46.9898 - mae: 219.5745 - mape: 46.9898 - val_loss: 46.7966 - val_mae: 239.1437 - val_mape: 46.7966 - lr: 1.0000e-04\n",
      "Epoch 19/200\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 46.9180 - mae: 219.0142 - mape: 46.9180INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 46.8739 - mae: 219.3172 - mape: 46.8739 - val_loss: 46.4236 - val_mae: 238.9667 - val_mape: 46.4236 - lr: 1.0000e-04\n",
      "Epoch 20/200\n",
      "91/92 [============================>.] - ETA: 0s - loss: 46.7568 - mae: 220.0161 - mape: 46.7568INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 46.6828 - mae: 218.9908 - mape: 46.6828 - val_loss: 46.2361 - val_mae: 238.7364 - val_mape: 46.2361 - lr: 1.0000e-04\n",
      "Epoch 21/200\n",
      "90/92 [============================>.] - ETA: 0s - loss: 46.4318 - mae: 220.5367 - mape: 46.4318INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 46.4389 - mae: 218.4447 - mape: 46.4389 - val_loss: 46.0134 - val_mae: 238.8483 - val_mape: 46.0134 - lr: 1.0000e-04\n",
      "Epoch 22/200\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 46.2683 - mae: 218.2788 - mape: 46.2683 - val_loss: 46.0226 - val_mae: 237.8339 - val_mape: 46.0226 - lr: 1.0000e-04\n",
      "Epoch 23/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 46.1507 - mae: 217.6457 - mape: 46.1507 - val_loss: 46.1047 - val_mae: 237.0380 - val_mape: 46.1047 - lr: 1.0000e-04\n",
      "Epoch 24/200\n",
      "92/92 [==============================] - ETA: 0s - loss: 46.0396 - mae: 217.3675 - mape: 46.0396INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 46.0396 - mae: 217.3675 - mape: 46.0396 - val_loss: 45.5471 - val_mae: 238.2515 - val_mape: 45.5471 - lr: 1.0000e-04\n",
      "Epoch 25/200\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 45.8241 - mae: 217.1638 - mape: 45.8241 - val_loss: 45.6488 - val_mae: 236.4435 - val_mape: 45.6488 - lr: 1.0000e-04\n",
      "Epoch 26/200\n",
      "82/92 [=========================>....] - ETA: 0s - loss: 45.8992 - mae: 219.2600 - mape: 45.8992INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 45.6610 - mae: 216.5292 - mape: 45.6610 - val_loss: 45.4153 - val_mae: 236.1548 - val_mape: 45.4153 - lr: 1.0000e-04\n",
      "Epoch 27/200\n",
      "82/92 [=========================>....] - ETA: 0s - loss: 45.5756 - mae: 222.8627 - mape: 45.5756INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 45.4346 - mae: 216.1434 - mape: 45.4346 - val_loss: 45.0564 - val_mae: 236.1509 - val_mape: 45.0564 - lr: 1.0000e-04\n",
      "Epoch 28/200\n",
      "89/92 [============================>.] - ETA: 0s - loss: 45.4083 - mae: 218.6286 - mape: 45.4083INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 45.3465 - mae: 215.8344 - mape: 45.3465 - val_loss: 45.0012 - val_mae: 235.8248 - val_mape: 45.0012 - lr: 1.0000e-04\n",
      "Epoch 29/200\n",
      "84/92 [==========================>...] - ETA: 0s - loss: 45.2103 - mae: 210.3793 - mape: 45.2103INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 45.2157 - mae: 215.6289 - mape: 45.2157 - val_loss: 44.9912 - val_mae: 234.7243 - val_mape: 44.9912 - lr: 1.0000e-04\n",
      "Epoch 30/200\n",
      "85/92 [==========================>...] - ETA: 0s - loss: 44.9697 - mae: 215.0496 - mape: 44.9697INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 45.0443 - mae: 214.9264 - mape: 45.0443 - val_loss: 44.7643 - val_mae: 234.8106 - val_mape: 44.7643 - lr: 1.0000e-04\n",
      "Epoch 31/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 44.9164 - mae: 214.5942 - mape: 44.9164 - val_loss: 44.9053 - val_mae: 233.4741 - val_mape: 44.9053 - lr: 1.0000e-04\n",
      "Epoch 32/200\n",
      "86/92 [===========================>..] - ETA: 0s - loss: 44.8648 - mae: 216.8136 - mape: 44.8648INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 44.8228 - mae: 213.9838 - mape: 44.8228 - val_loss: 44.5065 - val_mae: 233.6051 - val_mape: 44.5065 - lr: 1.0000e-04\n",
      "Epoch 33/200\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 44.4566 - mae: 214.3305 - mape: 44.4566INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 44.6119 - mae: 213.8053 - mape: 44.6119 - val_loss: 44.4927 - val_mae: 233.4329 - val_mape: 44.4927 - lr: 1.0000e-04\n",
      "Epoch 34/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 44.6330 - mae: 213.5090 - mape: 44.6330 - val_loss: 44.6784 - val_mae: 232.1238 - val_mape: 44.6784 - lr: 1.0000e-04\n",
      "Epoch 35/200\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 44.2841 - mae: 208.3695 - mape: 44.2841INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 44.4149 - mae: 212.5730 - mape: 44.4149 - val_loss: 44.1194 - val_mae: 232.7997 - val_mape: 44.1194 - lr: 1.0000e-04\n",
      "Epoch 36/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 44.1965 - mae: 212.4410 - mape: 44.1965 - val_loss: 44.2320 - val_mae: 231.2807 - val_mape: 44.2320 - lr: 1.0000e-04\n",
      "Epoch 37/200\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 44.4048 - mae: 215.8445 - mape: 44.4048INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 44.2619 - mae: 211.8086 - mape: 44.2619 - val_loss: 44.0283 - val_mae: 232.9226 - val_mape: 44.0283 - lr: 1.0000e-04\n",
      "Epoch 38/200\n",
      "86/92 [===========================>..] - ETA: 0s - loss: 44.1658 - mae: 213.6890 - mape: 44.1658INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 43.9919 - mae: 211.6916 - mape: 43.9919 - val_loss: 43.8042 - val_mae: 231.3033 - val_mape: 43.8042 - lr: 1.0000e-04\n",
      "Epoch 39/200\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 43.8474 - mae: 211.4515 - mape: 43.8474 - val_loss: 44.0750 - val_mae: 229.9197 - val_mape: 44.0750 - lr: 1.0000e-04\n",
      "Epoch 40/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 43.6501 - mae: 211.1456 - mape: 43.6501 - val_loss: 43.8432 - val_mae: 229.9900 - val_mape: 43.8432 - lr: 1.0000e-04\n",
      "Epoch 41/200\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 43.5439 - mae: 213.5074 - mape: 43.5439INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 20ms/step - loss: 43.5577 - mae: 210.4916 - mape: 43.5577 - val_loss: 43.6404 - val_mae: 230.4003 - val_mape: 43.6404 - lr: 1.0000e-04\n",
      "Epoch 42/200\n",
      "86/92 [===========================>..] - ETA: 0s - loss: 43.5621 - mae: 214.3180 - mape: 43.5621INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 43.4603 - mae: 210.4302 - mape: 43.4603 - val_loss: 43.5157 - val_mae: 229.2446 - val_mape: 43.5157 - lr: 1.0000e-04\n",
      "Epoch 43/200\n",
      "90/92 [============================>.] - ETA: 0s - loss: 43.4838 - mae: 212.1676 - mape: 43.4838INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 43.3917 - mae: 209.5957 - mape: 43.3917 - val_loss: 43.4013 - val_mae: 229.5738 - val_mape: 43.4013 - lr: 1.0000e-04\n",
      "Epoch 44/200\n",
      "84/92 [==========================>...] - ETA: 0s - loss: 43.4596 - mae: 205.9773 - mape: 43.4596INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 43.2411 - mae: 209.5807 - mape: 43.2411 - val_loss: 43.4010 - val_mae: 228.2811 - val_mape: 43.4010 - lr: 1.0000e-04\n",
      "Epoch 45/200\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 43.0701 - mae: 214.1188 - mape: 43.0701INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 43.0726 - mae: 208.8761 - mape: 43.0726 - val_loss: 43.2358 - val_mae: 228.9594 - val_mape: 43.2358 - lr: 1.0000e-04\n",
      "Epoch 46/200\n",
      "85/92 [==========================>...] - ETA: 0s - loss: 43.1317 - mae: 211.6539 - mape: 43.1317INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 43.0011 - mae: 208.5891 - mape: 43.0011 - val_loss: 43.1899 - val_mae: 228.2050 - val_mape: 43.1899 - lr: 1.0000e-04\n",
      "Epoch 47/200\n",
      "90/92 [============================>.] - ETA: 0s - loss: 42.9018 - mae: 208.7468 - mape: 42.9018INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 42.9029 - mae: 208.5305 - mape: 42.9029 - val_loss: 42.9976 - val_mae: 227.7689 - val_mape: 42.9976 - lr: 1.0000e-04\n",
      "Epoch 48/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 42.7335 - mae: 207.4049 - mape: 42.7335 - val_loss: 43.0013 - val_mae: 227.3895 - val_mape: 43.0013 - lr: 1.0000e-04\n",
      "Epoch 49/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 42.6379 - mae: 207.4791 - mape: 42.6379 - val_loss: 43.0106 - val_mae: 226.4374 - val_mape: 43.0106 - lr: 1.0000e-04\n",
      "Epoch 50/200\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 42.4533 - mae: 205.8079 - mape: 42.4533INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 42.4129 - mae: 206.8804 - mape: 42.4129 - val_loss: 42.9132 - val_mae: 225.6300 - val_mape: 42.9132 - lr: 1.0000e-04\n",
      "Epoch 51/200\n",
      "83/92 [==========================>...] - ETA: 0s - loss: 42.4257 - mae: 206.6664 - mape: 42.4257INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 42.5285 - mae: 206.2496 - mape: 42.5285 - val_loss: 42.7872 - val_mae: 226.9552 - val_mape: 42.7872 - lr: 1.0000e-04\n",
      "Epoch 52/200\n",
      "82/92 [=========================>....] - ETA: 0s - loss: 42.3954 - mae: 208.5152 - mape: 42.3954INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 42.2121 - mae: 206.2442 - mape: 42.2121 - val_loss: 42.7056 - val_mae: 224.8539 - val_mape: 42.7056 - lr: 1.0000e-04\n",
      "Epoch 53/200\n",
      "83/92 [==========================>...] - ETA: 0s - loss: 42.0995 - mae: 201.6490 - mape: 42.0995INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 42.1472 - mae: 205.5338 - mape: 42.1472 - val_loss: 42.6071 - val_mae: 224.9037 - val_mape: 42.6071 - lr: 1.0000e-04\n",
      "Epoch 54/200\n",
      "89/92 [============================>.] - ETA: 0s - loss: 42.0854 - mae: 208.4677 - mape: 42.0854INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 42.0108 - mae: 205.3358 - mape: 42.0108 - val_loss: 42.4858 - val_mae: 224.8779 - val_mape: 42.4858 - lr: 1.0000e-04\n",
      "Epoch 55/200\n",
      "85/92 [==========================>...] - ETA: 0s - loss: 42.2262 - mae: 213.1375 - mape: 42.2262INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 41.9482 - mae: 205.0385 - mape: 41.9482 - val_loss: 42.4804 - val_mae: 224.5557 - val_mape: 42.4804 - lr: 1.0000e-04\n",
      "Epoch 56/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 41.9159 - mae: 204.2700 - mape: 41.9159 - val_loss: 42.9003 - val_mae: 223.5381 - val_mape: 42.9003 - lr: 1.0000e-04\n",
      "Epoch 57/200\n",
      "85/92 [==========================>...] - ETA: 0s - loss: 42.1424 - mae: 209.7284 - mape: 42.1424INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 41.9035 - mae: 204.1234 - mape: 41.9035 - val_loss: 42.1881 - val_mae: 224.8696 - val_mape: 42.1881 - lr: 1.0000e-04\n",
      "Epoch 58/200\n",
      "89/92 [============================>.] - ETA: 0s - loss: 41.6429 - mae: 196.5326 - mape: 41.6429INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 41.6966 - mae: 203.7518 - mape: 41.6966 - val_loss: 42.1490 - val_mae: 224.7986 - val_mape: 42.1490 - lr: 1.0000e-04\n",
      "Epoch 59/200\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 41.4564 - mae: 196.7578 - mape: 41.4564INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 41.6207 - mae: 203.2597 - mape: 41.6207 - val_loss: 42.1165 - val_mae: 224.3743 - val_mape: 42.1165 - lr: 1.0000e-04\n",
      "Epoch 60/200\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 41.6011 - mae: 203.4169 - mape: 41.6011 - val_loss: 42.4210 - val_mae: 221.0379 - val_mape: 42.4210 - lr: 1.0000e-04\n",
      "Epoch 61/200\n",
      "81/92 [=========================>....] - ETA: 0s - loss: 41.0790 - mae: 205.5592 - mape: 41.0790INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 41.3086 - mae: 201.7945 - mape: 41.3086 - val_loss: 41.9874 - val_mae: 222.8000 - val_mape: 41.9874 - lr: 1.0000e-04\n",
      "Epoch 62/200\n",
      "81/92 [=========================>....] - ETA: 0s - loss: 40.9490 - mae: 207.8000 - mape: 40.9490INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 41.2800 - mae: 202.2328 - mape: 41.2800 - val_loss: 41.8913 - val_mae: 222.9176 - val_mape: 41.8913 - lr: 1.0000e-04\n",
      "Epoch 63/200\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 41.0674 - mae: 201.9782 - mape: 41.0674 - val_loss: 42.1512 - val_mae: 222.5365 - val_mape: 42.1512 - lr: 1.0000e-04\n",
      "Epoch 64/200\n",
      "82/92 [=========================>....] - ETA: 0s - loss: 40.9826 - mae: 199.6917 - mape: 40.9826INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 41.0230 - mae: 201.7580 - mape: 41.0230 - val_loss: 41.7715 - val_mae: 221.1081 - val_mape: 41.7715 - lr: 1.0000e-04\n",
      "Epoch 65/200\n",
      "90/92 [============================>.] - ETA: 0s - loss: 40.8334 - mae: 202.1191 - mape: 40.8334INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 40.8711 - mae: 201.1783 - mape: 40.8711 - val_loss: 41.4563 - val_mae: 221.9708 - val_mape: 41.4563 - lr: 1.0000e-04\n",
      "Epoch 66/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 40.8080 - mae: 200.8560 - mape: 40.8080 - val_loss: 41.5315 - val_mae: 219.8332 - val_mape: 41.5315 - lr: 1.0000e-04\n",
      "Epoch 67/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 40.6986 - mae: 200.2179 - mape: 40.6986 - val_loss: 41.6716 - val_mae: 218.7655 - val_mape: 41.6716 - lr: 1.0000e-04\n",
      "Epoch 68/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 40.5800 - mae: 200.4414 - mape: 40.5800 - val_loss: 41.6741 - val_mae: 218.2806 - val_mape: 41.6741 - lr: 1.0000e-04\n",
      "Epoch 69/200\n",
      "81/92 [=========================>....] - ETA: 0s - loss: 40.3507 - mae: 195.5561 - mape: 40.3507INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 20ms/step - loss: 40.4868 - mae: 199.0249 - mape: 40.4868 - val_loss: 41.1348 - val_mae: 220.4723 - val_mape: 41.1348 - lr: 1.0000e-04\n",
      "Epoch 70/200\n",
      "90/92 [============================>.] - ETA: 0s - loss: 40.5236 - mae: 197.4541 - mape: 40.5236INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 40.4859 - mae: 199.1672 - mape: 40.4859 - val_loss: 41.0185 - val_mae: 218.3407 - val_mape: 41.0185 - lr: 1.0000e-04\n",
      "Epoch 71/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 40.2229 - mae: 199.0762 - mape: 40.2229 - val_loss: 41.1242 - val_mae: 217.4353 - val_mape: 41.1242 - lr: 1.0000e-04\n",
      "Epoch 72/200\n",
      "82/92 [=========================>....] - ETA: 0s - loss: 40.1323 - mae: 203.4400 - mape: 40.1323INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 40.1388 - mae: 198.6013 - mape: 40.1388 - val_loss: 40.7528 - val_mae: 218.2279 - val_mape: 40.7528 - lr: 1.0000e-04\n",
      "Epoch 73/200\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 39.9874 - mae: 193.3846 - mape: 39.9874INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 40.1182 - mae: 198.0146 - mape: 40.1182 - val_loss: 40.5676 - val_mae: 217.7280 - val_mape: 40.5676 - lr: 1.0000e-04\n",
      "Epoch 74/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 40.0008 - mae: 197.8311 - mape: 40.0008 - val_loss: 40.7583 - val_mae: 216.7990 - val_mape: 40.7583 - lr: 1.0000e-04\n",
      "Epoch 75/200\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 39.5799 - mae: 198.3256 - mape: 39.5799INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 39.7447 - mae: 196.8119 - mape: 39.7447 - val_loss: 40.4283 - val_mae: 216.8984 - val_mape: 40.4283 - lr: 1.0000e-04\n",
      "Epoch 76/200\n",
      "90/92 [============================>.] - ETA: 0s - loss: 39.7659 - mae: 199.3621 - mape: 39.7659INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 39.6480 - mae: 196.9033 - mape: 39.6480 - val_loss: 40.2966 - val_mae: 216.0375 - val_mape: 40.2966 - lr: 1.0000e-04\n",
      "Epoch 77/200\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 39.3367 - mae: 196.4011 - mape: 39.3367 - val_loss: 40.3523 - val_mae: 214.2464 - val_mape: 40.3523 - lr: 1.0000e-04\n",
      "Epoch 78/200\n",
      "86/92 [===========================>..] - ETA: 0s - loss: 39.6403 - mae: 200.8394 - mape: 39.6403INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 39.4936 - mae: 195.0859 - mape: 39.4936 - val_loss: 39.8556 - val_mae: 215.5225 - val_mape: 39.8556 - lr: 1.0000e-04\n",
      "Epoch 79/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 39.2335 - mae: 195.4739 - mape: 39.2335 - val_loss: 40.1540 - val_mae: 213.5918 - val_mape: 40.1540 - lr: 1.0000e-04\n",
      "Epoch 80/200\n",
      "85/92 [==========================>...] - ETA: 0s - loss: 38.9031 - mae: 194.6810 - mape: 38.9031INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 39.0057 - mae: 194.9264 - mape: 39.0057 - val_loss: 39.6986 - val_mae: 215.9336 - val_mape: 39.6986 - lr: 1.0000e-04\n",
      "Epoch 81/200\n",
      "86/92 [===========================>..] - ETA: 0s - loss: 39.1710 - mae: 196.9779 - mape: 39.1710INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 38.9125 - mae: 194.7180 - mape: 38.9125 - val_loss: 39.6050 - val_mae: 213.8049 - val_mape: 39.6050 - lr: 1.0000e-04\n",
      "Epoch 82/200\n",
      "90/92 [============================>.] - ETA: 0s - loss: 38.9168 - mae: 195.9119 - mape: 38.9168INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 38.8395 - mae: 193.8203 - mape: 38.8395 - val_loss: 39.5426 - val_mae: 213.9178 - val_mape: 39.5426 - lr: 1.0000e-04\n",
      "Epoch 83/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 38.7293 - mae: 193.3904 - mape: 38.7293 - val_loss: 39.7076 - val_mae: 214.2290 - val_mape: 39.7076 - lr: 1.0000e-04\n",
      "Epoch 84/200\n",
      "92/92 [==============================] - ETA: 0s - loss: 38.6999 - mae: 193.0431 - mape: 38.6999INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 38.6999 - mae: 193.0431 - mape: 38.6999 - val_loss: 39.4591 - val_mae: 212.1891 - val_mape: 39.4591 - lr: 1.0000e-04\n",
      "Epoch 85/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 38.2806 - mae: 192.6195 - mape: 38.2806 - val_loss: 39.6254 - val_mae: 210.9147 - val_mape: 39.6254 - lr: 1.0000e-04\n",
      "Epoch 86/200\n",
      "86/92 [===========================>..] - ETA: 0s - loss: 38.0002 - mae: 187.2351 - mape: 38.0002INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 38.0711 - mae: 192.4361 - mape: 38.0711 - val_loss: 39.3154 - val_mae: 209.7453 - val_mape: 39.3154 - lr: 1.0000e-04\n",
      "Epoch 87/200\n",
      "85/92 [==========================>...] - ETA: 0s - loss: 37.9358 - mae: 193.7259 - mape: 37.9358INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 37.9982 - mae: 190.9782 - mape: 37.9982 - val_loss: 38.9595 - val_mae: 209.8335 - val_mape: 38.9595 - lr: 1.0000e-04\n",
      "Epoch 88/200\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 37.8461 - mae: 194.4142 - mape: 37.8461INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 37.9212 - mae: 191.4969 - mape: 37.9212 - val_loss: 38.6017 - val_mae: 211.3721 - val_mape: 38.6017 - lr: 1.0000e-04\n",
      "Epoch 89/200\n",
      "85/92 [==========================>...] - ETA: 0s - loss: 37.8302 - mae: 195.5559 - mape: 37.8302INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 37.7139 - mae: 190.1888 - mape: 37.7139 - val_loss: 38.5833 - val_mae: 209.2116 - val_mape: 38.5833 - lr: 1.0000e-04\n",
      "Epoch 90/200\n",
      "83/92 [==========================>...] - ETA: 0s - loss: 37.6637 - mae: 183.1430 - mape: 37.6637INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 37.6364 - mae: 189.9897 - mape: 37.6364 - val_loss: 38.2346 - val_mae: 209.5632 - val_mape: 38.2346 - lr: 1.0000e-04\n",
      "Epoch 91/200\n",
      "85/92 [==========================>...] - ETA: 0s - loss: 37.3106 - mae: 192.2076 - mape: 37.3106INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 37.3357 - mae: 189.6360 - mape: 37.3357 - val_loss: 38.1506 - val_mae: 208.6654 - val_mape: 38.1506 - lr: 1.0000e-04\n",
      "Epoch 92/200\n",
      "90/92 [============================>.] - ETA: 0s - loss: 37.2077 - mae: 190.6539 - mape: 37.2077INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 16ms/step - loss: 37.2124 - mae: 188.6328 - mape: 37.2124 - val_loss: 38.0476 - val_mae: 207.6520 - val_mape: 38.0476 - lr: 1.0000e-04\n",
      "Epoch 93/200\n",
      "85/92 [==========================>...] - ETA: 0s - loss: 37.0587 - mae: 195.3094 - mape: 37.0587INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 36.8413 - mae: 188.4287 - mape: 36.8413 - val_loss: 38.0105 - val_mae: 207.5425 - val_mape: 38.0105 - lr: 1.0000e-04\n",
      "Epoch 94/200\n",
      "92/92 [==============================] - ETA: 0s - loss: 36.7885 - mae: 187.8066 - mape: 36.7885INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 16ms/step - loss: 36.7885 - mae: 187.8066 - mape: 36.7885 - val_loss: 37.7701 - val_mae: 206.5996 - val_mape: 37.7701 - lr: 1.0000e-04\n",
      "Epoch 95/200\n",
      "83/92 [==========================>...] - ETA: 0s - loss: 36.6331 - mae: 190.4742 - mape: 36.6331INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 36.5449 - mae: 187.2770 - mape: 36.5449 - val_loss: 37.6501 - val_mae: 206.9266 - val_mape: 37.6501 - lr: 1.0000e-04\n",
      "Epoch 96/200\n",
      "85/92 [==========================>...] - ETA: 0s - loss: 36.3752 - mae: 187.1349 - mape: 36.3752INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 36.3877 - mae: 186.8549 - mape: 36.3877 - val_loss: 37.3796 - val_mae: 205.9633 - val_mape: 37.3796 - lr: 1.0000e-04\n",
      "Epoch 97/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 36.2115 - mae: 185.7955 - mape: 36.2115 - val_loss: 38.2571 - val_mae: 205.3835 - val_mape: 38.2571 - lr: 1.0000e-04\n",
      "Epoch 98/200\n",
      "86/92 [===========================>..] - ETA: 0s - loss: 35.9896 - mae: 178.7827 - mape: 35.9896INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 36.1096 - mae: 185.2850 - mape: 36.1096 - val_loss: 37.0078 - val_mae: 206.4829 - val_mape: 37.0078 - lr: 1.0000e-04\n",
      "Epoch 99/200\n",
      "86/92 [===========================>..] - ETA: 0s - loss: 35.9453 - mae: 182.1342 - mape: 35.9453INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 35.9496 - mae: 184.8823 - mape: 35.9496 - val_loss: 36.8784 - val_mae: 204.0326 - val_mape: 36.8784 - lr: 1.0000e-04\n",
      "Epoch 100/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 35.6374 - mae: 184.2274 - mape: 35.6374 - val_loss: 37.9764 - val_mae: 202.1953 - val_mape: 37.9764 - lr: 1.0000e-04\n",
      "Epoch 101/200\n",
      "85/92 [==========================>...] - ETA: 0s - loss: 35.7652 - mae: 174.5774 - mape: 35.7652INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 35.6468 - mae: 183.2462 - mape: 35.6468 - val_loss: 36.7169 - val_mae: 201.5786 - val_mape: 36.7169 - lr: 1.0000e-04\n",
      "Epoch 102/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 35.2146 - mae: 183.2119 - mape: 35.2146 - val_loss: 36.8783 - val_mae: 200.3940 - val_mape: 36.8783 - lr: 1.0000e-04\n",
      "Epoch 103/200\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 35.2631 - mae: 183.9740 - mape: 35.2631INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 35.0412 - mae: 182.0668 - mape: 35.0412 - val_loss: 36.4033 - val_mae: 201.7407 - val_mape: 36.4033 - lr: 1.0000e-04\n",
      "Epoch 104/200\n",
      "84/92 [==========================>...] - ETA: 0s - loss: 34.7455 - mae: 187.3043 - mape: 34.7455INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 34.8469 - mae: 181.9661 - mape: 34.8469 - val_loss: 36.2918 - val_mae: 199.3640 - val_mape: 36.2918 - lr: 1.0000e-04\n",
      "Epoch 105/200\n",
      "86/92 [===========================>..] - ETA: 0s - loss: 34.7222 - mae: 180.0014 - mape: 34.7222INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 34.6466 - mae: 180.5495 - mape: 34.6466 - val_loss: 35.7863 - val_mae: 200.3398 - val_mape: 35.7863 - lr: 1.0000e-04\n",
      "Epoch 106/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 34.6968 - mae: 180.2169 - mape: 34.6968 - val_loss: 36.1007 - val_mae: 198.1906 - val_mape: 36.1007 - lr: 1.0000e-04\n",
      "Epoch 107/200\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 34.8456 - mae: 180.5346 - mape: 34.8456INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 34.7930 - mae: 180.3964 - mape: 34.7930 - val_loss: 35.4996 - val_mae: 199.3325 - val_mape: 35.4996 - lr: 1.0000e-04\n",
      "Epoch 108/200\n",
      "91/92 [============================>.] - ETA: 0s - loss: 34.0659 - mae: 180.3125 - mape: 34.0659INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 34.0445 - mae: 179.4658 - mape: 34.0445 - val_loss: 35.3667 - val_mae: 197.2180 - val_mape: 35.3667 - lr: 1.0000e-04\n",
      "Epoch 109/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 33.7857 - mae: 179.0917 - mape: 33.7857 - val_loss: 35.8446 - val_mae: 194.6987 - val_mape: 35.8446 - lr: 1.0000e-04\n",
      "Epoch 110/200\n",
      "81/92 [=========================>....] - ETA: 0s - loss: 33.8772 - mae: 170.2721 - mape: 33.8772INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 33.7071 - mae: 177.5488 - mape: 33.7071 - val_loss: 35.1218 - val_mae: 196.1039 - val_mape: 35.1218 - lr: 1.0000e-04\n",
      "Epoch 111/200\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 33.5901 - mae: 181.4060 - mape: 33.5901INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 33.6081 - mae: 177.2238 - mape: 33.6081 - val_loss: 34.8345 - val_mae: 196.8199 - val_mape: 34.8345 - lr: 1.0000e-04\n",
      "Epoch 112/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 33.2731 - mae: 177.1446 - mape: 33.2731 - val_loss: 34.9514 - val_mae: 194.5363 - val_mape: 34.9514 - lr: 1.0000e-04\n",
      "Epoch 113/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 33.1826 - mae: 176.8362 - mape: 33.1826 - val_loss: 35.2818 - val_mae: 191.7573 - val_mape: 35.2818 - lr: 1.0000e-04\n",
      "Epoch 114/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 33.0188 - mae: 175.8038 - mape: 33.0188 - val_loss: 34.9849 - val_mae: 192.8114 - val_mape: 34.9849 - lr: 1.0000e-04\n",
      "Epoch 115/200\n",
      "84/92 [==========================>...] - ETA: 0s - loss: 32.9557 - mae: 173.6108 - mape: 32.9557INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 32.7488 - mae: 175.0558 - mape: 32.7488 - val_loss: 34.3005 - val_mae: 193.1209 - val_mape: 34.3005 - lr: 1.0000e-04\n",
      "Epoch 116/200\n",
      "85/92 [==========================>...] - ETA: 0s - loss: 33.0957 - mae: 179.4990 - mape: 33.0957INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 33.0313 - mae: 174.6779 - mape: 33.0313 - val_loss: 34.0386 - val_mae: 191.9870 - val_mape: 34.0386 - lr: 1.0000e-04\n",
      "Epoch 117/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 32.5338 - mae: 174.0692 - mape: 32.5338 - val_loss: 34.5059 - val_mae: 190.7889 - val_mape: 34.5059 - lr: 1.0000e-04\n",
      "Epoch 118/200\n",
      "86/92 [===========================>..] - ETA: 0s - loss: 32.0943 - mae: 170.5979 - mape: 32.0943INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 32.1890 - mae: 172.9153 - mape: 32.1890 - val_loss: 33.7382 - val_mae: 193.2949 - val_mape: 33.7382 - lr: 1.0000e-04\n",
      "Epoch 119/200\n",
      "89/92 [============================>.] - ETA: 0s - loss: 32.0638 - mae: 172.5372 - mape: 32.0638INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 32.1319 - mae: 173.0646 - mape: 32.1319 - val_loss: 33.7066 - val_mae: 190.6319 - val_mape: 33.7066 - lr: 1.0000e-04\n",
      "Epoch 120/200\n",
      "81/92 [=========================>....] - ETA: 0s - loss: 32.0452 - mae: 171.2499 - mape: 32.0452INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 32.2232 - mae: 171.8177 - mape: 32.2232 - val_loss: 33.4653 - val_mae: 190.1760 - val_mape: 33.4653 - lr: 1.0000e-04\n",
      "Epoch 121/200\n",
      "80/92 [=========================>....] - ETA: 0s - loss: 31.9444 - mae: 163.1045 - mape: 31.9444INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 32.0399 - mae: 171.7732 - mape: 32.0399 - val_loss: 33.0833 - val_mae: 189.6966 - val_mape: 33.0833 - lr: 1.0000e-04\n",
      "Epoch 122/200\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 31.7425 - mae: 170.8949 - mape: 31.7425 - val_loss: 33.2411 - val_mae: 188.7298 - val_mape: 33.2411 - lr: 1.0000e-04\n",
      "Epoch 123/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 31.5081 - mae: 170.1365 - mape: 31.5081 - val_loss: 33.1369 - val_mae: 188.3011 - val_mape: 33.1369 - lr: 1.0000e-04\n",
      "Epoch 124/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 31.5176 - mae: 169.9924 - mape: 31.5176 - val_loss: 33.1753 - val_mae: 186.7167 - val_mape: 33.1753 - lr: 1.0000e-04\n",
      "Epoch 125/200\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 31.3534 - mae: 170.2392 - mape: 31.3534INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 31.3194 - mae: 169.8000 - mape: 31.3194 - val_loss: 32.6841 - val_mae: 186.6352 - val_mape: 32.6841 - lr: 1.0000e-04\n",
      "Epoch 126/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 31.2528 - mae: 168.9337 - mape: 31.2528 - val_loss: 33.2101 - val_mae: 185.0079 - val_mape: 33.2101 - lr: 1.0000e-04\n",
      "Epoch 127/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 30.9486 - mae: 168.2329 - mape: 30.9486 - val_loss: 34.2715 - val_mae: 185.7424 - val_mape: 34.2715 - lr: 1.0000e-04\n",
      "Epoch 128/200\n",
      "84/92 [==========================>...] - ETA: 0s - loss: 31.2690 - mae: 167.5091 - mape: 31.2690INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 31.1151 - mae: 168.8348 - mape: 31.1151 - val_loss: 32.2757 - val_mae: 185.8742 - val_mape: 32.2757 - lr: 1.0000e-04\n",
      "Epoch 129/200\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 30.9738 - mae: 171.6969 - mape: 30.9738INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 30.9048 - mae: 167.7103 - mape: 30.9048 - val_loss: 32.0477 - val_mae: 184.2734 - val_mape: 32.0477 - lr: 1.0000e-04\n",
      "Epoch 130/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 30.8405 - mae: 167.4512 - mape: 30.8405 - val_loss: 32.4871 - val_mae: 184.0589 - val_mape: 32.4871 - lr: 1.0000e-04\n",
      "Epoch 131/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 30.6357 - mae: 167.1963 - mape: 30.6357 - val_loss: 32.1963 - val_mae: 185.7711 - val_mape: 32.1963 - lr: 1.0000e-04\n",
      "Epoch 132/200\n",
      "82/92 [=========================>....] - ETA: 0s - loss: 30.7107 - mae: 170.5383 - mape: 30.7107INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 30.5667 - mae: 166.7158 - mape: 30.5667 - val_loss: 32.0397 - val_mae: 181.3554 - val_mape: 32.0397 - lr: 1.0000e-04\n",
      "Epoch 133/200\n",
      "83/92 [==========================>...] - ETA: 0s - loss: 30.1886 - mae: 168.0558 - mape: 30.1886INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 30.2429 - mae: 164.9234 - mape: 30.2429 - val_loss: 31.5777 - val_mae: 184.9471 - val_mape: 31.5777 - lr: 1.0000e-04\n",
      "Epoch 134/200\n",
      "84/92 [==========================>...] - ETA: 0s - loss: 30.3649 - mae: 168.6009 - mape: 30.3649INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 30.2350 - mae: 166.0660 - mape: 30.2350 - val_loss: 31.5720 - val_mae: 182.4075 - val_mape: 31.5720 - lr: 1.0000e-04\n",
      "Epoch 135/200\n",
      "89/92 [============================>.] - ETA: 0s - loss: 29.9030 - mae: 162.5550 - mape: 29.9030INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 29.9294 - mae: 164.8779 - mape: 29.9294 - val_loss: 31.5517 - val_mae: 180.7191 - val_mape: 31.5517 - lr: 1.0000e-04\n",
      "Epoch 136/200\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 29.6295 - mae: 162.3440 - mape: 29.6295INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 29.8102 - mae: 164.4020 - mape: 29.8102 - val_loss: 31.4524 - val_mae: 179.6733 - val_mape: 31.4524 - lr: 1.0000e-04\n",
      "Epoch 137/200\n",
      "86/92 [===========================>..] - ETA: 0s - loss: 29.7016 - mae: 169.5741 - mape: 29.7016INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 29.7017 - mae: 163.8844 - mape: 29.7017 - val_loss: 31.0519 - val_mae: 182.0514 - val_mape: 31.0519 - lr: 1.0000e-04\n",
      "Epoch 138/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 29.6781 - mae: 163.4395 - mape: 29.6781 - val_loss: 31.1500 - val_mae: 179.9303 - val_mape: 31.1500 - lr: 1.0000e-04\n",
      "Epoch 139/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 29.3939 - mae: 163.2135 - mape: 29.3939 - val_loss: 31.3711 - val_mae: 176.4969 - val_mape: 31.3711 - lr: 1.0000e-04\n",
      "Epoch 140/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 29.5616 - mae: 162.0963 - mape: 29.5616 - val_loss: 31.1386 - val_mae: 179.5737 - val_mape: 31.1386 - lr: 1.0000e-04\n",
      "Epoch 141/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 29.1137 - mae: 161.4308 - mape: 29.1137 - val_loss: 31.2248 - val_mae: 177.4347 - val_mape: 31.2248 - lr: 1.0000e-04\n",
      "Epoch 142/200\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 29.1757 - mae: 162.1933 - mape: 29.1757INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 29.1479 - mae: 162.1835 - mape: 29.1479 - val_loss: 30.7704 - val_mae: 175.8433 - val_mape: 30.7704 - lr: 1.0000e-04\n",
      "Epoch 143/200\n",
      "85/92 [==========================>...] - ETA: 0s - loss: 29.0535 - mae: 158.3137 - mape: 29.0535INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 29.1804 - mae: 160.7930 - mape: 29.1804 - val_loss: 30.3285 - val_mae: 177.5409 - val_mape: 30.3285 - lr: 1.0000e-04\n",
      "Epoch 144/200\n",
      "92/92 [==============================] - 1s 5ms/step - loss: 28.9159 - mae: 160.6049 - mape: 28.9159 - val_loss: 30.3579 - val_mae: 174.8409 - val_mape: 30.3579 - lr: 1.0000e-04\n",
      "Epoch 145/200\n",
      "90/92 [============================>.] - ETA: 0s - loss: 28.9715 - mae: 161.9091 - mape: 28.9715INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 28.8872 - mae: 160.0937 - mape: 28.8872 - val_loss: 30.2751 - val_mae: 173.6712 - val_mape: 30.2751 - lr: 1.0000e-04\n",
      "Epoch 146/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 28.9868 - mae: 159.0957 - mape: 28.9868 - val_loss: 31.2425 - val_mae: 174.6872 - val_mape: 31.2425 - lr: 1.0000e-04\n",
      "Epoch 147/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 28.7996 - mae: 158.9545 - mape: 28.7996 - val_loss: 31.5183 - val_mae: 170.7982 - val_mape: 31.5183 - lr: 1.0000e-04\n",
      "Epoch 148/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 28.6188 - mae: 158.0220 - mape: 28.6188 - val_loss: 30.5282 - val_mae: 171.6435 - val_mape: 30.5282 - lr: 1.0000e-04\n",
      "Epoch 149/200\n",
      "82/92 [=========================>....] - ETA: 0s - loss: 28.4566 - mae: 156.7052 - mape: 28.4566INFO:tensorflow:Assets written to: baseline_nosupp_tuner\\assets\n",
      "92/92 [==============================] - 2s 20ms/step - loss: 28.5890 - mae: 157.2063 - mape: 28.5890 - val_loss: 30.1568 - val_mae: 172.6817 - val_mape: 30.1568 - lr: 1.0000e-04\n",
      "Epoch 150/200\n",
      "32/92 [=========>....................] - ETA: 0s - loss: 28.0928 - mae: 140.8772 - mape: 28.0928"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mg:\\Unief\\Masterproef\\Notebooks\\baselinemodel_nosupport.ipynb Cell 13\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Unief/Masterproef/Notebooks/baselinemodel_nosupport.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#Search\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/Unief/Masterproef/Notebooks/baselinemodel_nosupport.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m tuner\u001b[39m.\u001b[39;49msearch(xtraint, ytraint, epochs\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(xtestt, ytestt), callbacks\u001b[39m=\u001b[39;49mget_callbacks(\u001b[39m'\u001b[39;49m\u001b[39mbaseline_nosupp_tuner\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Unief/Masterproef/Notebooks/baselinemodel_nosupport.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                                                                                                     patience\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Unief/Masterproef/Notebooks/baselinemodel_nosupport.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                                                                                     lr_factor\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py:183\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_trial_begin(trial)\n\u001b[1;32m--> 183\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_trial(trial, \u001b[39m*\u001b[39mfit_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs)\n\u001b[0;32m    184\u001b[0m \u001b[39m# `results` is None indicates user updated oracle in `run_trial()`.\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[39mif\u001b[39;00m results \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\keras_tuner\\tuners\\hyperband.py:384\u001b[0m, in \u001b[0;36mHyperband.run_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    382\u001b[0m     fit_kwargs[\u001b[39m\"\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m hp\u001b[39m.\u001b[39mvalues[\u001b[39m\"\u001b[39m\u001b[39mtuner/epochs\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    383\u001b[0m     fit_kwargs[\u001b[39m\"\u001b[39m\u001b[39minitial_epoch\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m hp\u001b[39m.\u001b[39mvalues[\u001b[39m\"\u001b[39m\u001b[39mtuner/initial_epoch\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m--> 384\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(Hyperband, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mrun_trial(trial, \u001b[39m*\u001b[39mfit_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\keras_tuner\\engine\\tuner.py:295\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(model_checkpoint)\n\u001b[0;32m    294\u001b[0m     copied_kwargs[\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m callbacks\n\u001b[1;32m--> 295\u001b[0m     obj_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_and_fit_model(trial, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcopied_kwargs)\n\u001b[0;32m    297\u001b[0m     histories\u001b[39m.\u001b[39mappend(obj_value)\n\u001b[0;32m    298\u001b[0m \u001b[39mreturn\u001b[39;00m histories\n",
      "File \u001b[1;32mc:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\keras_tuner\\engine\\tuner.py:222\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m hp \u001b[39m=\u001b[39m trial\u001b[39m.\u001b[39mhyperparameters\n\u001b[0;32m    221\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_build(hp)\n\u001b[1;32m--> 222\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhypermodel\u001b[39m.\u001b[39mfit(hp, model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    223\u001b[0m tuner_utils\u001b[39m.\u001b[39mvalidate_trial_results(\n\u001b[0;32m    224\u001b[0m     results, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mobjective, \u001b[39m\"\u001b[39m\u001b[39mHyperModel.fit()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    225\u001b[0m )\n\u001b[0;32m    226\u001b[0m \u001b[39mreturn\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py:140\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[1;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, hp, model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    117\u001b[0m     \u001b[39m\"\"\"Train the model.\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \n\u001b[0;32m    119\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m     \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39mfit(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\keras\\engine\\training.py:1570\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1568\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs\n\u001b[0;32m   1569\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[1;32m-> 1570\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[0;32m   1571\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[0;32m   1572\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\keras\\callbacks.py:470\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \n\u001b[0;32m    465\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 470\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m\"\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m\"\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[1;32mc:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\keras\\callbacks.py:317\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    316\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 317\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[0;32m    318\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    320\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mExpected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\keras\\callbacks.py:340\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    337\u001b[0m     batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[0;32m    338\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 340\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[0;32m    342\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    343\u001b[0m     end_hook_name \u001b[39m=\u001b[39m hook_name\n",
      "File \u001b[1;32mc:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\keras\\callbacks.py:385\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[0;32m    383\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> 385\u001b[0m logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_logs(logs, is_batch_hook\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    386\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m    387\u001b[0m     hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n",
      "File \u001b[1;32mc:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\keras\\callbacks.py:292\u001b[0m, in \u001b[0;36mCallbackList._process_logs\u001b[1;34m(self, logs, is_batch_hook)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[39mif\u001b[39;00m is_batch_hook \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_hooks_support_tf_logs:\n\u001b[0;32m    291\u001b[0m     \u001b[39mreturn\u001b[39;00m logs\n\u001b[1;32m--> 292\u001b[0m \u001b[39mreturn\u001b[39;00m tf_utils\u001b[39m.\u001b[39;49msync_to_numpy_or_python_type(logs)\n",
      "File \u001b[1;32mc:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\keras\\utils\\tf_utils.py:635\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[39mreturn\u001b[39;00m t\n\u001b[0;32m    633\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mitem() \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mndim(t) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m t\n\u001b[1;32m--> 635\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(_to_single_numpy_or_python_type, tensors)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\keras\\utils\\tf_utils.py:628\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    626\u001b[0m     \u001b[39m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, tf\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m--> 628\u001b[0m         t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mnumpy()\n\u001b[0;32m    629\u001b[0m     \u001b[39m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[39m# as-is.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(t, (np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mgeneric)):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\ops.py:1157\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[39m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \n\u001b[0;32m   1136\u001b[0m \u001b[39mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[39m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1155\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m maybe_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_arr\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_arr, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\ops.py:1123\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1122\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy_internal()\n\u001b[0;32m   1124\u001b[0m   \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m     \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Search\n",
    "tuner.search(xtraint, ytraint, epochs=200, validation_data=(xtestt, ytestt), callbacks=get_callbacks('baseline_nosupp_tuner',\n",
    "                                                                                                    patience=50,\n",
    "                                                                                                    lr_factor=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in .\\multiple_layers_tuning\n",
      "Showing 1 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x000001C906641D20>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "layers: 5\n",
      "units_0: 248\n",
      "learning_rate: 0.001\n",
      "units_1: 128\n",
      "units_2: 232\n",
      "units_3: 168\n",
      "units_4: 208\n",
      "tuner/epochs: 200\n",
      "tuner/initial_epoch: 67\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0228\n",
      "Score: 11.070463180541992\n"
     ]
    }
   ],
   "source": [
    "#Results\n",
    "tuner.results_summary(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_1 (Normalizat  (None, 12)               25        \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 24)                216       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 136)               3400      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 232)               31784     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 232)               54056     \n",
      "                                                                 \n",
      " dropbaseline (Dropout)      (None, 232)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 233       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 89,818\n",
      "Trainable params: 89,793\n",
      "Non-trainable params: 25\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Build new model\n",
    "model = tf.keras.Sequential([\n",
    "                               tf.keras.layers.Input(12,name='baselineinput'),\n",
    "                               tf.keras.layers.Normalization(),\n",
    "                               tf.keras.layers.Dense(8, activation='relu'),\n",
    "                               tf.keras.layers.Dense(24, activation='relu'),\n",
    "                               tf.keras.layers.Dense(136, activation='relu'),\n",
    "                               tf.keras.layers.Dense(232, activation='relu'),\n",
    "                               tf.keras.layers.Dense(232, activation='relu'),\n",
    "                               tf.keras.layers.Dropout(0.5, name='dropbaseline'),\n",
    "                               tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss='mape',\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                metrics=['mae',tf.keras.metrics.MeanAbsolutePercentageError(name=\"mape\")]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive fit model\n",
    "model.fit(xtraint, ytraint, epochs=400, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 0s 2ms/step\n",
      "Mean absolute error: 21.534584045410156\n",
      "Mean percentage error: 12.700477600097656\n"
     ]
    }
   ],
   "source": [
    "#Test new model\n",
    "preds = model.predict(xtestt)\n",
    "\n",
    "mae = tf.metrics.mean_absolute_error(y_true=ytestt, y_pred=preds.squeeze()).numpy()\n",
    "print(f\"Mean absolute error: {mae}\")\n",
    "\n",
    "m = tf.keras.metrics.MeanAbsolutePercentageError()\n",
    "m.update_state(y_true=ytestt, y_pred=preds.squeeze())\n",
    "print(f\"Mean percentage error: {m.result().numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "80/92 [=========================>....] - ETA: 0s - loss: 67.9070 - mae: 229.4724 - mape: 67.9070INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 3s 18ms/step - loss: 66.1126 - mae: 232.5914 - mape: 66.1126 - val_loss: 55.3307 - val_mae: 276.7819 - val_mape: 55.3307 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "86/92 [===========================>..] - ETA: 0s - loss: 51.1140 - mae: 215.1774 - mape: 51.1140INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 51.0438 - mae: 213.9786 - mape: 51.0438 - val_loss: 51.0305 - val_mae: 272.7459 - val_mape: 51.0305 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "92/92 [==============================] - ETA: 0s - loss: 49.0380 - mae: 210.8426 - mape: 49.0380INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 49.0380 - mae: 210.8426 - mape: 49.0380 - val_loss: 50.9779 - val_mae: 266.5056 - val_mape: 50.9779 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 49.0025 - mae: 207.5206 - mape: 49.0025INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 49.0016 - mae: 209.0781 - mape: 49.0016 - val_loss: 48.8676 - val_mae: 270.3101 - val_mape: 48.8676 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "85/92 [==========================>...] - ETA: 0s - loss: 48.5668 - mae: 204.9263 - mape: 48.5668INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 48.4661 - mae: 207.9494 - mape: 48.4661 - val_loss: 48.4996 - val_mae: 263.3173 - val_mape: 48.4996 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "91/92 [============================>.] - ETA: 0s - loss: 46.5387 - mae: 203.8438 - mape: 46.5387INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 46.5643 - mae: 204.0228 - mape: 46.5643 - val_loss: 47.1128 - val_mae: 266.0718 - val_mape: 47.1128 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "92/92 [==============================] - ETA: 0s - loss: 46.6551 - mae: 202.3945 - mape: 46.6551INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 46.6551 - mae: 202.3945 - mape: 46.6551 - val_loss: 46.5794 - val_mae: 262.6109 - val_mape: 46.5794 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "85/92 [==========================>...] - ETA: 0s - loss: 45.2160 - mae: 199.2972 - mape: 45.2160INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 45.3804 - mae: 198.6747 - mape: 45.3804 - val_loss: 45.9349 - val_mae: 261.0621 - val_mape: 45.9349 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "86/92 [===========================>..] - ETA: 0s - loss: 45.4494 - mae: 199.4580 - mape: 45.4494INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 45.1738 - mae: 198.3747 - mape: 45.1738 - val_loss: 45.1667 - val_mae: 254.6553 - val_mape: 45.1667 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "83/92 [==========================>...] - ETA: 0s - loss: 44.0385 - mae: 189.1423 - mape: 44.0385INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 44.1118 - mae: 196.0868 - mape: 44.1118 - val_loss: 44.2330 - val_mae: 250.1807 - val_mape: 44.2330 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "92/92 [==============================] - ETA: 0s - loss: 44.3584 - mae: 194.0714 - mape: 44.3584INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 44.3584 - mae: 194.0714 - mape: 44.3584 - val_loss: 43.7304 - val_mae: 248.9932 - val_mape: 43.7304 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 42.8150 - mae: 190.8647 - mape: 42.8150 - val_loss: 45.5041 - val_mae: 255.0837 - val_mape: 45.5041 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 42.8325 - mae: 186.3239 - mape: 42.8325 - val_loss: 43.8405 - val_mae: 250.3412 - val_mape: 43.8405 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "92/92 [==============================] - ETA: 0s - loss: 41.3681 - mae: 183.0090 - mape: 41.3681INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 41.3681 - mae: 183.0090 - mape: 41.3681 - val_loss: 42.6890 - val_mae: 240.6282 - val_mape: 42.6890 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "83/92 [==========================>...] - ETA: 0s - loss: 41.0643 - mae: 181.5359 - mape: 41.0643INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 41.0540 - mae: 179.6629 - mape: 41.0540 - val_loss: 42.1914 - val_mae: 231.6516 - val_mape: 42.1914 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 40.5884 - mae: 173.0798 - mape: 40.5884INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 40.5390 - mae: 175.5658 - mape: 40.5390 - val_loss: 41.1427 - val_mae: 231.4965 - val_mape: 41.1427 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "83/92 [==========================>...] - ETA: 0s - loss: 39.9479 - mae: 173.5758 - mape: 39.9479INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 39.9253 - mae: 170.6041 - mape: 39.9253 - val_loss: 40.6572 - val_mae: 229.3761 - val_mape: 40.6572 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "83/92 [==========================>...] - ETA: 0s - loss: 39.6340 - mae: 168.6965 - mape: 39.6340INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 39.5424 - mae: 167.7763 - mape: 39.5424 - val_loss: 39.5959 - val_mae: 217.2353 - val_mape: 39.5959 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 37.7938 - mae: 161.5805 - mape: 37.7938 - val_loss: 39.9323 - val_mae: 221.1919 - val_mape: 39.9323 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "89/92 [============================>.] - ETA: 0s - loss: 37.8960 - mae: 159.0851 - mape: 37.8960INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 37.8799 - mae: 158.3573 - mape: 37.8799 - val_loss: 38.2381 - val_mae: 209.5269 - val_mape: 38.2381 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 38.3448 - mae: 155.5835 - mape: 38.3448 - val_loss: 40.1367 - val_mae: 205.8303 - val_mape: 40.1367 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 38.8515 - mae: 156.0773 - mape: 38.8515 - val_loss: 39.3209 - val_mae: 209.0612 - val_mape: 39.3209 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 37.4612 - mae: 151.8512 - mape: 37.4612 - val_loss: 38.6790 - val_mae: 214.2279 - val_mape: 38.6790 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 36.1793 - mae: 146.6622 - mape: 36.1793 - val_loss: 38.3578 - val_mae: 202.6852 - val_mape: 38.3578 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "89/92 [============================>.] - ETA: 0s - loss: 36.9475 - mae: 147.3751 - mape: 36.9475INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 36.9597 - mae: 146.3647 - mape: 36.9597 - val_loss: 36.4796 - val_mae: 195.3114 - val_mape: 36.4796 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 35.3614 - mae: 141.4417 - mape: 35.3614 - val_loss: 37.6592 - val_mae: 182.2812 - val_mape: 37.6592 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "92/92 [==============================] - 1s 5ms/step - loss: 36.7307 - mae: 139.5882 - mape: 36.7307 - val_loss: 37.7654 - val_mae: 176.2896 - val_mape: 37.7654 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 35.3840 - mae: 132.9934 - mape: 35.3840INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 35.3230 - mae: 132.3487 - mape: 35.3230 - val_loss: 34.8228 - val_mae: 187.2375 - val_mape: 34.8228 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 34.4817 - mae: 133.4581 - mape: 34.4817 - val_loss: 37.9174 - val_mae: 176.0597 - val_mape: 37.9174 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 34.3677 - mae: 126.1432 - mape: 34.3677 - val_loss: 34.8326 - val_mae: 181.0921 - val_mape: 34.8326 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 34.5124 - mae: 127.2777 - mape: 34.5124 - val_loss: 35.7978 - val_mae: 173.8570 - val_mape: 35.7978 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 33.5507 - mae: 123.6551 - mape: 33.5507 - val_loss: 36.2107 - val_mae: 190.8004 - val_mape: 36.2107 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "92/92 [==============================] - 1s 8ms/step - loss: 34.0790 - mae: 125.4422 - mape: 34.0790 - val_loss: 35.5187 - val_mae: 179.9050 - val_mape: 35.5187 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 32.8045 - mae: 119.4506 - mape: 32.8045 - val_loss: 36.1177 - val_mae: 191.2303 - val_mape: 36.1177 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "91/92 [============================>.] - ETA: 0s - loss: 32.5068 - mae: 119.1046 - mape: 32.5068INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 32.5262 - mae: 118.7615 - mape: 32.5262 - val_loss: 34.2640 - val_mae: 168.8209 - val_mape: 34.2640 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 32.7457 - mae: 115.4596 - mape: 32.7457 - val_loss: 35.5607 - val_mae: 167.5428 - val_mape: 35.5607 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 33.0500 - mae: 118.2667 - mape: 33.0500 - val_loss: 34.4473 - val_mae: 162.6514 - val_mape: 34.4473 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "92/92 [==============================] - 1s 5ms/step - loss: 32.5668 - mae: 113.6137 - mape: 32.5668 - val_loss: 34.6867 - val_mae: 163.5732 - val_mape: 34.6867 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "91/92 [============================>.] - ETA: 0s - loss: 31.5621 - mae: 108.9131 - mape: 31.5621INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 31.5746 - mae: 109.1427 - mape: 31.5746 - val_loss: 33.0899 - val_mae: 145.9537 - val_mape: 33.0899 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 31.7984 - mae: 109.2657 - mape: 31.7984 - val_loss: 34.3473 - val_mae: 156.9476 - val_mape: 34.3473 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 32.0935 - mae: 110.9109 - mape: 32.0935INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 32.2096 - mae: 110.0861 - mape: 32.2096 - val_loss: 32.5660 - val_mae: 151.4617 - val_mape: 32.5660 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 31.1629 - mae: 109.9218 - mape: 31.1629 - val_loss: 32.9706 - val_mae: 143.4349 - val_mape: 32.9706 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 30.7731 - mae: 101.6504 - mape: 30.7731 - val_loss: 33.6647 - val_mae: 155.8721 - val_mape: 33.6647 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 31.5357 - mae: 102.9500 - mape: 31.5357 - val_loss: 38.2409 - val_mae: 166.4431 - val_mape: 38.2409 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "83/92 [==========================>...] - ETA: 0s - loss: 30.9370 - mae: 99.7194 - mape: 30.9370 INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 30.9644 - mae: 101.2703 - mape: 30.9644 - val_loss: 32.1528 - val_mae: 134.8362 - val_mape: 32.1528 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 30.0656 - mae: 97.4329 - mape: 30.0656 - val_loss: 32.2296 - val_mae: 138.2891 - val_mape: 32.2296 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 30.5409 - mae: 98.2166 - mape: 30.5409 - val_loss: 32.2976 - val_mae: 140.8613 - val_mape: 32.2976 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "84/92 [==========================>...] - ETA: 0s - loss: 30.0068 - mae: 100.1340 - mape: 30.0068INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 29.8474 - mae: 95.9320 - mape: 29.8474 - val_loss: 31.6177 - val_mae: 131.6895 - val_mape: 31.6177 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 30.4174 - mae: 86.7552 - mape: 30.4174INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 30.6680 - mae: 92.7270 - mape: 30.6680 - val_loss: 31.4995 - val_mae: 121.5976 - val_mape: 31.4995 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 29.8326 - mae: 91.2303 - mape: 29.8326INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 29.7156 - mae: 92.4676 - mape: 29.7156 - val_loss: 31.3696 - val_mae: 124.7440 - val_mape: 31.3696 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "84/92 [==========================>...] - ETA: 0s - loss: 29.4883 - mae: 101.3932 - mape: 29.4883INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 29.2571 - mae: 97.6379 - mape: 29.2571 - val_loss: 31.1875 - val_mae: 128.0963 - val_mape: 31.1875 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "91/92 [============================>.] - ETA: 0s - loss: 29.2237 - mae: 94.3810 - mape: 29.2237INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 29.1842 - mae: 93.9195 - mape: 29.1842 - val_loss: 30.8414 - val_mae: 119.2766 - val_mape: 30.8414 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 29.1168 - mae: 90.6151 - mape: 29.1168 - val_loss: 30.9546 - val_mae: 121.0686 - val_mape: 30.9546 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 28.9757 - mae: 91.2695 - mape: 28.9757 - val_loss: 33.0322 - val_mae: 167.4844 - val_mape: 33.0322 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 28.4499 - mae: 89.7772 - mape: 28.4499 - val_loss: 31.4753 - val_mae: 131.8802 - val_mape: 31.4753 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 28.6789 - mae: 87.3002 - mape: 28.6789 - val_loss: 31.7208 - val_mae: 138.6927 - val_mape: 31.7208 - lr: 0.0010\n",
      "Epoch 57/1000\n",
      "89/92 [============================>.] - ETA: 0s - loss: 27.8759 - mae: 86.3455 - mape: 27.8759INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 27.8551 - mae: 85.9874 - mape: 27.8551 - val_loss: 29.5602 - val_mae: 124.7400 - val_mape: 29.5602 - lr: 0.0010\n",
      "Epoch 58/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 28.0245 - mae: 85.7534 - mape: 28.0245 - val_loss: 35.5647 - val_mae: 119.1636 - val_mape: 35.5647 - lr: 0.0010\n",
      "Epoch 59/1000\n",
      "92/92 [==============================] - 1s 5ms/step - loss: 28.2115 - mae: 87.8025 - mape: 28.2115 - val_loss: 30.1826 - val_mae: 108.7006 - val_mape: 30.1826 - lr: 0.0010\n",
      "Epoch 60/1000\n",
      "91/92 [============================>.] - ETA: 0s - loss: 27.7780 - mae: 88.4009 - mape: 27.7780INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 27.7459 - mae: 88.0154 - mape: 27.7459 - val_loss: 29.0597 - val_mae: 117.0294 - val_mape: 29.0597 - lr: 0.0010\n",
      "Epoch 61/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 27.7200 - mae: 86.0547 - mape: 27.7200 - val_loss: 30.9302 - val_mae: 120.7056 - val_mape: 30.9302 - lr: 0.0010\n",
      "Epoch 62/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 27.3611 - mae: 84.8633 - mape: 27.3611 - val_loss: 29.7057 - val_mae: 116.3153 - val_mape: 29.7057 - lr: 0.0010\n",
      "Epoch 63/1000\n",
      "92/92 [==============================] - 1s 5ms/step - loss: 27.8210 - mae: 78.7808 - mape: 27.8210 - val_loss: 29.5378 - val_mae: 102.1628 - val_mape: 29.5378 - lr: 0.0010\n",
      "Epoch 64/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 26.6601 - mae: 76.0649 - mape: 26.6601 - val_loss: 29.3019 - val_mae: 104.5476 - val_mape: 29.3019 - lr: 0.0010\n",
      "Epoch 65/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 26.1937 - mae: 79.9455 - mape: 26.1937 - val_loss: 29.3937 - val_mae: 112.5631 - val_mape: 29.3937 - lr: 0.0010\n",
      "Epoch 66/1000\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 26.4490 - mae: 74.7988 - mape: 26.4490INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 26.4136 - mae: 76.0609 - mape: 26.4136 - val_loss: 28.1963 - val_mae: 98.5968 - val_mape: 28.1963 - lr: 0.0010\n",
      "Epoch 67/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 26.4144 - mae: 75.2596 - mape: 26.4144 - val_loss: 33.1541 - val_mae: 94.7746 - val_mape: 33.1541 - lr: 0.0010\n",
      "Epoch 68/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 27.0316 - mae: 76.0629 - mape: 27.0316 - val_loss: 30.2657 - val_mae: 131.9391 - val_mape: 30.2657 - lr: 0.0010\n",
      "Epoch 69/1000\n",
      "92/92 [==============================] - ETA: 0s - loss: 26.9673 - mae: 78.6493 - mape: 26.9673INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 26.9673 - mae: 78.6493 - mape: 26.9673 - val_loss: 28.0896 - val_mae: 99.7484 - val_mape: 28.0896 - lr: 0.0010\n",
      "Epoch 70/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 26.2217 - mae: 73.6781 - mape: 26.2217 - val_loss: 28.7011 - val_mae: 116.2398 - val_mape: 28.7011 - lr: 0.0010\n",
      "Epoch 71/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 26.5318 - mae: 76.2434 - mape: 26.5318 - val_loss: 30.5894 - val_mae: 118.0173 - val_mape: 30.5894 - lr: 0.0010\n",
      "Epoch 72/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 25.9053 - mae: 73.1616 - mape: 25.9053 - val_loss: 29.3180 - val_mae: 115.6841 - val_mape: 29.3180 - lr: 0.0010\n",
      "Epoch 73/1000\n",
      "92/92 [==============================] - ETA: 0s - loss: 25.1312 - mae: 71.6492 - mape: 25.1312INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 25.1312 - mae: 71.6492 - mape: 25.1312 - val_loss: 27.8898 - val_mae: 106.9258 - val_mape: 27.8898 - lr: 0.0010\n",
      "Epoch 74/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 25.7159 - mae: 67.2552 - mape: 25.7159 - val_loss: 30.3834 - val_mae: 117.2070 - val_mape: 30.3834 - lr: 0.0010\n",
      "Epoch 75/1000\n",
      "85/92 [==========================>...] - ETA: 0s - loss: 25.3005 - mae: 67.1484 - mape: 25.3005INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 25.3622 - mae: 65.3046 - mape: 25.3622 - val_loss: 26.4073 - val_mae: 92.0167 - val_mape: 26.4073 - lr: 0.0010\n",
      "Epoch 76/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 25.4938 - mae: 69.5890 - mape: 25.4938 - val_loss: 26.9348 - val_mae: 108.4715 - val_mape: 26.9348 - lr: 0.0010\n",
      "Epoch 77/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 26.8791 - mae: 73.6847 - mape: 26.8791 - val_loss: 31.5291 - val_mae: 88.7722 - val_mape: 31.5291 - lr: 0.0010\n",
      "Epoch 78/1000\n",
      "89/92 [============================>.] - ETA: 0s - loss: 25.6811 - mae: 69.6132 - mape: 25.6811INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 25.5493 - mae: 69.0882 - mape: 25.5493 - val_loss: 26.3657 - val_mae: 94.1580 - val_mape: 26.3657 - lr: 0.0010\n",
      "Epoch 79/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 25.1238 - mae: 68.6076 - mape: 25.1238 - val_loss: 29.7670 - val_mae: 86.5497 - val_mape: 29.7670 - lr: 0.0010\n",
      "Epoch 80/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 24.6761 - mae: 67.8842 - mape: 24.6761 - val_loss: 26.9277 - val_mae: 89.1438 - val_mape: 26.9277 - lr: 0.0010\n",
      "Epoch 81/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 23.9742 - mae: 65.3379 - mape: 23.9742 - val_loss: 27.2747 - val_mae: 113.4774 - val_mape: 27.2747 - lr: 0.0010\n",
      "Epoch 82/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 26.2350 - mae: 66.6514 - mape: 26.2350 - val_loss: 26.8188 - val_mae: 88.8362 - val_mape: 26.8188 - lr: 0.0010\n",
      "Epoch 83/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 24.4070 - mae: 66.1044 - mape: 24.4070 - val_loss: 27.2302 - val_mae: 90.3359 - val_mape: 27.2302 - lr: 0.0010\n",
      "Epoch 84/1000\n",
      "92/92 [==============================] - ETA: 0s - loss: 23.8961 - mae: 64.7881 - mape: 23.8961INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 20ms/step - loss: 23.8961 - mae: 64.7881 - mape: 23.8961 - val_loss: 25.8162 - val_mae: 88.0040 - val_mape: 25.8162 - lr: 0.0010\n",
      "Epoch 85/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 24.5689 - mae: 72.7520 - mape: 24.5689 - val_loss: 27.8215 - val_mae: 79.8218 - val_mape: 27.8215 - lr: 0.0010\n",
      "Epoch 86/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 23.4063 - mae: 61.9451 - mape: 23.4063 - val_loss: 26.0500 - val_mae: 112.8802 - val_mape: 26.0500 - lr: 0.0010\n",
      "Epoch 87/1000\n",
      "83/92 [==========================>...] - ETA: 0s - loss: 23.5994 - mae: 67.5266 - mape: 23.5994INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 23.5446 - mae: 66.7235 - mape: 23.5446 - val_loss: 24.9981 - val_mae: 84.2190 - val_mape: 24.9981 - lr: 0.0010\n",
      "Epoch 88/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 23.8366 - mae: 63.5984 - mape: 23.8366 - val_loss: 26.0124 - val_mae: 98.2720 - val_mape: 26.0124 - lr: 0.0010\n",
      "Epoch 89/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 23.3857 - mae: 65.6754 - mape: 23.3857 - val_loss: 25.0882 - val_mae: 79.2322 - val_mape: 25.0882 - lr: 0.0010\n",
      "Epoch 90/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 22.8674 - mae: 64.1572 - mape: 22.8674 - val_loss: 25.8286 - val_mae: 85.8660 - val_mape: 25.8286 - lr: 0.0010\n",
      "Epoch 91/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 22.8481 - mae: 60.3943 - mape: 22.8481 - val_loss: 27.5166 - val_mae: 133.9280 - val_mape: 27.5166 - lr: 0.0010\n",
      "Epoch 92/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 23.6205 - mae: 67.5184 - mape: 23.6205 - val_loss: 26.4896 - val_mae: 83.7629 - val_mape: 26.4896 - lr: 0.0010\n",
      "Epoch 93/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 24.5395 - mae: 61.7311 - mape: 24.5395 - val_loss: 28.6586 - val_mae: 76.6705 - val_mape: 28.6586 - lr: 0.0010\n",
      "Epoch 94/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 23.2833 - mae: 59.4338 - mape: 23.2833 - val_loss: 28.4464 - val_mae: 110.2089 - val_mape: 28.4464 - lr: 0.0010\n",
      "Epoch 95/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 23.6141 - mae: 59.6693 - mape: 23.6141 - val_loss: 31.6300 - val_mae: 101.8835 - val_mape: 31.6300 - lr: 0.0010\n",
      "Epoch 96/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 23.9638 - mae: 68.9748 - mape: 23.9638 - val_loss: 25.4255 - val_mae: 87.5919 - val_mape: 25.4255 - lr: 0.0010\n",
      "Epoch 97/1000\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 22.6463 - mae: 62.7474 - mape: 22.6463INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 22.8385 - mae: 62.2393 - mape: 22.8385 - val_loss: 24.7538 - val_mae: 88.1335 - val_mape: 24.7538 - lr: 0.0010\n",
      "Epoch 98/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 22.6258 - mae: 62.0120 - mape: 22.6258 - val_loss: 27.6045 - val_mae: 72.4652 - val_mape: 27.6045 - lr: 0.0010\n",
      "Epoch 99/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 22.3226 - mae: 58.4601 - mape: 22.3226 - val_loss: 26.3908 - val_mae: 78.7052 - val_mape: 26.3908 - lr: 0.0010\n",
      "Epoch 100/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 22.5540 - mae: 59.1993 - mape: 22.5540 - val_loss: 25.7399 - val_mae: 85.8399 - val_mape: 25.7399 - lr: 0.0010\n",
      "Epoch 101/1000\n",
      "84/92 [==========================>...] - ETA: 0s - loss: 22.0205 - mae: 53.9236 - mape: 22.0205INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 22.1285 - mae: 55.4794 - mape: 22.1285 - val_loss: 24.2200 - val_mae: 71.6364 - val_mape: 24.2200 - lr: 0.0010\n",
      "Epoch 102/1000\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 22.2365 - mae: 59.2621 - mape: 22.2365INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 22.1579 - mae: 58.1895 - mape: 22.1579 - val_loss: 24.0851 - val_mae: 99.8629 - val_mape: 24.0851 - lr: 0.0010\n",
      "Epoch 103/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 22.1136 - mae: 56.6825 - mape: 22.1136 - val_loss: 28.1594 - val_mae: 89.0698 - val_mape: 28.1594 - lr: 0.0010\n",
      "Epoch 104/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 22.9823 - mae: 58.8118 - mape: 22.9823 - val_loss: 27.1738 - val_mae: 103.0339 - val_mape: 27.1738 - lr: 0.0010\n",
      "Epoch 105/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 21.7150 - mae: 54.6572 - mape: 21.7150 - val_loss: 25.6809 - val_mae: 87.2679 - val_mape: 25.6809 - lr: 0.0010\n",
      "Epoch 106/1000\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 22.0737 - mae: 57.7477 - mape: 22.0737INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 22.1259 - mae: 57.6548 - mape: 22.1259 - val_loss: 23.0156 - val_mae: 76.6548 - val_mape: 23.0156 - lr: 0.0010\n",
      "Epoch 107/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 22.4817 - mae: 55.0007 - mape: 22.4817 - val_loss: 23.8314 - val_mae: 82.1125 - val_mape: 23.8314 - lr: 0.0010\n",
      "Epoch 108/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 22.1933 - mae: 53.0274 - mape: 22.1933 - val_loss: 24.2562 - val_mae: 70.0051 - val_mape: 24.2562 - lr: 0.0010\n",
      "Epoch 109/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 21.7319 - mae: 60.3358 - mape: 21.7319 - val_loss: 23.9051 - val_mae: 74.2456 - val_mape: 23.9051 - lr: 0.0010\n",
      "Epoch 110/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 21.0463 - mae: 52.3150 - mape: 21.0463 - val_loss: 23.1242 - val_mae: 66.1797 - val_mape: 23.1242 - lr: 0.0010\n",
      "Epoch 111/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 21.2582 - mae: 56.1569 - mape: 21.2582 - val_loss: 23.7281 - val_mae: 85.3079 - val_mape: 23.7281 - lr: 0.0010\n",
      "Epoch 112/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 20.8166 - mae: 50.7566 - mape: 20.8166 - val_loss: 25.0160 - val_mae: 89.7766 - val_mape: 25.0160 - lr: 0.0010\n",
      "Epoch 113/1000\n",
      "82/92 [=========================>....] - ETA: 0s - loss: 21.3709 - mae: 52.1177 - mape: 21.3709INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 21.2716 - mae: 52.7029 - mape: 21.2716 - val_loss: 22.9526 - val_mae: 68.4066 - val_mape: 22.9526 - lr: 0.0010\n",
      "Epoch 114/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 21.6078 - mae: 55.4004 - mape: 21.6078 - val_loss: 23.1116 - val_mae: 74.4097 - val_mape: 23.1116 - lr: 0.0010\n",
      "Epoch 115/1000\n",
      "92/92 [==============================] - 1s 5ms/step - loss: 21.1576 - mae: 51.7348 - mape: 21.1576 - val_loss: 24.4195 - val_mae: 83.3393 - val_mape: 24.4195 - lr: 0.0010\n",
      "Epoch 116/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 20.6453 - mae: 49.0781 - mape: 20.6453 - val_loss: 24.2332 - val_mae: 78.6855 - val_mape: 24.2332 - lr: 0.0010\n",
      "Epoch 117/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 21.3089 - mae: 55.2333 - mape: 21.3089 - val_loss: 23.5629 - val_mae: 113.1412 - val_mape: 23.5629 - lr: 0.0010\n",
      "Epoch 118/1000\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 21.8784 - mae: 52.6325 - mape: 21.8784INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 21.7898 - mae: 51.5572 - mape: 21.7898 - val_loss: 22.8283 - val_mae: 68.2886 - val_mape: 22.8283 - lr: 0.0010\n",
      "Epoch 119/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 21.0498 - mae: 50.1763 - mape: 21.0498 - val_loss: 25.8442 - val_mae: 67.5118 - val_mape: 25.8442 - lr: 0.0010\n",
      "Epoch 120/1000\n",
      "85/92 [==========================>...] - ETA: 0s - loss: 20.2178 - mae: 53.0937 - mape: 20.2178INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 20.2667 - mae: 52.3112 - mape: 20.2667 - val_loss: 21.7600 - val_mae: 62.7493 - val_mape: 21.7600 - lr: 0.0010\n",
      "Epoch 121/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 20.0350 - mae: 51.5725 - mape: 20.0350 - val_loss: 22.7269 - val_mae: 67.9036 - val_mape: 22.7269 - lr: 0.0010\n",
      "Epoch 122/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 19.9990 - mae: 48.0588 - mape: 19.9990 - val_loss: 22.4313 - val_mae: 60.5631 - val_mape: 22.4313 - lr: 0.0010\n",
      "Epoch 123/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 20.6708 - mae: 54.7418 - mape: 20.6708 - val_loss: 28.4943 - val_mae: 69.3212 - val_mape: 28.4943 - lr: 0.0010\n",
      "Epoch 124/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 21.0654 - mae: 53.4095 - mape: 21.0654 - val_loss: 22.6007 - val_mae: 88.4373 - val_mape: 22.6007 - lr: 0.0010\n",
      "Epoch 125/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 20.1890 - mae: 59.3443 - mape: 20.1890 - val_loss: 22.3078 - val_mae: 62.7830 - val_mape: 22.3078 - lr: 0.0010\n",
      "Epoch 126/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 20.2047 - mae: 47.4257 - mape: 20.2047 - val_loss: 21.7613 - val_mae: 66.8580 - val_mape: 21.7613 - lr: 0.0010\n",
      "Epoch 127/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 19.4843 - mae: 47.3715 - mape: 19.4843 - val_loss: 23.6239 - val_mae: 59.6415 - val_mape: 23.6239 - lr: 0.0010\n",
      "Epoch 128/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 19.5998 - mae: 49.7843 - mape: 19.5998 - val_loss: 23.4712 - val_mae: 66.8743 - val_mape: 23.4712 - lr: 0.0010\n",
      "Epoch 129/1000\n",
      "90/92 [============================>.] - ETA: 0s - loss: 20.5555 - mae: 52.3857 - mape: 20.5555INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 20.5138 - mae: 51.8330 - mape: 20.5138 - val_loss: 21.2409 - val_mae: 58.3958 - val_mape: 21.2409 - lr: 0.0010\n",
      "Epoch 130/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 20.3427 - mae: 48.4864 - mape: 20.3427 - val_loss: 26.7964 - val_mae: 76.7970 - val_mape: 26.7964 - lr: 0.0010\n",
      "Epoch 131/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 20.1821 - mae: 48.9236 - mape: 20.1821 - val_loss: 30.7491 - val_mae: 80.0502 - val_mape: 30.7491 - lr: 0.0010\n",
      "Epoch 132/1000\n",
      "84/92 [==========================>...] - ETA: 0s - loss: 22.6917 - mae: 52.1699 - mape: 22.6917INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 22.5090 - mae: 51.5024 - mape: 22.5090 - val_loss: 20.6262 - val_mae: 59.8697 - val_mape: 20.6262 - lr: 0.0010\n",
      "Epoch 133/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 18.9233 - mae: 45.7502 - mape: 18.9233 - val_loss: 21.5103 - val_mae: 68.6438 - val_mape: 21.5103 - lr: 0.0010\n",
      "Epoch 134/1000\n",
      "82/92 [=========================>....] - ETA: 0s - loss: 18.8229 - mae: 47.1892 - mape: 18.8229INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 18.7236 - mae: 47.1833 - mape: 18.7236 - val_loss: 20.5032 - val_mae: 57.5766 - val_mape: 20.5032 - lr: 0.0010\n",
      "Epoch 135/1000\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 19.1375 - mae: 45.8823 - mape: 19.1375INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 19.1305 - mae: 46.6326 - mape: 19.1305 - val_loss: 20.2275 - val_mae: 65.0878 - val_mape: 20.2275 - lr: 0.0010\n",
      "Epoch 136/1000\n",
      "90/92 [============================>.] - ETA: 0s - loss: 19.0843 - mae: 44.9850 - mape: 19.0843INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 19.0308 - mae: 44.6362 - mape: 19.0308 - val_loss: 20.1120 - val_mae: 57.9371 - val_mape: 20.1120 - lr: 0.0010\n",
      "Epoch 137/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 20.0386 - mae: 46.7862 - mape: 20.0386 - val_loss: 20.4591 - val_mae: 66.3754 - val_mape: 20.4591 - lr: 0.0010\n",
      "Epoch 138/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 20.0516 - mae: 46.0676 - mape: 20.0516 - val_loss: 20.6131 - val_mae: 76.2225 - val_mape: 20.6131 - lr: 0.0010\n",
      "Epoch 139/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 18.8193 - mae: 52.2777 - mape: 18.8193 - val_loss: 20.3073 - val_mae: 59.8335 - val_mape: 20.3073 - lr: 0.0010\n",
      "Epoch 140/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 19.9633 - mae: 48.9378 - mape: 19.9633 - val_loss: 21.8129 - val_mae: 63.9410 - val_mape: 21.8129 - lr: 0.0010\n",
      "Epoch 141/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 19.3913 - mae: 49.4821 - mape: 19.3913 - val_loss: 20.6511 - val_mae: 58.3264 - val_mape: 20.6511 - lr: 0.0010\n",
      "Epoch 142/1000\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 19.5746 - mae: 46.7487 - mape: 19.5746INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 19.5455 - mae: 46.8726 - mape: 19.5455 - val_loss: 19.8667 - val_mae: 82.8589 - val_mape: 19.8667 - lr: 0.0010\n",
      "Epoch 143/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 19.8122 - mae: 49.8608 - mape: 19.8122 - val_loss: 21.5162 - val_mae: 78.0829 - val_mape: 21.5162 - lr: 0.0010\n",
      "Epoch 144/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 20.2442 - mae: 50.5222 - mape: 20.2442 - val_loss: 20.2830 - val_mae: 71.8366 - val_mape: 20.2830 - lr: 0.0010\n",
      "Epoch 145/1000\n",
      "89/92 [============================>.] - ETA: 0s - loss: 19.4938 - mae: 45.3555 - mape: 19.4938INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 19.5526 - mae: 45.3133 - mape: 19.5526 - val_loss: 19.7793 - val_mae: 59.6577 - val_mape: 19.7793 - lr: 0.0010\n",
      "Epoch 146/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 19.8504 - mae: 49.7040 - mape: 19.8504 - val_loss: 20.6406 - val_mae: 58.5230 - val_mape: 20.6406 - lr: 0.0010\n",
      "Epoch 147/1000\n",
      "84/92 [==========================>...] - ETA: 0s - loss: 18.8001 - mae: 45.9168 - mape: 18.8001INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 18.7553 - mae: 45.2017 - mape: 18.7553 - val_loss: 19.5009 - val_mae: 60.1895 - val_mape: 19.5009 - lr: 0.0010\n",
      "Epoch 148/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 19.2278 - mae: 45.1003 - mape: 19.2278 - val_loss: 21.1097 - val_mae: 92.5259 - val_mape: 21.1097 - lr: 0.0010\n",
      "Epoch 149/1000\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 19.0876 - mae: 47.1162 - mape: 19.0876 - val_loss: 21.0448 - val_mae: 54.9181 - val_mape: 21.0448 - lr: 0.0010\n",
      "Epoch 150/1000\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 18.7024 - mae: 46.7250 - mape: 18.7024 - val_loss: 27.1030 - val_mae: 60.5378 - val_mape: 27.1030 - lr: 0.0010\n",
      "Epoch 151/1000\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 19.2151 - mae: 45.0266 - mape: 19.2151 - val_loss: 19.6508 - val_mae: 51.8891 - val_mape: 19.6508 - lr: 0.0010\n",
      "Epoch 152/1000\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 18.5041 - mae: 42.0856 - mape: 18.5041 - val_loss: 23.1118 - val_mae: 59.6121 - val_mape: 23.1118 - lr: 0.0010\n",
      "Epoch 153/1000\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 18.9951 - mae: 44.5484 - mape: 18.9951 - val_loss: 19.5141 - val_mae: 60.9309 - val_mape: 19.5141 - lr: 0.0010\n",
      "Epoch 154/1000\n",
      "85/92 [==========================>...] - ETA: 0s - loss: 18.2239 - mae: 45.6532 - mape: 18.2239INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 18.3083 - mae: 45.5061 - mape: 18.3083 - val_loss: 18.7663 - val_mae: 72.1859 - val_mape: 18.7663 - lr: 0.0010\n",
      "Epoch 155/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 17.9380 - mae: 40.6500 - mape: 17.9380 - val_loss: 20.8617 - val_mae: 84.8964 - val_mape: 20.8617 - lr: 0.0010\n",
      "Epoch 156/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 18.0301 - mae: 45.6648 - mape: 18.0301 - val_loss: 19.6125 - val_mae: 50.9663 - val_mape: 19.6125 - lr: 0.0010\n",
      "Epoch 157/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 18.5976 - mae: 44.6352 - mape: 18.5976 - val_loss: 22.1201 - val_mae: 55.3165 - val_mape: 22.1201 - lr: 0.0010\n",
      "Epoch 158/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 19.2314 - mae: 49.5773 - mape: 19.2314 - val_loss: 20.7006 - val_mae: 91.4697 - val_mape: 20.7006 - lr: 0.0010\n",
      "Epoch 159/1000\n",
      "83/92 [==========================>...] - ETA: 0s - loss: 19.0695 - mae: 46.7176 - mape: 19.0695INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 18.9857 - mae: 46.1770 - mape: 18.9857 - val_loss: 18.7372 - val_mae: 73.4369 - val_mape: 18.7372 - lr: 0.0010\n",
      "Epoch 160/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 18.8092 - mae: 43.1839 - mape: 18.8092 - val_loss: 18.8175 - val_mae: 50.3142 - val_mape: 18.8175 - lr: 0.0010\n",
      "Epoch 161/1000\n",
      "84/92 [==========================>...] - ETA: 0s - loss: 17.6218 - mae: 39.6672 - mape: 17.6218INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 17.7435 - mae: 40.3607 - mape: 17.7435 - val_loss: 18.6702 - val_mae: 54.8151 - val_mape: 18.6702 - lr: 0.0010\n",
      "Epoch 162/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 17.7563 - mae: 41.2766 - mape: 17.7563 - val_loss: 19.2313 - val_mae: 54.3199 - val_mape: 19.2313 - lr: 0.0010\n",
      "Epoch 163/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 18.1488 - mae: 40.9288 - mape: 18.1488 - val_loss: 20.5378 - val_mae: 52.5723 - val_mape: 20.5378 - lr: 0.0010\n",
      "Epoch 164/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 18.2235 - mae: 41.8546 - mape: 18.2235 - val_loss: 21.8565 - val_mae: 60.6540 - val_mape: 21.8565 - lr: 0.0010\n",
      "Epoch 165/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 18.8581 - mae: 44.2121 - mape: 18.8581 - val_loss: 23.8245 - val_mae: 60.5755 - val_mape: 23.8245 - lr: 0.0010\n",
      "Epoch 166/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 18.6955 - mae: 51.6038 - mape: 18.6955 - val_loss: 28.0053 - val_mae: 63.0501 - val_mape: 28.0053 - lr: 0.0010\n",
      "Epoch 167/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 18.0497 - mae: 46.3731 - mape: 18.0497 - val_loss: 19.0080 - val_mae: 54.7204 - val_mape: 19.0080 - lr: 0.0010\n",
      "Epoch 168/1000\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 17.6528 - mae: 42.7945 - mape: 17.6528INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 17.7549 - mae: 44.4770 - mape: 17.7549 - val_loss: 18.3499 - val_mae: 49.1959 - val_mape: 18.3499 - lr: 0.0010\n",
      "Epoch 169/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 17.8834 - mae: 43.5557 - mape: 17.8834 - val_loss: 18.4631 - val_mae: 56.5977 - val_mape: 18.4631 - lr: 0.0010\n",
      "Epoch 170/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 18.2063 - mae: 43.0906 - mape: 18.2063 - val_loss: 20.9064 - val_mae: 51.5214 - val_mape: 20.9064 - lr: 0.0010\n",
      "Epoch 171/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 19.1388 - mae: 44.2988 - mape: 19.1388 - val_loss: 18.5572 - val_mae: 54.4752 - val_mape: 18.5572 - lr: 0.0010\n",
      "Epoch 172/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 17.3966 - mae: 43.9099 - mape: 17.3966 - val_loss: 21.5769 - val_mae: 56.5920 - val_mape: 21.5769 - lr: 0.0010\n",
      "Epoch 173/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 18.7927 - mae: 41.0495 - mape: 18.7927 - val_loss: 25.7356 - val_mae: 82.9090 - val_mape: 25.7356 - lr: 0.0010\n",
      "Epoch 174/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 18.5917 - mae: 44.4217 - mape: 18.5917 - val_loss: 20.8710 - val_mae: 62.5500 - val_mape: 20.8710 - lr: 0.0010\n",
      "Epoch 175/1000\n",
      "89/92 [============================>.] - ETA: 0s - loss: 17.7616 - mae: 38.0985 - mape: 17.7616INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 17.6541 - mae: 38.3898 - mape: 17.6541 - val_loss: 18.0738 - val_mae: 50.7154 - val_mape: 18.0738 - lr: 0.0010\n",
      "Epoch 176/1000\n",
      "84/92 [==========================>...] - ETA: 0s - loss: 17.7652 - mae: 43.8943 - mape: 17.7652INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 17.8039 - mae: 43.8726 - mape: 17.8039 - val_loss: 17.7555 - val_mae: 51.7281 - val_mape: 17.7555 - lr: 0.0010\n",
      "Epoch 177/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 17.8333 - mae: 39.5062 - mape: 17.8333 - val_loss: 24.3572 - val_mae: 56.2405 - val_mape: 24.3572 - lr: 0.0010\n",
      "Epoch 178/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 18.9569 - mae: 48.2973 - mape: 18.9569 - val_loss: 18.0995 - val_mae: 48.7610 - val_mape: 18.0995 - lr: 0.0010\n",
      "Epoch 179/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 17.6799 - mae: 42.2596 - mape: 17.6799 - val_loss: 18.9952 - val_mae: 55.2823 - val_mape: 18.9952 - lr: 0.0010\n",
      "Epoch 180/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 16.8498 - mae: 40.7975 - mape: 16.8498 - val_loss: 17.8628 - val_mae: 47.9216 - val_mape: 17.8628 - lr: 0.0010\n",
      "Epoch 181/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 17.6254 - mae: 44.0661 - mape: 17.6254 - val_loss: 20.9302 - val_mae: 51.5739 - val_mape: 20.9302 - lr: 0.0010\n",
      "Epoch 182/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 16.9301 - mae: 43.0678 - mape: 16.9301 - val_loss: 21.2580 - val_mae: 58.1246 - val_mape: 21.2580 - lr: 0.0010\n",
      "Epoch 183/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 18.7193 - mae: 44.0240 - mape: 18.7193 - val_loss: 19.7186 - val_mae: 56.4729 - val_mape: 19.7186 - lr: 0.0010\n",
      "Epoch 184/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 17.1946 - mae: 45.1197 - mape: 17.1946 - val_loss: 20.9699 - val_mae: 54.1356 - val_mape: 20.9699 - lr: 0.0010\n",
      "Epoch 185/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 17.6721 - mae: 43.2902 - mape: 17.6721 - val_loss: 19.3959 - val_mae: 49.0658 - val_mape: 19.3959 - lr: 0.0010\n",
      "Epoch 186/1000\n",
      "85/92 [==========================>...] - ETA: 0s - loss: 17.7133 - mae: 39.4580 - mape: 17.7133INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 17.5971 - mae: 39.2112 - mape: 17.5971 - val_loss: 17.5004 - val_mae: 47.0201 - val_mape: 17.5004 - lr: 0.0010\n",
      "Epoch 187/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 16.8333 - mae: 39.4681 - mape: 16.8333 - val_loss: 17.6504 - val_mae: 46.2129 - val_mape: 17.6504 - lr: 0.0010\n",
      "Epoch 188/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 16.6012 - mae: 37.2048 - mape: 16.6012 - val_loss: 20.6011 - val_mae: 56.8660 - val_mape: 20.6011 - lr: 0.0010\n",
      "Epoch 189/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 17.8762 - mae: 41.2046 - mape: 17.8762 - val_loss: 18.8649 - val_mae: 50.0577 - val_mape: 18.8649 - lr: 0.0010\n",
      "Epoch 190/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 17.4608 - mae: 41.6870 - mape: 17.4608 - val_loss: 18.9423 - val_mae: 48.0951 - val_mape: 18.9423 - lr: 0.0010\n",
      "Epoch 191/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 16.7042 - mae: 36.9566 - mape: 16.7042 - val_loss: 17.8691 - val_mae: 55.1975 - val_mape: 17.8691 - lr: 0.0010\n",
      "Epoch 192/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 16.8171 - mae: 38.4436 - mape: 16.8171 - val_loss: 19.2129 - val_mae: 59.5389 - val_mape: 19.2129 - lr: 0.0010\n",
      "Epoch 193/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 17.7989 - mae: 40.0958 - mape: 17.7989 - val_loss: 18.6343 - val_mae: 71.8951 - val_mape: 18.6343 - lr: 0.0010\n",
      "Epoch 194/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 18.9219 - mae: 47.7457 - mape: 18.9219 - val_loss: 18.0754 - val_mae: 56.7724 - val_mape: 18.0754 - lr: 0.0010\n",
      "Epoch 195/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 16.2878 - mae: 37.6883 - mape: 16.2878 - val_loss: 18.5655 - val_mae: 57.2922 - val_mape: 18.5655 - lr: 0.0010\n",
      "Epoch 196/1000\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 16.6413 - mae: 38.8286 - mape: 16.6413INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 16.7014 - mae: 39.2547 - mape: 16.7014 - val_loss: 17.4205 - val_mae: 44.7485 - val_mape: 17.4205 - lr: 0.0010\n",
      "Epoch 197/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 16.9664 - mae: 38.2077 - mape: 16.9664 - val_loss: 18.0454 - val_mae: 60.7061 - val_mape: 18.0454 - lr: 0.0010\n",
      "Epoch 198/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 16.7007 - mae: 40.2455 - mape: 16.7007 - val_loss: 20.7322 - val_mae: 54.4567 - val_mape: 20.7322 - lr: 0.0010\n",
      "Epoch 199/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 17.3483 - mae: 41.2296 - mape: 17.3483 - val_loss: 18.9782 - val_mae: 80.8234 - val_mape: 18.9782 - lr: 0.0010\n",
      "Epoch 200/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 16.5196 - mae: 36.3554 - mape: 16.5196 - val_loss: 18.7794 - val_mae: 50.6254 - val_mape: 18.7794 - lr: 0.0010\n",
      "Epoch 201/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 16.6156 - mae: 40.0608 - mape: 16.6156 - val_loss: 22.9638 - val_mae: 56.4939 - val_mape: 22.9638 - lr: 0.0010\n",
      "Epoch 202/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 18.3533 - mae: 39.7726 - mape: 18.3533 - val_loss: 17.5153 - val_mae: 44.5137 - val_mape: 17.5153 - lr: 0.0010\n",
      "Epoch 203/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 17.0297 - mae: 40.9841 - mape: 17.0297 - val_loss: 18.6481 - val_mae: 53.8407 - val_mape: 18.6481 - lr: 0.0010\n",
      "Epoch 204/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 16.4951 - mae: 36.6259 - mape: 16.4951 - val_loss: 17.4915 - val_mae: 60.8532 - val_mape: 17.4915 - lr: 0.0010\n",
      "Epoch 205/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 18.3967 - mae: 43.8854 - mape: 18.3967 - val_loss: 18.5975 - val_mae: 60.4030 - val_mape: 18.5975 - lr: 0.0010\n",
      "Epoch 206/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 16.4348 - mae: 40.1310 - mape: 16.4348 - val_loss: 17.5518 - val_mae: 46.7765 - val_mape: 17.5518 - lr: 0.0010\n",
      "Epoch 207/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 16.9268 - mae: 37.9969 - mape: 16.9268 - val_loss: 17.5865 - val_mae: 56.7096 - val_mape: 17.5865 - lr: 0.0010\n",
      "Epoch 208/1000\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 17.2701 - mae: 39.1753 - mape: 17.2701INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 17.2095 - mae: 38.9538 - mape: 17.2095 - val_loss: 17.0609 - val_mae: 45.8192 - val_mape: 17.0609 - lr: 0.0010\n",
      "Epoch 209/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 16.5294 - mae: 42.6825 - mape: 16.5294 - val_loss: 23.6625 - val_mae: 53.8834 - val_mape: 23.6625 - lr: 0.0010\n",
      "Epoch 210/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 17.3861 - mae: 41.5639 - mape: 17.3861 - val_loss: 20.9593 - val_mae: 48.0686 - val_mape: 20.9593 - lr: 0.0010\n",
      "Epoch 211/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 17.4456 - mae: 38.8436 - mape: 17.4456 - val_loss: 19.1512 - val_mae: 56.4156 - val_mape: 19.1512 - lr: 0.0010\n",
      "Epoch 212/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 17.1575 - mae: 38.1490 - mape: 17.1575 - val_loss: 18.4283 - val_mae: 52.4852 - val_mape: 18.4283 - lr: 0.0010\n",
      "Epoch 213/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 16.5811 - mae: 37.9712 - mape: 16.5811 - val_loss: 18.8938 - val_mae: 74.0920 - val_mape: 18.8938 - lr: 0.0010\n",
      "Epoch 214/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 17.1031 - mae: 42.6226 - mape: 17.1031 - val_loss: 18.4026 - val_mae: 46.5071 - val_mape: 18.4026 - lr: 0.0010\n",
      "Epoch 215/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 16.1489 - mae: 36.9191 - mape: 16.1489 - val_loss: 22.3274 - val_mae: 54.5536 - val_mape: 22.3274 - lr: 0.0010\n",
      "Epoch 216/1000\n",
      "85/92 [==========================>...] - ETA: 0s - loss: 16.8446 - mae: 40.4167 - mape: 16.8446INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 16.8444 - mae: 39.7449 - mape: 16.8444 - val_loss: 16.5669 - val_mae: 42.0817 - val_mape: 16.5669 - lr: 0.0010\n",
      "Epoch 217/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 16.6784 - mae: 36.0486 - mape: 16.6784 - val_loss: 19.3937 - val_mae: 51.8682 - val_mape: 19.3937 - lr: 0.0010\n",
      "Epoch 218/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 17.0140 - mae: 38.0816 - mape: 17.0140 - val_loss: 20.9618 - val_mae: 50.1062 - val_mape: 20.9618 - lr: 0.0010\n",
      "Epoch 219/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 17.2685 - mae: 42.8881 - mape: 17.2685 - val_loss: 17.6047 - val_mae: 86.8120 - val_mape: 17.6047 - lr: 0.0010\n",
      "Epoch 220/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 17.1743 - mae: 42.7795 - mape: 17.1743 - val_loss: 18.2820 - val_mae: 45.8811 - val_mape: 18.2820 - lr: 0.0010\n",
      "Epoch 221/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 16.5642 - mae: 37.2741 - mape: 16.5642 - val_loss: 17.4317 - val_mae: 67.0931 - val_mape: 17.4317 - lr: 0.0010\n",
      "Epoch 222/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 16.7152 - mae: 41.2725 - mape: 16.7152 - val_loss: 19.9499 - val_mae: 52.2840 - val_mape: 19.9499 - lr: 0.0010\n",
      "Epoch 223/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 17.6270 - mae: 41.5060 - mape: 17.6270 - val_loss: 17.5525 - val_mae: 43.0668 - val_mape: 17.5525 - lr: 0.0010\n",
      "Epoch 224/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 17.1327 - mae: 41.3011 - mape: 17.1327 - val_loss: 21.8522 - val_mae: 81.0807 - val_mape: 21.8522 - lr: 0.0010\n",
      "Epoch 225/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 16.1487 - mae: 38.9999 - mape: 16.1487 - val_loss: 24.9310 - val_mae: 48.4297 - val_mape: 24.9310 - lr: 0.0010\n",
      "Epoch 226/1000\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 16.5101 - mae: 41.8522 - mape: 16.5101INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 16.3797 - mae: 41.2023 - mape: 16.3797 - val_loss: 16.5200 - val_mae: 46.6764 - val_mape: 16.5200 - lr: 0.0010\n",
      "Epoch 227/1000\n",
      "92/92 [==============================] - 1s 8ms/step - loss: 16.7300 - mae: 38.1729 - mape: 16.7300 - val_loss: 16.6465 - val_mae: 49.4856 - val_mape: 16.6465 - lr: 0.0010\n",
      "Epoch 228/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 15.4280 - mae: 38.2161 - mape: 15.4280 - val_loss: 17.3244 - val_mae: 52.8118 - val_mape: 17.3244 - lr: 0.0010\n",
      "Epoch 229/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 16.5443 - mae: 38.2047 - mape: 16.5443 - val_loss: 17.6278 - val_mae: 42.5504 - val_mape: 17.6278 - lr: 0.0010\n",
      "Epoch 230/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 15.7305 - mae: 37.1216 - mape: 15.7305 - val_loss: 18.4011 - val_mae: 51.9042 - val_mape: 18.4011 - lr: 0.0010\n",
      "Epoch 231/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 17.2881 - mae: 39.2926 - mape: 17.2881 - val_loss: 16.7314 - val_mae: 47.7900 - val_mape: 16.7314 - lr: 0.0010\n",
      "Epoch 232/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 16.2874 - mae: 37.1748 - mape: 16.2874 - val_loss: 16.6676 - val_mae: 43.1667 - val_mape: 16.6676 - lr: 0.0010\n",
      "Epoch 233/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 14.9992 - mae: 37.1376 - mape: 14.9992 - val_loss: 17.6870 - val_mae: 70.2977 - val_mape: 17.6870 - lr: 0.0010\n",
      "Epoch 234/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 16.2330 - mae: 42.2523 - mape: 16.2330 - val_loss: 17.3547 - val_mae: 42.1970 - val_mape: 17.3547 - lr: 0.0010\n",
      "Epoch 235/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 17.3596 - mae: 38.1405 - mape: 17.3596 - val_loss: 19.7170 - val_mae: 46.7617 - val_mape: 19.7170 - lr: 0.0010\n",
      "Epoch 236/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 16.3280 - mae: 41.9867 - mape: 16.3280 - val_loss: 16.5275 - val_mae: 47.8379 - val_mape: 16.5275 - lr: 0.0010\n",
      "Epoch 237/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 15.4054 - mae: 35.8266 - mape: 15.4054 - val_loss: 18.3479 - val_mae: 51.5057 - val_mape: 18.3479 - lr: 0.0010\n",
      "Epoch 238/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 16.3122 - mae: 39.7609 - mape: 16.3122 - val_loss: 16.6620 - val_mae: 51.5117 - val_mape: 16.6620 - lr: 0.0010\n",
      "Epoch 239/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 15.8842 - mae: 38.8339 - mape: 15.8842 - val_loss: 19.4011 - val_mae: 80.2620 - val_mape: 19.4011 - lr: 0.0010\n",
      "Epoch 240/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 15.9443 - mae: 40.0491 - mape: 15.9443 - val_loss: 17.5529 - val_mae: 46.1401 - val_mape: 17.5529 - lr: 0.0010\n",
      "Epoch 241/1000\n",
      "85/92 [==========================>...] - ETA: 0s - loss: 16.0582 - mae: 38.7993 - mape: 16.0582INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 15.9294 - mae: 38.3790 - mape: 15.9294 - val_loss: 16.2862 - val_mae: 41.0572 - val_mape: 16.2862 - lr: 0.0010\n",
      "Epoch 242/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 15.6245 - mae: 37.4916 - mape: 15.6245 - val_loss: 16.6403 - val_mae: 62.5417 - val_mape: 16.6403 - lr: 0.0010\n",
      "Epoch 243/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 15.5859 - mae: 39.3553 - mape: 15.5859 - val_loss: 16.3561 - val_mae: 53.1478 - val_mape: 16.3561 - lr: 0.0010\n",
      "Epoch 244/1000\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 15.2462 - mae: 39.1574 - mape: 15.2462INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 15.1433 - mae: 38.3927 - mape: 15.1433 - val_loss: 15.7059 - val_mae: 43.7094 - val_mape: 15.7059 - lr: 0.0010\n",
      "Epoch 245/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 16.1356 - mae: 34.1385 - mape: 16.1356 - val_loss: 16.5687 - val_mae: 52.1364 - val_mape: 16.5687 - lr: 0.0010\n",
      "Epoch 246/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 15.5754 - mae: 36.6579 - mape: 15.5754 - val_loss: 17.2264 - val_mae: 44.7557 - val_mape: 17.2264 - lr: 0.0010\n",
      "Epoch 247/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 16.5683 - mae: 38.1861 - mape: 16.5683 - val_loss: 16.5977 - val_mae: 40.5571 - val_mape: 16.5977 - lr: 0.0010\n",
      "Epoch 248/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 15.1914 - mae: 35.0844 - mape: 15.1914 - val_loss: 16.1854 - val_mae: 49.1521 - val_mape: 16.1854 - lr: 0.0010\n",
      "Epoch 249/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 15.3818 - mae: 35.2014 - mape: 15.3818 - val_loss: 16.0532 - val_mae: 40.7208 - val_mape: 16.0532 - lr: 0.0010\n",
      "Epoch 250/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 16.0942 - mae: 35.6860 - mape: 16.0942 - val_loss: 19.4929 - val_mae: 57.0377 - val_mape: 19.4929 - lr: 0.0010\n",
      "Epoch 251/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 16.2605 - mae: 34.2791 - mape: 16.2605 - val_loss: 16.1075 - val_mae: 44.3941 - val_mape: 16.1075 - lr: 0.0010\n",
      "Epoch 252/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 15.4197 - mae: 36.2182 - mape: 15.4197 - val_loss: 17.9082 - val_mae: 41.1562 - val_mape: 17.9082 - lr: 0.0010\n",
      "Epoch 253/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 16.1436 - mae: 38.9384 - mape: 16.1436 - val_loss: 17.4087 - val_mae: 64.5341 - val_mape: 17.4087 - lr: 0.0010\n",
      "Epoch 254/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 16.6142 - mae: 38.4428 - mape: 16.6142 - val_loss: 17.2189 - val_mae: 45.7931 - val_mape: 17.2189 - lr: 0.0010\n",
      "Epoch 255/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 15.5614 - mae: 42.8081 - mape: 15.5614 - val_loss: 18.3869 - val_mae: 66.2363 - val_mape: 18.3869 - lr: 0.0010\n",
      "Epoch 256/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 16.0499 - mae: 37.4740 - mape: 16.0499 - val_loss: 20.4890 - val_mae: 68.2626 - val_mape: 20.4890 - lr: 0.0010\n",
      "Epoch 257/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 16.7244 - mae: 43.3505 - mape: 16.7244 - val_loss: 17.2777 - val_mae: 72.1330 - val_mape: 17.2777 - lr: 0.0010\n",
      "Epoch 258/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 15.4616 - mae: 37.5873 - mape: 15.4616 - val_loss: 16.6288 - val_mae: 40.1780 - val_mape: 16.6288 - lr: 0.0010\n",
      "Epoch 259/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 16.8094 - mae: 40.2768 - mape: 16.8094 - val_loss: 19.3729 - val_mae: 45.1670 - val_mape: 19.3729 - lr: 0.0010\n",
      "Epoch 260/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 17.2331 - mae: 43.0206 - mape: 17.2331 - val_loss: 16.4499 - val_mae: 41.8917 - val_mape: 16.4499 - lr: 0.0010\n",
      "Epoch 261/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 15.8744 - mae: 39.8420 - mape: 15.8744 - val_loss: 16.8691 - val_mae: 77.6211 - val_mape: 16.8691 - lr: 0.0010\n",
      "Epoch 262/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 17.2562 - mae: 43.0589 - mape: 17.2562 - val_loss: 17.9137 - val_mae: 43.4144 - val_mape: 17.9137 - lr: 0.0010\n",
      "Epoch 263/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 15.5591 - mae: 35.8905 - mape: 15.5591 - val_loss: 20.4124 - val_mae: 91.1341 - val_mape: 20.4124 - lr: 0.0010\n",
      "Epoch 264/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 16.3774 - mae: 42.2517 - mape: 16.3774 - val_loss: 17.1787 - val_mae: 41.9209 - val_mape: 17.1787 - lr: 0.0010\n",
      "Epoch 265/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 15.5895 - mae: 35.9065 - mape: 15.5895 - val_loss: 16.9058 - val_mae: 39.1072 - val_mape: 16.9058 - lr: 0.0010\n",
      "Epoch 266/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 15.4451 - mae: 34.0824 - mape: 15.4451 - val_loss: 16.5921 - val_mae: 43.3272 - val_mape: 16.5921 - lr: 0.0010\n",
      "Epoch 267/1000\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 15.8410 - mae: 39.2426 - mape: 15.8410INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 15.8119 - mae: 39.5163 - mape: 15.8119 - val_loss: 15.5428 - val_mae: 40.3861 - val_mape: 15.5428 - lr: 0.0010\n",
      "Epoch 268/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 15.9118 - mae: 37.4881 - mape: 15.9118 - val_loss: 15.6088 - val_mae: 39.0970 - val_mape: 15.6088 - lr: 0.0010\n",
      "Epoch 269/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.4099 - mae: 36.1842 - mape: 14.4099 - val_loss: 16.2137 - val_mae: 43.7612 - val_mape: 16.2137 - lr: 0.0010\n",
      "Epoch 270/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 16.1431 - mae: 40.7546 - mape: 16.1431 - val_loss: 19.0234 - val_mae: 50.1666 - val_mape: 19.0234 - lr: 0.0010\n",
      "Epoch 271/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 16.9215 - mae: 39.1147 - mape: 16.9215 - val_loss: 17.3937 - val_mae: 48.9102 - val_mape: 17.3937 - lr: 0.0010\n",
      "Epoch 272/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 16.1835 - mae: 40.4671 - mape: 16.1835 - val_loss: 16.7735 - val_mae: 58.5651 - val_mape: 16.7735 - lr: 0.0010\n",
      "Epoch 273/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.9432 - mae: 35.5392 - mape: 14.9432 - val_loss: 28.4183 - val_mae: 54.0100 - val_mape: 28.4183 - lr: 0.0010\n",
      "Epoch 274/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 16.5712 - mae: 40.5534 - mape: 16.5712 - val_loss: 17.5492 - val_mae: 55.6638 - val_mape: 17.5492 - lr: 0.0010\n",
      "Epoch 275/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 16.1797 - mae: 41.3298 - mape: 16.1797 - val_loss: 17.1757 - val_mae: 42.3396 - val_mape: 17.1756 - lr: 0.0010\n",
      "Epoch 276/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 15.5245 - mae: 37.0688 - mape: 15.5245 - val_loss: 15.8152 - val_mae: 48.8035 - val_mape: 15.8152 - lr: 0.0010\n",
      "Epoch 277/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.9760 - mae: 33.3663 - mape: 14.9760 - val_loss: 17.8485 - val_mae: 47.7204 - val_mape: 17.8485 - lr: 0.0010\n",
      "Epoch 278/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 14.6614 - mae: 35.9288 - mape: 14.6614 - val_loss: 17.5027 - val_mae: 48.2315 - val_mape: 17.5027 - lr: 0.0010\n",
      "Epoch 279/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 16.5745 - mae: 39.2058 - mape: 16.5745 - val_loss: 15.9719 - val_mae: 56.6258 - val_mape: 15.9719 - lr: 0.0010\n",
      "Epoch 280/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 15.3300 - mae: 35.3276 - mape: 15.3300 - val_loss: 15.8379 - val_mae: 39.5625 - val_mape: 15.8379 - lr: 0.0010\n",
      "Epoch 281/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.5438 - mae: 35.7120 - mape: 14.5438 - val_loss: 17.9741 - val_mae: 41.2460 - val_mape: 17.9741 - lr: 0.0010\n",
      "Epoch 282/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 14.8943 - mae: 34.7151 - mape: 14.8943 - val_loss: 15.8227 - val_mae: 45.2856 - val_mape: 15.8227 - lr: 0.0010\n",
      "Epoch 283/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 15.0797 - mae: 35.1454 - mape: 15.0797 - val_loss: 18.0513 - val_mae: 44.2773 - val_mape: 18.0513 - lr: 0.0010\n",
      "Epoch 284/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 16.4370 - mae: 40.5746 - mape: 16.4370 - val_loss: 17.6002 - val_mae: 47.7380 - val_mape: 17.6002 - lr: 0.0010\n",
      "Epoch 285/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 15.4468 - mae: 39.0608 - mape: 15.4468 - val_loss: 15.7537 - val_mae: 38.2351 - val_mape: 15.7537 - lr: 0.0010\n",
      "Epoch 286/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 14.5690 - mae: 37.2465 - mape: 14.5690 - val_loss: 18.6955 - val_mae: 45.2231 - val_mape: 18.6955 - lr: 0.0010\n",
      "Epoch 287/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 15.2513 - mae: 35.9105 - mape: 15.2513 - val_loss: 16.5907 - val_mae: 36.8662 - val_mape: 16.5907 - lr: 0.0010\n",
      "Epoch 288/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.8400 - mae: 34.4957 - mape: 14.8400 - val_loss: 16.6624 - val_mae: 40.6037 - val_mape: 16.6624 - lr: 0.0010\n",
      "Epoch 289/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.9021 - mae: 37.4099 - mape: 14.9021 - val_loss: 17.9531 - val_mae: 41.1853 - val_mape: 17.9531 - lr: 0.0010\n",
      "Epoch 290/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 15.6452 - mae: 37.5335 - mape: 15.6452 - val_loss: 16.7345 - val_mae: 41.7364 - val_mape: 16.7345 - lr: 0.0010\n",
      "Epoch 291/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 15.9091 - mae: 34.7129 - mape: 15.9091 - val_loss: 16.5656 - val_mae: 61.9775 - val_mape: 16.5656 - lr: 0.0010\n",
      "Epoch 292/1000\n",
      "86/92 [===========================>..] - ETA: 0s - loss: 15.6543 - mae: 33.4677 - mape: 15.6543INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 15.5151 - mae: 34.3193 - mape: 15.5151 - val_loss: 15.4388 - val_mae: 37.3345 - val_mape: 15.4388 - lr: 0.0010\n",
      "Epoch 293/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 15.5697 - mae: 35.0131 - mape: 15.5697 - val_loss: 16.5701 - val_mae: 57.5645 - val_mape: 16.5701 - lr: 0.0010\n",
      "Epoch 294/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.4895 - mae: 38.4770 - mape: 14.4895 - val_loss: 16.5232 - val_mae: 38.3829 - val_mape: 16.5232 - lr: 0.0010\n",
      "Epoch 295/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 16.8661 - mae: 42.0236 - mape: 16.8661 - val_loss: 16.7521 - val_mae: 41.4388 - val_mape: 16.7521 - lr: 0.0010\n",
      "Epoch 296/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.9630 - mae: 37.3128 - mape: 14.9630 - val_loss: 17.6504 - val_mae: 43.1599 - val_mape: 17.6504 - lr: 0.0010\n",
      "Epoch 297/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.9159 - mae: 37.4261 - mape: 14.9159 - val_loss: 16.4887 - val_mae: 56.7060 - val_mape: 16.4887 - lr: 0.0010\n",
      "Epoch 298/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 16.0888 - mae: 35.4420 - mape: 16.0888 - val_loss: 18.7072 - val_mae: 43.1098 - val_mape: 18.7072 - lr: 0.0010\n",
      "Epoch 299/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 15.1175 - mae: 34.9201 - mape: 15.1175 - val_loss: 17.8649 - val_mae: 39.4982 - val_mape: 17.8649 - lr: 0.0010\n",
      "Epoch 300/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 15.0002 - mae: 35.4025 - mape: 15.0002 - val_loss: 16.3241 - val_mae: 47.6634 - val_mape: 16.3241 - lr: 0.0010\n",
      "Epoch 301/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.7265 - mae: 34.2864 - mape: 14.7265 - val_loss: 17.5463 - val_mae: 57.2402 - val_mape: 17.5463 - lr: 0.0010\n",
      "Epoch 302/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 15.0235 - mae: 36.5564 - mape: 15.0235 - val_loss: 17.7241 - val_mae: 47.1936 - val_mape: 17.7241 - lr: 0.0010\n",
      "Epoch 303/1000\n",
      "86/92 [===========================>..] - ETA: 0s - loss: 14.7285 - mae: 37.8372 - mape: 14.7285INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 14.6864 - mae: 37.0315 - mape: 14.6864 - val_loss: 15.1938 - val_mae: 37.2972 - val_mape: 15.1938 - lr: 0.0010\n",
      "Epoch 304/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 16.2426 - mae: 35.2726 - mape: 16.2426 - val_loss: 22.0918 - val_mae: 57.6669 - val_mape: 22.0918 - lr: 0.0010\n",
      "Epoch 305/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 15.5350 - mae: 34.5729 - mape: 15.5350 - val_loss: 16.5249 - val_mae: 40.6317 - val_mape: 16.5249 - lr: 0.0010\n",
      "Epoch 306/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.8122 - mae: 36.2231 - mape: 14.8122 - val_loss: 15.9282 - val_mae: 52.2242 - val_mape: 15.9282 - lr: 0.0010\n",
      "Epoch 307/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.6322 - mae: 33.9203 - mape: 14.6322 - val_loss: 16.3490 - val_mae: 44.0494 - val_mape: 16.3490 - lr: 0.0010\n",
      "Epoch 308/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.3038 - mae: 32.2397 - mape: 14.3038 - val_loss: 18.1866 - val_mae: 39.3301 - val_mape: 18.1866 - lr: 0.0010\n",
      "Epoch 309/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.6662 - mae: 34.7277 - mape: 14.6662 - val_loss: 21.0199 - val_mae: 53.6982 - val_mape: 21.0199 - lr: 0.0010\n",
      "Epoch 310/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 15.4608 - mae: 36.3566 - mape: 15.4608 - val_loss: 15.8062 - val_mae: 36.8814 - val_mape: 15.8062 - lr: 0.0010\n",
      "Epoch 311/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.5738 - mae: 34.7508 - mape: 14.5738 - val_loss: 15.2446 - val_mae: 39.8232 - val_mape: 15.2446 - lr: 0.0010\n",
      "Epoch 312/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.5395 - mae: 35.0353 - mape: 14.5395 - val_loss: 16.1096 - val_mae: 40.1576 - val_mape: 16.1096 - lr: 0.0010\n",
      "Epoch 313/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.4051 - mae: 37.4146 - mape: 14.4051 - val_loss: 16.2052 - val_mae: 59.1407 - val_mape: 16.2052 - lr: 0.0010\n",
      "Epoch 314/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 15.6350 - mae: 42.0198 - mape: 15.6350 - val_loss: 16.0502 - val_mae: 38.6303 - val_mape: 16.0502 - lr: 0.0010\n",
      "Epoch 315/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.9691 - mae: 33.9561 - mape: 14.9691 - val_loss: 16.0347 - val_mae: 36.4370 - val_mape: 16.0347 - lr: 0.0010\n",
      "Epoch 316/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.8807 - mae: 36.3890 - mape: 14.8807 - val_loss: 18.0212 - val_mae: 44.9475 - val_mape: 18.0212 - lr: 0.0010\n",
      "Epoch 317/1000\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 14.6499 - mae: 38.0625 - mape: 14.6499INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 14.6290 - mae: 38.2587 - mape: 14.6290 - val_loss: 15.0382 - val_mae: 49.0054 - val_mape: 15.0382 - lr: 0.0010\n",
      "Epoch 318/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 15.1857 - mae: 37.3350 - mape: 15.1857 - val_loss: 16.8824 - val_mae: 38.4803 - val_mape: 16.8824 - lr: 0.0010\n",
      "Epoch 319/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 14.9567 - mae: 36.1542 - mape: 14.9567 - val_loss: 15.6375 - val_mae: 40.9431 - val_mape: 15.6375 - lr: 0.0010\n",
      "Epoch 320/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 16.0698 - mae: 34.5322 - mape: 16.0698 - val_loss: 18.6655 - val_mae: 55.4485 - val_mape: 18.6655 - lr: 0.0010\n",
      "Epoch 321/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.4252 - mae: 35.8088 - mape: 14.4252 - val_loss: 15.6806 - val_mae: 37.6754 - val_mape: 15.6806 - lr: 0.0010\n",
      "Epoch 322/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.0921 - mae: 32.7973 - mape: 14.0921 - val_loss: 15.7449 - val_mae: 40.3558 - val_mape: 15.7449 - lr: 0.0010\n",
      "Epoch 323/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.8640 - mae: 38.3545 - mape: 14.8640 - val_loss: 17.2651 - val_mae: 52.4126 - val_mape: 17.2651 - lr: 0.0010\n",
      "Epoch 324/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 14.4092 - mae: 38.7231 - mape: 14.4092 - val_loss: 20.8572 - val_mae: 61.5025 - val_mape: 20.8572 - lr: 0.0010\n",
      "Epoch 325/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 14.3971 - mae: 36.1589 - mape: 14.3971 - val_loss: 17.0496 - val_mae: 39.4294 - val_mape: 17.0496 - lr: 0.0010\n",
      "Epoch 326/1000\n",
      "85/92 [==========================>...] - ETA: 0s - loss: 14.5090 - mae: 35.9139 - mape: 14.5090INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 14.3545 - mae: 34.7953 - mape: 14.3545 - val_loss: 14.9060 - val_mae: 41.4052 - val_mape: 14.9060 - lr: 0.0010\n",
      "Epoch 327/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 14.6727 - mae: 37.3720 - mape: 14.6727 - val_loss: 15.6829 - val_mae: 65.3486 - val_mape: 15.6829 - lr: 0.0010\n",
      "Epoch 328/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 15.3884 - mae: 36.8060 - mape: 15.3884 - val_loss: 16.4962 - val_mae: 39.8171 - val_mape: 16.4962 - lr: 0.0010\n",
      "Epoch 329/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 15.0200 - mae: 37.4148 - mape: 15.0200 - val_loss: 15.0665 - val_mae: 46.3707 - val_mape: 15.0665 - lr: 0.0010\n",
      "Epoch 330/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 15.7380 - mae: 35.9240 - mape: 15.7380 - val_loss: 17.0391 - val_mae: 36.4072 - val_mape: 17.0391 - lr: 0.0010\n",
      "Epoch 331/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.5416 - mae: 35.0270 - mape: 14.5416 - val_loss: 16.1155 - val_mae: 64.4586 - val_mape: 16.1155 - lr: 0.0010\n",
      "Epoch 332/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 13.7235 - mae: 35.3405 - mape: 13.7235 - val_loss: 15.8055 - val_mae: 35.2744 - val_mape: 15.8055 - lr: 0.0010\n",
      "Epoch 333/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.5249 - mae: 36.5800 - mape: 14.5249 - val_loss: 19.9232 - val_mae: 52.6411 - val_mape: 19.9232 - lr: 0.0010\n",
      "Epoch 334/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 15.6111 - mae: 36.1851 - mape: 15.6111 - val_loss: 20.2979 - val_mae: 60.5074 - val_mape: 20.2979 - lr: 0.0010\n",
      "Epoch 335/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 15.0544 - mae: 38.6656 - mape: 15.0544 - val_loss: 19.4024 - val_mae: 45.3044 - val_mape: 19.4024 - lr: 0.0010\n",
      "Epoch 336/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 14.3806 - mae: 34.5809 - mape: 14.3806 - val_loss: 17.1218 - val_mae: 83.4399 - val_mape: 17.1218 - lr: 0.0010\n",
      "Epoch 337/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.9837 - mae: 35.6334 - mape: 14.9837 - val_loss: 16.5590 - val_mae: 39.8448 - val_mape: 16.5590 - lr: 0.0010\n",
      "Epoch 338/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.2304 - mae: 35.6877 - mape: 14.2304 - val_loss: 15.0062 - val_mae: 40.2442 - val_mape: 15.0062 - lr: 0.0010\n",
      "Epoch 339/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.6139 - mae: 36.5299 - mape: 14.6139 - val_loss: 15.8338 - val_mae: 35.7831 - val_mape: 15.8338 - lr: 0.0010\n",
      "Epoch 340/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.0731 - mae: 31.7964 - mape: 14.0731 - val_loss: 15.8129 - val_mae: 49.2571 - val_mape: 15.8129 - lr: 0.0010\n",
      "Epoch 341/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.6557 - mae: 36.4561 - mape: 14.6557 - val_loss: 16.3932 - val_mae: 37.5915 - val_mape: 16.3932 - lr: 0.0010\n",
      "Epoch 342/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.8179 - mae: 37.3830 - mape: 14.8179 - val_loss: 15.8619 - val_mae: 38.9908 - val_mape: 15.8619 - lr: 0.0010\n",
      "Epoch 343/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 13.8973 - mae: 29.1728 - mape: 13.8973 - val_loss: 15.2515 - val_mae: 40.0386 - val_mape: 15.2515 - lr: 0.0010\n",
      "Epoch 344/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.5761 - mae: 36.9990 - mape: 14.5761 - val_loss: 15.3577 - val_mae: 42.6246 - val_mape: 15.3577 - lr: 0.0010\n",
      "Epoch 345/1000\n",
      "89/92 [============================>.] - ETA: 0s - loss: 13.7825 - mae: 38.5768 - mape: 13.7825INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 13.7280 - mae: 38.0980 - mape: 13.7280 - val_loss: 14.8403 - val_mae: 37.7401 - val_mape: 14.8403 - lr: 0.0010\n",
      "Epoch 346/1000\n",
      "92/92 [==============================] - 1s 8ms/step - loss: 14.4320 - mae: 34.9365 - mape: 14.4320 - val_loss: 15.2614 - val_mae: 36.8368 - val_mape: 15.2614 - lr: 0.0010\n",
      "Epoch 347/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 15.9972 - mae: 38.2128 - mape: 15.9972 - val_loss: 15.7022 - val_mae: 47.5966 - val_mape: 15.7022 - lr: 0.0010\n",
      "Epoch 348/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 15.8943 - mae: 39.2153 - mape: 15.8943 - val_loss: 15.4510 - val_mae: 37.4806 - val_mape: 15.4510 - lr: 0.0010\n",
      "Epoch 349/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 15.0612 - mae: 37.2919 - mape: 15.0612 - val_loss: 16.9436 - val_mae: 41.0912 - val_mape: 16.9436 - lr: 0.0010\n",
      "Epoch 350/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.3585 - mae: 35.5484 - mape: 14.3585 - val_loss: 15.2856 - val_mae: 45.5745 - val_mape: 15.2856 - lr: 0.0010\n",
      "Epoch 351/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 13.7091 - mae: 32.8714 - mape: 13.7091 - val_loss: 15.3045 - val_mae: 38.2005 - val_mape: 15.3045 - lr: 0.0010\n",
      "Epoch 352/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 15.5892 - mae: 38.4909 - mape: 15.5892 - val_loss: 23.0535 - val_mae: 49.5600 - val_mape: 23.0535 - lr: 0.0010\n",
      "Epoch 353/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 15.3025 - mae: 43.4602 - mape: 15.3025 - val_loss: 15.7064 - val_mae: 60.6686 - val_mape: 15.7064 - lr: 0.0010\n",
      "Epoch 354/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 14.5274 - mae: 36.2756 - mape: 14.5274 - val_loss: 15.4864 - val_mae: 36.8896 - val_mape: 15.4864 - lr: 0.0010\n",
      "Epoch 355/1000\n",
      "89/92 [============================>.] - ETA: 0s - loss: 14.3526 - mae: 34.0819 - mape: 14.3526INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 14.3730 - mae: 34.7725 - mape: 14.3730 - val_loss: 14.7418 - val_mae: 35.2002 - val_mape: 14.7418 - lr: 0.0010\n",
      "Epoch 356/1000\n",
      "92/92 [==============================] - 1s 8ms/step - loss: 14.2341 - mae: 34.5202 - mape: 14.2341 - val_loss: 24.6277 - val_mae: 79.6302 - val_mape: 24.6277 - lr: 0.0010\n",
      "Epoch 357/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 15.5165 - mae: 39.5278 - mape: 15.5165 - val_loss: 16.4324 - val_mae: 49.4673 - val_mape: 16.4324 - lr: 0.0010\n",
      "Epoch 358/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 14.5917 - mae: 36.1929 - mape: 14.5917 - val_loss: 15.8285 - val_mae: 38.8356 - val_mape: 15.8285 - lr: 0.0010\n",
      "Epoch 359/1000\n",
      "92/92 [==============================] - 1s 5ms/step - loss: 15.5792 - mae: 37.8775 - mape: 15.5792 - val_loss: 16.7399 - val_mae: 41.4912 - val_mape: 16.7399 - lr: 0.0010\n",
      "Epoch 360/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 14.3043 - mae: 39.0468 - mape: 14.3043 - val_loss: 23.8189 - val_mae: 74.5633 - val_mape: 23.8189 - lr: 0.0010\n",
      "Epoch 361/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.3826 - mae: 42.3824 - mape: 14.3826 - val_loss: 15.5504 - val_mae: 38.5440 - val_mape: 15.5504 - lr: 0.0010\n",
      "Epoch 362/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.2513 - mae: 34.2012 - mape: 14.2513 - val_loss: 14.9179 - val_mae: 35.8098 - val_mape: 14.9179 - lr: 0.0010\n",
      "Epoch 363/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 14.1809 - mae: 35.9059 - mape: 14.1809 - val_loss: 15.3958 - val_mae: 39.1863 - val_mape: 15.3958 - lr: 0.0010\n",
      "Epoch 364/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 14.7628 - mae: 36.3531 - mape: 14.7628 - val_loss: 15.3960 - val_mae: 37.4362 - val_mape: 15.3960 - lr: 0.0010\n",
      "Epoch 365/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 15.3629 - mae: 35.3129 - mape: 15.3629 - val_loss: 17.1121 - val_mae: 40.0844 - val_mape: 17.1121 - lr: 0.0010\n",
      "Epoch 366/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 14.5522 - mae: 36.8196 - mape: 14.5522 - val_loss: 15.0643 - val_mae: 35.4138 - val_mape: 15.0643 - lr: 0.0010\n",
      "Epoch 367/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 14.0240 - mae: 40.6271 - mape: 14.0240 - val_loss: 16.2596 - val_mae: 39.7458 - val_mape: 16.2596 - lr: 0.0010\n",
      "Epoch 368/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 15.2215 - mae: 36.5945 - mape: 15.2215 - val_loss: 14.7980 - val_mae: 38.6440 - val_mape: 14.7980 - lr: 0.0010\n",
      "Epoch 369/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 14.0963 - mae: 34.7879 - mape: 14.0963 - val_loss: 19.7744 - val_mae: 39.8578 - val_mape: 19.7744 - lr: 0.0010\n",
      "Epoch 370/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 14.6745 - mae: 33.7545 - mape: 14.6745 - val_loss: 18.5534 - val_mae: 38.9295 - val_mape: 18.5534 - lr: 0.0010\n",
      "Epoch 371/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 14.6101 - mae: 33.3476 - mape: 14.6101 - val_loss: 15.8512 - val_mae: 49.8583 - val_mape: 15.8512 - lr: 0.0010\n",
      "Epoch 372/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 13.4403 - mae: 34.0482 - mape: 13.4403 - val_loss: 16.4510 - val_mae: 59.4902 - val_mape: 16.4510 - lr: 0.0010\n",
      "Epoch 373/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 15.1771 - mae: 38.5331 - mape: 15.1771 - val_loss: 16.9496 - val_mae: 61.5409 - val_mape: 16.9496 - lr: 0.0010\n",
      "Epoch 374/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 15.1081 - mae: 40.9637 - mape: 15.1081 - val_loss: 16.2636 - val_mae: 43.7985 - val_mape: 16.2636 - lr: 0.0010\n",
      "Epoch 375/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 14.0884 - mae: 35.9270 - mape: 14.0884 - val_loss: 17.6783 - val_mae: 40.7364 - val_mape: 17.6783 - lr: 0.0010\n",
      "Epoch 376/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 13.9137 - mae: 35.6153 - mape: 13.9137 - val_loss: 15.8230 - val_mae: 63.7498 - val_mape: 15.8230 - lr: 0.0010\n",
      "Epoch 377/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 14.1341 - mae: 35.9500 - mape: 14.1341 - val_loss: 15.0660 - val_mae: 59.6830 - val_mape: 15.0660 - lr: 0.0010\n",
      "Epoch 378/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 13.8112 - mae: 31.5661 - mape: 13.8112 - val_loss: 16.0255 - val_mae: 39.8769 - val_mape: 16.0255 - lr: 0.0010\n",
      "Epoch 379/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 15.5159 - mae: 36.5770 - mape: 15.5159 - val_loss: 15.7057 - val_mae: 63.4206 - val_mape: 15.7057 - lr: 0.0010\n",
      "Epoch 380/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 13.5976 - mae: 38.7041 - mape: 13.5976 - val_loss: 14.8981 - val_mae: 37.6803 - val_mape: 14.8981 - lr: 0.0010\n",
      "Epoch 381/1000\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 13.5813 - mae: 32.1519 - mape: 13.5813INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 13.5898 - mae: 32.0766 - mape: 13.5898 - val_loss: 14.7295 - val_mae: 38.3631 - val_mape: 14.7295 - lr: 0.0010\n",
      "Epoch 382/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 13.8906 - mae: 36.8279 - mape: 13.8906 - val_loss: 21.3389 - val_mae: 48.1345 - val_mape: 21.3389 - lr: 0.0010\n",
      "Epoch 383/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 15.4119 - mae: 40.2081 - mape: 15.4119 - val_loss: 15.8639 - val_mae: 45.9514 - val_mape: 15.8639 - lr: 0.0010\n",
      "Epoch 384/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.7558 - mae: 40.0353 - mape: 14.7558 - val_loss: 15.9123 - val_mae: 38.4263 - val_mape: 15.9123 - lr: 0.0010\n",
      "Epoch 385/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 13.9714 - mae: 34.8136 - mape: 13.9714 - val_loss: 16.8067 - val_mae: 57.8391 - val_mape: 16.8067 - lr: 0.0010\n",
      "Epoch 386/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.2404 - mae: 38.1091 - mape: 14.2404 - val_loss: 15.6878 - val_mae: 43.3780 - val_mape: 15.6878 - lr: 0.0010\n",
      "Epoch 387/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.5900 - mae: 37.0029 - mape: 14.5900 - val_loss: 24.4763 - val_mae: 68.5963 - val_mape: 24.4763 - lr: 0.0010\n",
      "Epoch 388/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 15.5605 - mae: 34.8206 - mape: 15.5605 - val_loss: 15.5113 - val_mae: 59.4975 - val_mape: 15.5113 - lr: 0.0010\n",
      "Epoch 389/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 13.8438 - mae: 33.5509 - mape: 13.8438 - val_loss: 18.7340 - val_mae: 55.5546 - val_mape: 18.7340 - lr: 0.0010\n",
      "Epoch 390/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 13.9981 - mae: 33.5429 - mape: 13.9981 - val_loss: 15.5654 - val_mae: 37.2173 - val_mape: 15.5654 - lr: 0.0010\n",
      "Epoch 391/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 13.7344 - mae: 35.8248 - mape: 13.7344 - val_loss: 15.8090 - val_mae: 41.8041 - val_mape: 15.8090 - lr: 0.0010\n",
      "Epoch 392/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 13.6337 - mae: 35.6888 - mape: 13.6337 - val_loss: 14.8832 - val_mae: 51.4408 - val_mape: 14.8832 - lr: 0.0010\n",
      "Epoch 393/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 13.1135 - mae: 35.7529 - mape: 13.1135 - val_loss: 20.4858 - val_mae: 40.8399 - val_mape: 20.4858 - lr: 0.0010\n",
      "Epoch 394/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.6739 - mae: 36.4727 - mape: 14.6739 - val_loss: 16.0366 - val_mae: 63.5930 - val_mape: 16.0366 - lr: 0.0010\n",
      "Epoch 395/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 14.1231 - mae: 35.1774 - mape: 14.1231 - val_loss: 17.4036 - val_mae: 34.3617 - val_mape: 17.4036 - lr: 0.0010\n",
      "Epoch 396/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 13.7275 - mae: 32.1784 - mape: 13.7275 - val_loss: 15.1599 - val_mae: 36.7269 - val_mape: 15.1599 - lr: 0.0010\n",
      "Epoch 397/1000\n",
      "86/92 [===========================>..] - ETA: 0s - loss: 13.2937 - mae: 37.4964 - mape: 13.2937INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 13.3086 - mae: 38.7867 - mape: 13.3086 - val_loss: 14.6195 - val_mae: 43.2619 - val_mape: 14.6195 - lr: 0.0010\n",
      "Epoch 398/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.5780 - mae: 37.2537 - mape: 14.5780 - val_loss: 15.4529 - val_mae: 61.6991 - val_mape: 15.4529 - lr: 0.0010\n",
      "Epoch 399/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.9467 - mae: 38.1642 - mape: 14.9467 - val_loss: 18.1549 - val_mae: 42.4154 - val_mape: 18.1549 - lr: 0.0010\n",
      "Epoch 400/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 15.2016 - mae: 35.1070 - mape: 15.2016 - val_loss: 15.5297 - val_mae: 38.0093 - val_mape: 15.5297 - lr: 0.0010\n",
      "Epoch 401/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 13.2719 - mae: 32.6006 - mape: 13.2719 - val_loss: 15.6002 - val_mae: 62.0765 - val_mape: 15.6002 - lr: 0.0010\n",
      "Epoch 402/1000\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 13.6364 - mae: 32.9662 - mape: 13.6364INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 13.6090 - mae: 32.7262 - mape: 13.6090 - val_loss: 14.5850 - val_mae: 40.0060 - val_mape: 14.5850 - lr: 0.0010\n",
      "Epoch 403/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 13.7241 - mae: 35.4272 - mape: 13.7241 - val_loss: 15.1427 - val_mae: 40.2710 - val_mape: 15.1427 - lr: 0.0010\n",
      "Epoch 404/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.1629 - mae: 45.3027 - mape: 14.1629 - val_loss: 14.8396 - val_mae: 38.7204 - val_mape: 14.8396 - lr: 0.0010\n",
      "Epoch 405/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.2344 - mae: 38.5910 - mape: 14.2344 - val_loss: 14.8416 - val_mae: 37.8663 - val_mape: 14.8416 - lr: 0.0010\n",
      "Epoch 406/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.4234 - mae: 33.0896 - mape: 14.4234 - val_loss: 15.5212 - val_mae: 34.5098 - val_mape: 15.5212 - lr: 0.0010\n",
      "Epoch 407/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.2234 - mae: 32.8797 - mape: 14.2234 - val_loss: 16.3413 - val_mae: 34.9760 - val_mape: 16.3413 - lr: 0.0010\n",
      "Epoch 408/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.0572 - mae: 38.5841 - mape: 14.0572 - val_loss: 14.6111 - val_mae: 41.9677 - val_mape: 14.6111 - lr: 0.0010\n",
      "Epoch 409/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 13.5144 - mae: 34.3103 - mape: 13.5144 - val_loss: 15.5412 - val_mae: 45.6882 - val_mape: 15.5412 - lr: 0.0010\n",
      "Epoch 410/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 13.8255 - mae: 35.2248 - mape: 13.8255 - val_loss: 19.7047 - val_mae: 37.9832 - val_mape: 19.7047 - lr: 0.0010\n",
      "Epoch 411/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 15.1864 - mae: 38.7444 - mape: 15.1864 - val_loss: 17.5509 - val_mae: 37.1928 - val_mape: 17.5509 - lr: 0.0010\n",
      "Epoch 412/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 13.7224 - mae: 31.7255 - mape: 13.7224 - val_loss: 14.6133 - val_mae: 35.7858 - val_mape: 14.6133 - lr: 0.0010\n",
      "Epoch 413/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 13.2879 - mae: 33.8067 - mape: 13.2879 - val_loss: 14.6014 - val_mae: 33.8589 - val_mape: 14.6014 - lr: 0.0010\n",
      "Epoch 414/1000\n",
      "89/92 [============================>.] - ETA: 0s - loss: 12.8496 - mae: 30.6266 - mape: 12.8496INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 12.8902 - mae: 30.7890 - mape: 12.8902 - val_loss: 14.0607 - val_mae: 35.6245 - val_mape: 14.0607 - lr: 0.0010\n",
      "Epoch 415/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 14.2709 - mae: 35.0303 - mape: 14.2709 - val_loss: 15.6265 - val_mae: 47.1261 - val_mape: 15.6265 - lr: 0.0010\n",
      "Epoch 416/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 13.8505 - mae: 32.5846 - mape: 13.8505 - val_loss: 15.6469 - val_mae: 36.4005 - val_mape: 15.6469 - lr: 0.0010\n",
      "Epoch 417/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.1746 - mae: 33.9365 - mape: 14.1746 - val_loss: 19.2799 - val_mae: 40.1397 - val_mape: 19.2799 - lr: 0.0010\n",
      "Epoch 418/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.3329 - mae: 33.1430 - mape: 14.3329 - val_loss: 15.9064 - val_mae: 43.1323 - val_mape: 15.9064 - lr: 0.0010\n",
      "Epoch 419/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.0300 - mae: 35.9387 - mape: 14.0300 - val_loss: 15.9142 - val_mae: 42.2369 - val_mape: 15.9142 - lr: 0.0010\n",
      "Epoch 420/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 15.4041 - mae: 35.2940 - mape: 15.4041 - val_loss: 21.4575 - val_mae: 67.1277 - val_mape: 21.4575 - lr: 0.0010\n",
      "Epoch 421/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.5915 - mae: 37.3357 - mape: 14.5915 - val_loss: 15.0196 - val_mae: 52.4083 - val_mape: 15.0196 - lr: 0.0010\n",
      "Epoch 422/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 13.3196 - mae: 34.7626 - mape: 13.3196 - val_loss: 14.7581 - val_mae: 37.9295 - val_mape: 14.7581 - lr: 0.0010\n",
      "Epoch 423/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 13.9426 - mae: 37.4083 - mape: 13.9426 - val_loss: 15.6739 - val_mae: 38.5925 - val_mape: 15.6739 - lr: 0.0010\n",
      "Epoch 424/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 13.2424 - mae: 33.7558 - mape: 13.2424 - val_loss: 15.3813 - val_mae: 60.3061 - val_mape: 15.3813 - lr: 0.0010\n",
      "Epoch 425/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 14.3886 - mae: 33.6706 - mape: 14.3886 - val_loss: 19.8296 - val_mae: 38.2080 - val_mape: 19.8296 - lr: 0.0010\n",
      "Epoch 426/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 13.2239 - mae: 30.5779 - mape: 13.2239 - val_loss: 17.5934 - val_mae: 35.1239 - val_mape: 17.5934 - lr: 0.0010\n",
      "Epoch 427/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 14.0116 - mae: 33.5811 - mape: 14.0116 - val_loss: 14.9919 - val_mae: 38.7655 - val_mape: 14.9919 - lr: 0.0010\n",
      "Epoch 428/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 13.1730 - mae: 37.0241 - mape: 13.1730 - val_loss: 18.5975 - val_mae: 58.9014 - val_mape: 18.5975 - lr: 0.0010\n",
      "Epoch 429/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 13.8954 - mae: 35.6804 - mape: 13.8954 - val_loss: 14.6504 - val_mae: 37.7373 - val_mape: 14.6504 - lr: 0.0010\n",
      "Epoch 430/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 13.4176 - mae: 33.4411 - mape: 13.4176 - val_loss: 16.3311 - val_mae: 37.7084 - val_mape: 16.3311 - lr: 0.0010\n",
      "Epoch 431/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 13.6458 - mae: 32.0445 - mape: 13.6458 - val_loss: 15.9881 - val_mae: 40.1539 - val_mape: 15.9881 - lr: 0.0010\n",
      "Epoch 432/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 14.0368 - mae: 35.5210 - mape: 14.0368 - val_loss: 16.6809 - val_mae: 73.3946 - val_mape: 16.6809 - lr: 0.0010\n",
      "Epoch 433/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 13.1840 - mae: 36.9056 - mape: 13.1840 - val_loss: 15.8859 - val_mae: 44.2794 - val_mape: 15.8859 - lr: 0.0010\n",
      "Epoch 434/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 13.2455 - mae: 36.4463 - mape: 13.2455 - val_loss: 17.6546 - val_mae: 74.2202 - val_mape: 17.6546 - lr: 0.0010\n",
      "Epoch 435/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 13.5943 - mae: 33.8467 - mape: 13.5943 - val_loss: 16.5991 - val_mae: 34.2686 - val_mape: 16.5991 - lr: 0.0010\n",
      "Epoch 436/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 15.5161 - mae: 37.1535 - mape: 15.5161 - val_loss: 22.3663 - val_mae: 59.7360 - val_mape: 22.3663 - lr: 0.0010\n",
      "Epoch 437/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 14.8068 - mae: 38.3586 - mape: 14.8068 - val_loss: 16.1644 - val_mae: 36.1387 - val_mape: 16.1644 - lr: 0.0010\n",
      "Epoch 438/1000\n",
      "92/92 [==============================] - 1s 8ms/step - loss: 13.0778 - mae: 31.3164 - mape: 13.0778 - val_loss: 14.6970 - val_mae: 32.8605 - val_mape: 14.6970 - lr: 0.0010\n",
      "Epoch 439/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 13.1131 - mae: 33.9477 - mape: 13.1131 - val_loss: 15.2542 - val_mae: 38.5478 - val_mape: 15.2542 - lr: 0.0010\n",
      "Epoch 440/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 12.9555 - mae: 32.4322 - mape: 12.9555 - val_loss: 17.4466 - val_mae: 33.3985 - val_mape: 17.4466 - lr: 0.0010\n",
      "Epoch 441/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 14.6543 - mae: 39.3450 - mape: 14.6543 - val_loss: 15.2294 - val_mae: 42.9159 - val_mape: 15.2294 - lr: 0.0010\n",
      "Epoch 442/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 13.8289 - mae: 34.6814 - mape: 13.8289 - val_loss: 15.9738 - val_mae: 64.8468 - val_mape: 15.9738 - lr: 0.0010\n",
      "Epoch 443/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 13.5201 - mae: 35.5188 - mape: 13.5201 - val_loss: 15.1338 - val_mae: 42.4284 - val_mape: 15.1338 - lr: 0.0010\n",
      "Epoch 444/1000\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 13.5343 - mae: 36.2359 - mape: 13.5343\n",
      "Epoch 444: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 13.5625 - mae: 36.8156 - mape: 13.5625 - val_loss: 16.7255 - val_mae: 47.5465 - val_mape: 16.7255 - lr: 0.0010\n",
      "Epoch 445/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 12.3725 - mae: 30.0056 - mape: 12.3725 - val_loss: 14.4060 - val_mae: 39.3094 - val_mape: 14.4060 - lr: 2.0000e-04\n",
      "Epoch 446/1000\n",
      "89/92 [============================>.] - ETA: 0s - loss: 11.8714 - mae: 29.5719 - mape: 11.8714INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 11.8532 - mae: 29.3087 - mape: 11.8532 - val_loss: 14.0028 - val_mae: 33.9698 - val_mape: 14.0028 - lr: 2.0000e-04\n",
      "Epoch 447/1000\n",
      "84/92 [==========================>...] - ETA: 0s - loss: 11.4080 - mae: 27.2086 - mape: 11.4080INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 11.4160 - mae: 27.1127 - mape: 11.4160 - val_loss: 13.9474 - val_mae: 36.7762 - val_mape: 13.9474 - lr: 2.0000e-04\n",
      "Epoch 448/1000\n",
      "92/92 [==============================] - 1s 8ms/step - loss: 11.2421 - mae: 26.8443 - mape: 11.2421 - val_loss: 14.1282 - val_mae: 37.8297 - val_mape: 14.1282 - lr: 2.0000e-04\n",
      "Epoch 449/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 11.3237 - mae: 28.4708 - mape: 11.3237 - val_loss: 14.2507 - val_mae: 34.2160 - val_mape: 14.2507 - lr: 2.0000e-04\n",
      "Epoch 450/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 11.4246 - mae: 28.7840 - mape: 11.4246 - val_loss: 14.4614 - val_mae: 38.4715 - val_mape: 14.4614 - lr: 2.0000e-04\n",
      "Epoch 451/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 11.2945 - mae: 28.1766 - mape: 11.2945 - val_loss: 14.1161 - val_mae: 32.5428 - val_mape: 14.1161 - lr: 2.0000e-04\n",
      "Epoch 452/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 11.3835 - mae: 29.5980 - mape: 11.3835 - val_loss: 13.9604 - val_mae: 37.0894 - val_mape: 13.9604 - lr: 2.0000e-04\n",
      "Epoch 453/1000\n",
      "91/92 [============================>.] - ETA: 0s - loss: 11.4935 - mae: 27.8075 - mape: 11.4935INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 11.4750 - mae: 27.6826 - mape: 11.4750 - val_loss: 13.8479 - val_mae: 34.2950 - val_mape: 13.8479 - lr: 2.0000e-04\n",
      "Epoch 454/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 11.6330 - mae: 28.4810 - mape: 11.6330 - val_loss: 14.2127 - val_mae: 32.2764 - val_mape: 14.2127 - lr: 2.0000e-04\n",
      "Epoch 455/1000\n",
      "84/92 [==========================>...] - ETA: 0s - loss: 11.3797 - mae: 25.1700 - mape: 11.3797INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 11.3865 - mae: 26.3945 - mape: 11.3865 - val_loss: 13.6212 - val_mae: 31.4936 - val_mape: 13.6212 - lr: 2.0000e-04\n",
      "Epoch 456/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 11.2402 - mae: 28.0747 - mape: 11.2402 - val_loss: 13.7620 - val_mae: 32.7103 - val_mape: 13.7620 - lr: 2.0000e-04\n",
      "Epoch 457/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 11.3707 - mae: 29.7315 - mape: 11.3707 - val_loss: 14.0906 - val_mae: 42.0900 - val_mape: 14.0906 - lr: 2.0000e-04\n",
      "Epoch 458/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 11.6750 - mae: 26.1417 - mape: 11.6750 - val_loss: 13.8364 - val_mae: 32.9970 - val_mape: 13.8364 - lr: 2.0000e-04\n",
      "Epoch 459/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 11.3531 - mae: 28.0012 - mape: 11.3531 - val_loss: 13.6299 - val_mae: 30.7339 - val_mape: 13.6299 - lr: 2.0000e-04\n",
      "Epoch 460/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 11.3369 - mae: 27.6833 - mape: 11.3369 - val_loss: 13.7658 - val_mae: 31.4026 - val_mape: 13.7658 - lr: 2.0000e-04\n",
      "Epoch 461/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 11.5826 - mae: 28.3036 - mape: 11.5826 - val_loss: 13.9088 - val_mae: 33.5140 - val_mape: 13.9088 - lr: 2.0000e-04\n",
      "Epoch 462/1000\n",
      "92/92 [==============================] - ETA: 0s - loss: 11.3631 - mae: 25.1358 - mape: 11.3631INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 11.3631 - mae: 25.1358 - mape: 11.3631 - val_loss: 13.5818 - val_mae: 30.8924 - val_mape: 13.5818 - lr: 2.0000e-04\n",
      "Epoch 463/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 11.1629 - mae: 28.9504 - mape: 11.1629 - val_loss: 13.8465 - val_mae: 32.0315 - val_mape: 13.8465 - lr: 2.0000e-04\n",
      "Epoch 464/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 11.4962 - mae: 28.4776 - mape: 11.4962 - val_loss: 14.2998 - val_mae: 42.7318 - val_mape: 14.2998 - lr: 2.0000e-04\n",
      "Epoch 465/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 11.3798 - mae: 27.5788 - mape: 11.3798 - val_loss: 13.8926 - val_mae: 39.2266 - val_mape: 13.8926 - lr: 2.0000e-04\n",
      "Epoch 466/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 11.1366 - mae: 27.8385 - mape: 11.1366 - val_loss: 13.6626 - val_mae: 34.9567 - val_mape: 13.6626 - lr: 2.0000e-04\n",
      "Epoch 467/1000\n",
      "92/92 [==============================] - ETA: 0s - loss: 11.4172 - mae: 28.5598 - mape: 11.4172INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 11.4172 - mae: 28.5598 - mape: 11.4172 - val_loss: 13.5470 - val_mae: 38.0350 - val_mape: 13.5470 - lr: 2.0000e-04\n",
      "Epoch 468/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 11.1024 - mae: 26.9026 - mape: 11.1024 - val_loss: 13.7022 - val_mae: 34.0212 - val_mape: 13.7022 - lr: 2.0000e-04\n",
      "Epoch 469/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 11.1442 - mae: 27.1385 - mape: 11.1442 - val_loss: 13.6193 - val_mae: 32.0746 - val_mape: 13.6193 - lr: 2.0000e-04\n",
      "Epoch 470/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 11.4689 - mae: 25.9336 - mape: 11.4689 - val_loss: 14.0159 - val_mae: 30.8292 - val_mape: 14.0159 - lr: 2.0000e-04\n",
      "Epoch 471/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 11.4457 - mae: 28.9381 - mape: 11.4457 - val_loss: 13.7315 - val_mae: 35.0894 - val_mape: 13.7315 - lr: 2.0000e-04\n",
      "Epoch 472/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 11.1992 - mae: 26.3676 - mape: 11.1992 - val_loss: 13.9575 - val_mae: 32.7251 - val_mape: 13.9575 - lr: 2.0000e-04\n",
      "Epoch 473/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 11.2732 - mae: 26.4426 - mape: 11.2732 - val_loss: 13.7743 - val_mae: 33.4261 - val_mape: 13.7743 - lr: 2.0000e-04\n",
      "Epoch 474/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.9579 - mae: 29.0354 - mape: 10.9579 - val_loss: 13.7453 - val_mae: 34.8987 - val_mape: 13.7453 - lr: 2.0000e-04\n",
      "Epoch 475/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 11.2477 - mae: 25.6863 - mape: 11.2477 - val_loss: 14.2064 - val_mae: 30.8701 - val_mape: 14.2064 - lr: 2.0000e-04\n",
      "Epoch 476/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 11.1328 - mae: 27.6462 - mape: 11.1328 - val_loss: 13.5812 - val_mae: 31.8708 - val_mape: 13.5812 - lr: 2.0000e-04\n",
      "Epoch 477/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 10.9502 - mae: 27.6767 - mape: 10.9502 - val_loss: 13.5629 - val_mae: 31.5823 - val_mape: 13.5629 - lr: 2.0000e-04\n",
      "Epoch 478/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 11.4266 - mae: 26.4951 - mape: 11.4266 - val_loss: 14.0719 - val_mae: 30.5722 - val_mape: 14.0719 - lr: 2.0000e-04\n",
      "Epoch 479/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 11.3507 - mae: 27.8241 - mape: 11.3507 - val_loss: 13.5988 - val_mae: 32.9706 - val_mape: 13.5988 - lr: 2.0000e-04\n",
      "Epoch 480/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 11.1688 - mae: 26.5829 - mape: 11.1688 - val_loss: 13.5943 - val_mae: 35.0191 - val_mape: 13.5943 - lr: 2.0000e-04\n",
      "Epoch 481/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 11.3795 - mae: 27.8254 - mape: 11.3795 - val_loss: 13.8248 - val_mae: 38.6960 - val_mape: 13.8248 - lr: 2.0000e-04\n",
      "Epoch 482/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 11.2560 - mae: 26.3109 - mape: 11.2560 - val_loss: 13.8059 - val_mae: 32.3483 - val_mape: 13.8059 - lr: 2.0000e-04\n",
      "Epoch 483/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 11.0237 - mae: 26.7151 - mape: 11.0237 - val_loss: 15.5701 - val_mae: 37.8156 - val_mape: 15.5701 - lr: 2.0000e-04\n",
      "Epoch 484/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 11.0696 - mae: 27.6081 - mape: 11.0696 - val_loss: 13.7265 - val_mae: 37.5280 - val_mape: 13.7265 - lr: 2.0000e-04\n",
      "Epoch 485/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 11.0605 - mae: 27.3929 - mape: 11.0605 - val_loss: 13.8495 - val_mae: 30.9375 - val_mape: 13.8495 - lr: 2.0000e-04\n",
      "Epoch 486/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 11.1509 - mae: 27.9057 - mape: 11.1509 - val_loss: 13.9548 - val_mae: 32.8241 - val_mape: 13.9548 - lr: 2.0000e-04\n",
      "Epoch 487/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 11.2742 - mae: 28.7083 - mape: 11.2742 - val_loss: 13.9965 - val_mae: 31.7472 - val_mape: 13.9965 - lr: 2.0000e-04\n",
      "Epoch 488/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.9235 - mae: 27.3391 - mape: 10.9235 - val_loss: 13.7538 - val_mae: 35.4784 - val_mape: 13.7538 - lr: 2.0000e-04\n",
      "Epoch 489/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.9660 - mae: 27.6926 - mape: 10.9660 - val_loss: 14.9112 - val_mae: 31.1044 - val_mape: 14.9112 - lr: 2.0000e-04\n",
      "Epoch 490/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 11.0814 - mae: 27.6217 - mape: 11.0814 - val_loss: 13.5786 - val_mae: 30.1833 - val_mape: 13.5786 - lr: 2.0000e-04\n",
      "Epoch 491/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 11.0928 - mae: 27.0477 - mape: 11.0928 - val_loss: 13.9417 - val_mae: 32.4300 - val_mape: 13.9417 - lr: 2.0000e-04\n",
      "Epoch 492/1000\n",
      "92/92 [==============================] - 1s 8ms/step - loss: 11.1606 - mae: 27.7012 - mape: 11.1606 - val_loss: 13.7751 - val_mae: 34.6566 - val_mape: 13.7751 - lr: 2.0000e-04\n",
      "Epoch 493/1000\n",
      "89/92 [============================>.] - ETA: 0s - loss: 11.0295 - mae: 26.4539 - mape: 11.0295INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 11.0154 - mae: 26.2939 - mape: 11.0154 - val_loss: 13.5099 - val_mae: 31.3652 - val_mape: 13.5099 - lr: 2.0000e-04\n",
      "Epoch 494/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 11.0570 - mae: 26.5282 - mape: 11.0570 - val_loss: 13.8322 - val_mae: 32.9259 - val_mape: 13.8322 - lr: 2.0000e-04\n",
      "Epoch 495/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 11.1039 - mae: 27.8285 - mape: 11.1039 - val_loss: 13.7012 - val_mae: 42.6049 - val_mape: 13.7012 - lr: 2.0000e-04\n",
      "Epoch 496/1000\n",
      "86/92 [===========================>..] - ETA: 0s - loss: 11.0610 - mae: 28.1551 - mape: 11.0610INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 11.0171 - mae: 27.4648 - mape: 11.0171 - val_loss: 13.3441 - val_mae: 30.9160 - val_mape: 13.3441 - lr: 2.0000e-04\n",
      "Epoch 497/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 11.1545 - mae: 28.1548 - mape: 11.1545 - val_loss: 14.8117 - val_mae: 39.5986 - val_mape: 14.8117 - lr: 2.0000e-04\n",
      "Epoch 498/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 11.4052 - mae: 29.2183 - mape: 11.4052 - val_loss: 13.5413 - val_mae: 33.7994 - val_mape: 13.5413 - lr: 2.0000e-04\n",
      "Epoch 499/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.9196 - mae: 27.7551 - mape: 10.9196 - val_loss: 13.9165 - val_mae: 38.7877 - val_mape: 13.9165 - lr: 2.0000e-04\n",
      "Epoch 500/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 10.8575 - mae: 27.1993 - mape: 10.8575 - val_loss: 13.5793 - val_mae: 31.4962 - val_mape: 13.5793 - lr: 2.0000e-04\n",
      "Epoch 501/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 10.9402 - mae: 28.0992 - mape: 10.9402 - val_loss: 13.5663 - val_mae: 29.8316 - val_mape: 13.5663 - lr: 2.0000e-04\n",
      "Epoch 502/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 10.8517 - mae: 27.6152 - mape: 10.8517 - val_loss: 13.8644 - val_mae: 32.6091 - val_mape: 13.8644 - lr: 2.0000e-04\n",
      "Epoch 503/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 11.0290 - mae: 27.6835 - mape: 11.0290 - val_loss: 14.4808 - val_mae: 31.9503 - val_mape: 14.4808 - lr: 2.0000e-04\n",
      "Epoch 504/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 10.9838 - mae: 24.4643 - mape: 10.9838 - val_loss: 13.7524 - val_mae: 29.0623 - val_mape: 13.7524 - lr: 2.0000e-04\n",
      "Epoch 505/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 11.1221 - mae: 28.2128 - mape: 11.1221 - val_loss: 13.6270 - val_mae: 36.2880 - val_mape: 13.6270 - lr: 2.0000e-04\n",
      "Epoch 506/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.9236 - mae: 29.2686 - mape: 10.9236 - val_loss: 13.5299 - val_mae: 31.0182 - val_mape: 13.5299 - lr: 2.0000e-04\n",
      "Epoch 507/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 11.5556 - mae: 26.6972 - mape: 11.5556 - val_loss: 13.5022 - val_mae: 36.1771 - val_mape: 13.5022 - lr: 2.0000e-04\n",
      "Epoch 508/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 11.0241 - mae: 25.0938 - mape: 11.0241 - val_loss: 13.5381 - val_mae: 30.1556 - val_mape: 13.5381 - lr: 2.0000e-04\n",
      "Epoch 509/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 10.9409 - mae: 27.9370 - mape: 10.9409 - val_loss: 14.6439 - val_mae: 30.4814 - val_mape: 14.6439 - lr: 2.0000e-04\n",
      "Epoch 510/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 11.2479 - mae: 27.4917 - mape: 11.2479 - val_loss: 13.8713 - val_mae: 31.6253 - val_mape: 13.8713 - lr: 2.0000e-04\n",
      "Epoch 511/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 11.0263 - mae: 26.2892 - mape: 11.0263 - val_loss: 13.5931 - val_mae: 30.0818 - val_mape: 13.5931 - lr: 2.0000e-04\n",
      "Epoch 512/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 11.0257 - mae: 28.0407 - mape: 11.0257 - val_loss: 13.9714 - val_mae: 40.5627 - val_mape: 13.9714 - lr: 2.0000e-04\n",
      "Epoch 513/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 11.1128 - mae: 26.6140 - mape: 11.1128 - val_loss: 13.5818 - val_mae: 32.3923 - val_mape: 13.5818 - lr: 2.0000e-04\n",
      "Epoch 514/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 11.0100 - mae: 27.7027 - mape: 11.0100 - val_loss: 13.9494 - val_mae: 30.2552 - val_mape: 13.9494 - lr: 2.0000e-04\n",
      "Epoch 515/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 10.9588 - mae: 28.0190 - mape: 10.9588 - val_loss: 13.5878 - val_mae: 34.0107 - val_mape: 13.5878 - lr: 2.0000e-04\n",
      "Epoch 516/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 10.7544 - mae: 28.6015 - mape: 10.7544 - val_loss: 13.5657 - val_mae: 30.7292 - val_mape: 13.5657 - lr: 2.0000e-04\n",
      "Epoch 517/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 11.0367 - mae: 28.6119 - mape: 11.0367 - val_loss: 13.6146 - val_mae: 42.4809 - val_mape: 13.6146 - lr: 2.0000e-04\n",
      "Epoch 518/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 11.0304 - mae: 27.5033 - mape: 11.0304 - val_loss: 13.7139 - val_mae: 35.0663 - val_mape: 13.7139 - lr: 2.0000e-04\n",
      "Epoch 519/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 10.7538 - mae: 24.6632 - mape: 10.7538 - val_loss: 13.5514 - val_mae: 30.9970 - val_mape: 13.5514 - lr: 2.0000e-04\n",
      "Epoch 520/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 11.1016 - mae: 27.2347 - mape: 11.1016 - val_loss: 13.7645 - val_mae: 39.3289 - val_mape: 13.7645 - lr: 2.0000e-04\n",
      "Epoch 521/1000\n",
      "92/92 [==============================] - 1s 8ms/step - loss: 10.7763 - mae: 25.4406 - mape: 10.7763 - val_loss: 13.5595 - val_mae: 33.9003 - val_mape: 13.5595 - lr: 2.0000e-04\n",
      "Epoch 522/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 10.8751 - mae: 28.0960 - mape: 10.8751 - val_loss: 13.7177 - val_mae: 30.0439 - val_mape: 13.7177 - lr: 2.0000e-04\n",
      "Epoch 523/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 10.8724 - mae: 25.0482 - mape: 10.8724 - val_loss: 13.7073 - val_mae: 30.5200 - val_mape: 13.7073 - lr: 2.0000e-04\n",
      "Epoch 524/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 10.7680 - mae: 26.7434 - mape: 10.7680 - val_loss: 14.2703 - val_mae: 31.0362 - val_mape: 14.2703 - lr: 2.0000e-04\n",
      "Epoch 525/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 10.9201 - mae: 27.8523 - mape: 10.9201 - val_loss: 13.8358 - val_mae: 34.4596 - val_mape: 13.8358 - lr: 2.0000e-04\n",
      "Epoch 526/1000\n",
      "88/92 [===========================>..] - ETA: 0s - loss: 11.1654 - mae: 26.2846 - mape: 11.1654\n",
      "Epoch 526: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 11.1748 - mae: 26.6792 - mape: 11.1748 - val_loss: 13.6804 - val_mae: 30.5905 - val_mape: 13.6804 - lr: 2.0000e-04\n",
      "Epoch 527/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 10.3670 - mae: 25.4273 - mape: 10.3670 - val_loss: 13.5068 - val_mae: 32.4928 - val_mape: 13.5068 - lr: 4.0000e-05\n",
      "Epoch 528/1000\n",
      "91/92 [============================>.] - ETA: 0s - loss: 10.5824 - mae: 27.1932 - mape: 10.5824INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 10.5674 - mae: 27.0787 - mape: 10.5674 - val_loss: 13.3380 - val_mae: 29.6655 - val_mape: 13.3380 - lr: 4.0000e-05\n",
      "Epoch 529/1000\n",
      "92/92 [==============================] - 1s 5ms/step - loss: 10.4601 - mae: 26.7544 - mape: 10.4601 - val_loss: 13.4127 - val_mae: 31.5345 - val_mape: 13.4127 - lr: 4.0000e-05\n",
      "Epoch 530/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 10.5744 - mae: 27.4961 - mape: 10.5744 - val_loss: 13.3820 - val_mae: 34.3161 - val_mape: 13.3820 - lr: 4.0000e-05\n",
      "Epoch 531/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.2905 - mae: 26.6937 - mape: 10.2905 - val_loss: 13.3477 - val_mae: 29.8844 - val_mape: 13.3477 - lr: 4.0000e-05\n",
      "Epoch 532/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 10.4057 - mae: 25.4737 - mape: 10.4057 - val_loss: 13.3473 - val_mae: 29.7139 - val_mape: 13.3473 - lr: 4.0000e-05\n",
      "Epoch 533/1000\n",
      "82/92 [=========================>....] - ETA: 0s - loss: 10.5012 - mae: 26.0677 - mape: 10.5012INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 10.4933 - mae: 25.1399 - mape: 10.4933 - val_loss: 13.3132 - val_mae: 30.2507 - val_mape: 13.3132 - lr: 4.0000e-05\n",
      "Epoch 534/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 10.5461 - mae: 26.7613 - mape: 10.5461 - val_loss: 13.6112 - val_mae: 32.5000 - val_mape: 13.6112 - lr: 4.0000e-05\n",
      "Epoch 535/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 10.6110 - mae: 25.5608 - mape: 10.6110 - val_loss: 13.4052 - val_mae: 32.2345 - val_mape: 13.4052 - lr: 4.0000e-05\n",
      "Epoch 536/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 10.4326 - mae: 25.4665 - mape: 10.4326 - val_loss: 13.4238 - val_mae: 29.3506 - val_mape: 13.4238 - lr: 4.0000e-05\n",
      "Epoch 537/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.4314 - mae: 24.9571 - mape: 10.4314 - val_loss: 13.9987 - val_mae: 31.0904 - val_mape: 13.9987 - lr: 4.0000e-05\n",
      "Epoch 538/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 10.6181 - mae: 27.6058 - mape: 10.6181 - val_loss: 13.3519 - val_mae: 31.0724 - val_mape: 13.3519 - lr: 4.0000e-05\n",
      "Epoch 539/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 10.4281 - mae: 27.8372 - mape: 10.4281 - val_loss: 13.4387 - val_mae: 32.4324 - val_mape: 13.4387 - lr: 4.0000e-05\n",
      "Epoch 540/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 10.2947 - mae: 26.4042 - mape: 10.2947 - val_loss: 13.3158 - val_mae: 30.5406 - val_mape: 13.3158 - lr: 4.0000e-05\n",
      "Epoch 541/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.3493 - mae: 25.1056 - mape: 10.3493 - val_loss: 13.3866 - val_mae: 34.1287 - val_mape: 13.3866 - lr: 4.0000e-05\n",
      "Epoch 542/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.4401 - mae: 27.0144 - mape: 10.4401 - val_loss: 13.4234 - val_mae: 33.8063 - val_mape: 13.4234 - lr: 4.0000e-05\n",
      "Epoch 543/1000\n",
      "83/92 [==========================>...] - ETA: 0s - loss: 10.4481 - mae: 25.4064 - mape: 10.4481INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 10.4720 - mae: 25.0995 - mape: 10.4720 - val_loss: 13.3097 - val_mae: 31.0417 - val_mape: 13.3097 - lr: 4.0000e-05\n",
      "Epoch 544/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 10.6539 - mae: 26.5178 - mape: 10.6539 - val_loss: 13.3936 - val_mae: 35.2402 - val_mape: 13.3936 - lr: 4.0000e-05\n",
      "Epoch 545/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.4569 - mae: 27.7516 - mape: 10.4569 - val_loss: 13.4607 - val_mae: 30.2490 - val_mape: 13.4607 - lr: 4.0000e-05\n",
      "Epoch 546/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 10.5823 - mae: 24.2976 - mape: 10.5823 - val_loss: 13.3693 - val_mae: 30.5636 - val_mape: 13.3693 - lr: 4.0000e-05\n",
      "Epoch 547/1000\n",
      "92/92 [==============================] - 1s 5ms/step - loss: 10.6208 - mae: 27.0758 - mape: 10.6208 - val_loss: 13.3876 - val_mae: 35.5182 - val_mape: 13.3876 - lr: 4.0000e-05\n",
      "Epoch 548/1000\n",
      "89/92 [============================>.] - ETA: 0s - loss: 10.4505 - mae: 26.1876 - mape: 10.4505INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 10.4434 - mae: 26.1261 - mape: 10.4434 - val_loss: 13.3026 - val_mae: 33.2224 - val_mape: 13.3026 - lr: 4.0000e-05\n",
      "Epoch 549/1000\n",
      "92/92 [==============================] - 1s 8ms/step - loss: 10.4771 - mae: 27.3868 - mape: 10.4771 - val_loss: 13.5540 - val_mae: 32.2338 - val_mape: 13.5540 - lr: 4.0000e-05\n",
      "Epoch 550/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.4293 - mae: 25.0865 - mape: 10.4293 - val_loss: 13.3407 - val_mae: 31.1944 - val_mape: 13.3407 - lr: 4.0000e-05\n",
      "Epoch 551/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 10.5564 - mae: 23.8580 - mape: 10.5564 - val_loss: 13.5476 - val_mae: 35.2686 - val_mape: 13.5476 - lr: 4.0000e-05\n",
      "Epoch 552/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.4426 - mae: 25.4838 - mape: 10.4426 - val_loss: 13.6855 - val_mae: 32.8461 - val_mape: 13.6855 - lr: 4.0000e-05\n",
      "Epoch 553/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.6240 - mae: 28.9259 - mape: 10.6240 - val_loss: 13.3619 - val_mae: 29.0534 - val_mape: 13.3619 - lr: 4.0000e-05\n",
      "Epoch 554/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.4586 - mae: 26.5260 - mape: 10.4586 - val_loss: 13.3149 - val_mae: 32.6428 - val_mape: 13.3149 - lr: 4.0000e-05\n",
      "Epoch 555/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.3906 - mae: 24.7409 - mape: 10.3906 - val_loss: 13.4120 - val_mae: 32.1597 - val_mape: 13.4120 - lr: 4.0000e-05\n",
      "Epoch 556/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.3392 - mae: 24.6247 - mape: 10.3392 - val_loss: 13.4057 - val_mae: 29.1351 - val_mape: 13.4057 - lr: 4.0000e-05\n",
      "Epoch 557/1000\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 10.2352 - mae: 22.4407 - mape: 10.2352INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 10.2534 - mae: 22.8336 - mape: 10.2534 - val_loss: 13.2836 - val_mae: 31.5369 - val_mape: 13.2836 - lr: 4.0000e-05\n",
      "Epoch 558/1000\n",
      "92/92 [==============================] - 1s 8ms/step - loss: 10.4044 - mae: 27.0619 - mape: 10.4044 - val_loss: 13.3547 - val_mae: 33.6334 - val_mape: 13.3547 - lr: 4.0000e-05\n",
      "Epoch 559/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.2774 - mae: 25.3748 - mape: 10.2774 - val_loss: 13.4278 - val_mae: 31.3475 - val_mape: 13.4278 - lr: 4.0000e-05\n",
      "Epoch 560/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.2047 - mae: 24.1761 - mape: 10.2047 - val_loss: 13.3013 - val_mae: 29.4549 - val_mape: 13.3013 - lr: 4.0000e-05\n",
      "Epoch 561/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 10.4153 - mae: 24.9014 - mape: 10.4153 - val_loss: 13.3104 - val_mae: 30.4357 - val_mape: 13.3104 - lr: 4.0000e-05\n",
      "Epoch 562/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 10.3405 - mae: 26.6885 - mape: 10.3405 - val_loss: 13.3583 - val_mae: 31.5124 - val_mape: 13.3583 - lr: 4.0000e-05\n",
      "Epoch 563/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 10.4764 - mae: 25.0137 - mape: 10.4764 - val_loss: 13.3380 - val_mae: 30.3413 - val_mape: 13.3380 - lr: 4.0000e-05\n",
      "Epoch 564/1000\n",
      "89/92 [============================>.] - ETA: 0s - loss: 10.3219 - mae: 26.4066 - mape: 10.3219INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 10.3434 - mae: 26.2226 - mape: 10.3434 - val_loss: 13.2659 - val_mae: 30.3696 - val_mape: 13.2659 - lr: 4.0000e-05\n",
      "Epoch 565/1000\n",
      "87/92 [===========================>..] - ETA: 0s - loss: 10.2742 - mae: 24.0930 - mape: 10.2742INFO:tensorflow:Assets written to: savedmodels\\remove\\assets\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 10.2578 - mae: 23.6975 - mape: 10.2578 - val_loss: 13.2199 - val_mae: 30.1000 - val_mape: 13.2199 - lr: 4.0000e-05\n",
      "Epoch 566/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.1650 - mae: 24.1490 - mape: 10.1650 - val_loss: 13.2702 - val_mae: 29.0613 - val_mape: 13.2702 - lr: 4.0000e-05\n",
      "Epoch 567/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.2947 - mae: 24.3794 - mape: 10.2947 - val_loss: 13.5187 - val_mae: 29.3459 - val_mape: 13.5187 - lr: 4.0000e-05\n",
      "Epoch 568/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.3113 - mae: 24.3507 - mape: 10.3113 - val_loss: 13.2663 - val_mae: 30.5499 - val_mape: 13.2663 - lr: 4.0000e-05\n",
      "Epoch 569/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.4487 - mae: 25.6654 - mape: 10.4487 - val_loss: 13.8009 - val_mae: 29.6512 - val_mape: 13.8009 - lr: 4.0000e-05\n",
      "Epoch 570/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 10.4142 - mae: 26.3059 - mape: 10.4142 - val_loss: 13.2668 - val_mae: 28.6728 - val_mape: 13.2668 - lr: 4.0000e-05\n",
      "Epoch 571/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 10.3094 - mae: 24.7331 - mape: 10.3094 - val_loss: 13.4505 - val_mae: 30.0788 - val_mape: 13.4505 - lr: 4.0000e-05\n",
      "Epoch 572/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 10.1406 - mae: 24.1872 - mape: 10.1406 - val_loss: 13.4715 - val_mae: 29.9096 - val_mape: 13.4715 - lr: 4.0000e-05\n",
      "Epoch 573/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 10.4764 - mae: 25.6605 - mape: 10.4764 - val_loss: 13.2420 - val_mae: 29.8149 - val_mape: 13.2420 - lr: 4.0000e-05\n",
      "Epoch 574/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 10.5714 - mae: 28.1115 - mape: 10.5714 - val_loss: 13.2851 - val_mae: 28.7774 - val_mape: 13.2851 - lr: 4.0000e-05\n",
      "Epoch 575/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 10.6380 - mae: 27.3542 - mape: 10.6380 - val_loss: 13.4329 - val_mae: 29.0072 - val_mape: 13.4329 - lr: 4.0000e-05\n",
      "Epoch 576/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.4298 - mae: 24.4387 - mape: 10.4298 - val_loss: 13.2561 - val_mae: 30.0943 - val_mape: 13.2561 - lr: 4.0000e-05\n",
      "Epoch 577/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.5273 - mae: 25.0520 - mape: 10.5273 - val_loss: 13.3267 - val_mae: 33.7669 - val_mape: 13.3267 - lr: 4.0000e-05\n",
      "Epoch 578/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.5337 - mae: 24.4078 - mape: 10.5337 - val_loss: 13.3367 - val_mae: 32.1821 - val_mape: 13.3367 - lr: 4.0000e-05\n",
      "Epoch 579/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.4181 - mae: 24.2530 - mape: 10.4181 - val_loss: 13.2394 - val_mae: 28.1386 - val_mape: 13.2394 - lr: 4.0000e-05\n",
      "Epoch 580/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.2452 - mae: 25.4559 - mape: 10.2452 - val_loss: 13.3363 - val_mae: 29.5907 - val_mape: 13.3363 - lr: 4.0000e-05\n",
      "Epoch 581/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.5263 - mae: 26.1585 - mape: 10.5263 - val_loss: 13.3492 - val_mae: 30.0663 - val_mape: 13.3492 - lr: 4.0000e-05\n",
      "Epoch 582/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.4930 - mae: 26.6015 - mape: 10.4930 - val_loss: 13.4188 - val_mae: 29.5478 - val_mape: 13.4188 - lr: 4.0000e-05\n",
      "Epoch 583/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.2930 - mae: 25.8179 - mape: 10.2930 - val_loss: 13.3566 - val_mae: 29.4998 - val_mape: 13.3566 - lr: 4.0000e-05\n",
      "Epoch 584/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.1911 - mae: 25.3104 - mape: 10.1911 - val_loss: 13.2585 - val_mae: 30.0874 - val_mape: 13.2585 - lr: 4.0000e-05\n",
      "Epoch 585/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 10.3780 - mae: 25.8647 - mape: 10.3780 - val_loss: 13.3026 - val_mae: 30.2568 - val_mape: 13.3026 - lr: 4.0000e-05\n",
      "Epoch 586/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 10.3305 - mae: 26.1544 - mape: 10.3305 - val_loss: 13.2711 - val_mae: 28.5838 - val_mape: 13.2711 - lr: 4.0000e-05\n",
      "Epoch 587/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 10.1133 - mae: 24.2358 - mape: 10.1133 - val_loss: 13.3021 - val_mae: 28.9100 - val_mape: 13.3021 - lr: 4.0000e-05\n",
      "Epoch 588/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 10.2860 - mae: 25.2290 - mape: 10.2860 - val_loss: 13.2838 - val_mae: 28.4463 - val_mape: 13.2838 - lr: 4.0000e-05\n",
      "Epoch 589/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.3018 - mae: 26.1653 - mape: 10.3018 - val_loss: 13.4351 - val_mae: 30.8991 - val_mape: 13.4351 - lr: 4.0000e-05\n",
      "Epoch 590/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.4036 - mae: 23.9094 - mape: 10.4036 - val_loss: 14.1749 - val_mae: 28.8989 - val_mape: 14.1749 - lr: 4.0000e-05\n",
      "Epoch 591/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.6334 - mae: 24.3377 - mape: 10.6334 - val_loss: 13.3751 - val_mae: 31.4359 - val_mape: 13.3751 - lr: 4.0000e-05\n",
      "Epoch 592/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.0505 - mae: 24.2652 - mape: 10.0505 - val_loss: 13.3255 - val_mae: 28.5442 - val_mape: 13.3255 - lr: 4.0000e-05\n",
      "Epoch 593/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.3990 - mae: 25.2982 - mape: 10.3990 - val_loss: 13.3171 - val_mae: 31.5100 - val_mape: 13.3171 - lr: 4.0000e-05\n",
      "Epoch 594/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.1926 - mae: 24.2697 - mape: 10.1926 - val_loss: 13.5177 - val_mae: 37.8496 - val_mape: 13.5177 - lr: 4.0000e-05\n",
      "Epoch 595/1000\n",
      "89/92 [============================>.] - ETA: 0s - loss: 10.1788 - mae: 25.5913 - mape: 10.1788\n",
      "Epoch 595: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.1632 - mae: 25.9208 - mape: 10.1632 - val_loss: 13.3494 - val_mae: 30.0021 - val_mape: 13.3494 - lr: 4.0000e-05\n",
      "Epoch 596/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.5448 - mae: 26.4516 - mape: 10.5448 - val_loss: 13.2759 - val_mae: 30.5642 - val_mape: 13.2759 - lr: 8.0000e-06\n",
      "Epoch 597/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.2240 - mae: 25.5438 - mape: 10.2240 - val_loss: 13.2674 - val_mae: 30.1458 - val_mape: 13.2674 - lr: 8.0000e-06\n",
      "Epoch 598/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.0564 - mae: 25.2467 - mape: 10.0564 - val_loss: 13.2462 - val_mae: 29.7875 - val_mape: 13.2462 - lr: 8.0000e-06\n",
      "Epoch 599/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.1889 - mae: 25.6511 - mape: 10.1889 - val_loss: 13.2768 - val_mae: 30.4883 - val_mape: 13.2768 - lr: 8.0000e-06\n",
      "Epoch 600/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.3159 - mae: 27.0004 - mape: 10.3159 - val_loss: 13.2671 - val_mae: 30.5081 - val_mape: 13.2671 - lr: 8.0000e-06\n",
      "Epoch 601/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 10.1603 - mae: 26.8662 - mape: 10.1603 - val_loss: 13.2561 - val_mae: 30.1647 - val_mape: 13.2561 - lr: 8.0000e-06\n",
      "Epoch 602/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 10.3538 - mae: 23.6237 - mape: 10.3538 - val_loss: 13.2579 - val_mae: 30.4403 - val_mape: 13.2579 - lr: 8.0000e-06\n",
      "Epoch 603/1000\n",
      "92/92 [==============================] - 1s 5ms/step - loss: 10.2679 - mae: 27.4655 - mape: 10.2679 - val_loss: 13.2758 - val_mae: 31.0758 - val_mape: 13.2758 - lr: 8.0000e-06\n",
      "Epoch 604/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.1820 - mae: 26.2089 - mape: 10.1820 - val_loss: 13.2747 - val_mae: 30.8072 - val_mape: 13.2747 - lr: 8.0000e-06\n",
      "Epoch 605/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.1995 - mae: 25.7059 - mape: 10.1995 - val_loss: 13.2733 - val_mae: 31.2165 - val_mape: 13.2733 - lr: 8.0000e-06\n",
      "Epoch 606/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.3159 - mae: 26.3453 - mape: 10.3159 - val_loss: 13.2819 - val_mae: 31.2438 - val_mape: 13.2819 - lr: 8.0000e-06\n",
      "Epoch 607/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 10.3270 - mae: 24.5281 - mape: 10.3270 - val_loss: 13.2919 - val_mae: 31.3545 - val_mape: 13.2919 - lr: 8.0000e-06\n",
      "Epoch 608/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 10.1656 - mae: 24.1070 - mape: 10.1656 - val_loss: 13.3560 - val_mae: 31.4378 - val_mape: 13.3560 - lr: 8.0000e-06\n",
      "Epoch 609/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 10.0376 - mae: 24.0312 - mape: 10.0376 - val_loss: 13.2863 - val_mae: 30.6331 - val_mape: 13.2863 - lr: 8.0000e-06\n",
      "Epoch 610/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.3154 - mae: 23.4460 - mape: 10.3154 - val_loss: 13.2826 - val_mae: 30.4077 - val_mape: 13.2826 - lr: 8.0000e-06\n",
      "Epoch 611/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.0492 - mae: 26.0211 - mape: 10.0492 - val_loss: 13.2826 - val_mae: 30.3635 - val_mape: 13.2826 - lr: 8.0000e-06\n",
      "Epoch 612/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.1791 - mae: 21.6903 - mape: 10.1791 - val_loss: 13.3076 - val_mae: 30.8884 - val_mape: 13.3076 - lr: 8.0000e-06\n",
      "Epoch 613/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.1988 - mae: 25.4723 - mape: 10.1988 - val_loss: 13.2531 - val_mae: 29.3228 - val_mape: 13.2531 - lr: 8.0000e-06\n",
      "Epoch 614/1000\n",
      "92/92 [==============================] - 1s 8ms/step - loss: 10.3157 - mae: 25.4898 - mape: 10.3157 - val_loss: 13.2623 - val_mae: 30.2347 - val_mape: 13.2623 - lr: 8.0000e-06\n",
      "Epoch 615/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 10.3945 - mae: 25.0093 - mape: 10.3945 - val_loss: 13.2544 - val_mae: 30.3613 - val_mape: 13.2544 - lr: 8.0000e-06\n",
      "Epoch 616/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 10.1331 - mae: 23.0998 - mape: 10.1331 - val_loss: 13.2632 - val_mae: 29.5503 - val_mape: 13.2632 - lr: 8.0000e-06\n",
      "Epoch 617/1000\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 10.2165 - mae: 25.1345 - mape: 10.2165 - val_loss: 13.2797 - val_mae: 30.1363 - val_mape: 13.2797 - lr: 8.0000e-06\n",
      "Epoch 618/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 10.1416 - mae: 24.3386 - mape: 10.1416 - val_loss: 13.2770 - val_mae: 29.3457 - val_mape: 13.2770 - lr: 8.0000e-06\n",
      "Epoch 619/1000\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 10.0195 - mae: 25.8563 - mape: 10.0195 - val_loss: 13.2434 - val_mae: 30.1078 - val_mape: 13.2434 - lr: 8.0000e-06\n",
      "Epoch 620/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.1901 - mae: 24.4883 - mape: 10.1901 - val_loss: 13.2467 - val_mae: 30.0095 - val_mape: 13.2467 - lr: 8.0000e-06\n",
      "Epoch 621/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 9.9203 - mae: 24.4368 - mape: 9.9203 - val_loss: 13.2561 - val_mae: 29.5397 - val_mape: 13.2561 - lr: 8.0000e-06\n",
      "Epoch 622/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.5264 - mae: 26.8690 - mape: 10.5264 - val_loss: 13.2501 - val_mae: 29.8025 - val_mape: 13.2501 - lr: 8.0000e-06\n",
      "Epoch 623/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.2865 - mae: 25.1345 - mape: 10.2865 - val_loss: 13.2688 - val_mae: 29.9417 - val_mape: 13.2688 - lr: 8.0000e-06\n",
      "Epoch 624/1000\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.1345 - mae: 25.2477 - mape: 10.1345 - val_loss: 13.2687 - val_mae: 31.1270 - val_mape: 13.2687 - lr: 8.0000e-06\n",
      "Epoch 625/1000\n",
      "91/92 [============================>.] - ETA: 0s - loss: 10.2256 - mae: 25.6993 - mape: 10.2256\n",
      "Epoch 625: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 10.2291 - mae: 25.6500 - mape: 10.2291 - val_loss: 13.2961 - val_mae: 30.5348 - val_mape: 13.2961 - lr: 8.0000e-06\n",
      "Epoch 625: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Fit with callbacks\n",
    "history = model.fit(xtraint, ytraint, validation_data=(xtestt, ytestt), epochs=1000, verbose=1,\n",
    "                    callbacks=get_callbacks('savedmodels/remove',\n",
    "                                            patience=60,\n",
    "                                            lr_factor=0.2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 4ms/step\n",
      "Mean absolute error: 30.83456039428711\n",
      "Mean percentage error: 21.921464920043945\n"
     ]
    }
   ],
   "source": [
    "#Test new model\n",
    "preds = model.predict(xtestt)\n",
    "\n",
    "mae = tf.metrics.mean_absolute_error(y_true=ytestt, y_pred=preds.squeeze()).numpy()\n",
    "print(f\"Mean absolute error: {mae}\")\n",
    "\n",
    "mape = tf.metrics.mape(y_true=ytestt, y_pred=preds.squeeze())\n",
    "print(f\"Mean percentage error: {mape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot learning rate\n",
    "plt.plot(history.history[\"lr\"])\n",
    "plt.title('model learning rate')\n",
    "plt.ylabel('learning rate')\n",
    "plt.xlabel('epoch')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot mae\n",
    "plt.plot(history.history[\"val_mape\"])\n",
    "plt.title('model mean absolute error')\n",
    "plt.ylabel('mae')\n",
    "plt.xlabel('epoch')\n",
    "plt.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d217d37ed9247659b2812203162b84cdb779e33cccd1fe199abf14cba0e180e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
