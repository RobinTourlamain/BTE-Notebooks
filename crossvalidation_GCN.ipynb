{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd;\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras_tuner as kt\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint\n",
    "from spektral.data.utils import to_tf_signature, prepend_none\n",
    "from spektral.data import DisjointLoader, Dataset\n",
    "from spektral.transforms import GCNFilter, NormalizeAdj\n",
    "import scipy.sparse\n",
    "from spektral.data.graph import Graph\n",
    "from keras.layers import Dense, concatenate, Dropout\n",
    "from keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('thingi10k_data.csv', index_col=0)\n",
    "adjpath = '../Data/stl/adjacency_stl_simplified/'\n",
    "cloudpath = '../Data/stl/nodefeatures_stl_simplified/'\n",
    "edgepath = '../Data/stl/edgefeaturesmatrix_stl_simplified/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data, drop unnecessary\n",
    "data = pd.read_csv('thingi10k_data.csv', index_col=0)\n",
    "\n",
    "df = data.copy()\n",
    "df.drop(\"source\", axis=1, inplace=True)\n",
    "df.drop(\"model_name\", axis=1, inplace=True)\n",
    "\n",
    "#Extract build times\n",
    "build_times = df[\"build_time\"]\n",
    "df.drop(\"build_time\", axis=1,inplace=True)\n",
    "\n",
    "#Make pipeline\n",
    "def log_transform(x):\n",
    "    return np.log(x + 1)\n",
    "\n",
    "def inv_log_transform(x):\n",
    "    return np.exp(x) - 1 \n",
    "\n",
    "logtransformer = FunctionTransformer(func=log_transform, inverse_func=inv_log_transform, check_inverse=False)\n",
    "pipe = Pipeline(steps=[ ('logtransformer', logtransformer)])\n",
    "\n",
    "#Log transform\n",
    "transformed = pipe.fit_transform(df)\n",
    "df = np.asarray(transformed)\n",
    "build_times = np.asarray(build_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, centroids, dev, index, **kwargs):\n",
    "        self.centroids = centroids\n",
    "        self.dev = dev\n",
    "        self.index = index\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self):\n",
    "        output = []\n",
    "        for i in self.index:\n",
    "            if(os.path.isfile(adjpath + f'{features[\"model_name\"][i]}.npz')):\n",
    "                point_cloud = np.load(cloudpath + f'{features[\"model_name\"][i]}.npy').reshape(-1,3)\n",
    "                edgefeat = scipy.sparse.load_npz(edgepath + f'{features[\"model_name\"][i]}.npz')\n",
    "                output.append(\n",
    "                    Graph(x=((point_cloud-self.centroids)/self.dev), a=edgefeat, y=features[\"build_time\"][i])\n",
    "                )\n",
    "            else:\n",
    "                print(f'object {i} missing!')\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_graphs(trainindex, testindex):\n",
    "    print(\"Getting split...\")\n",
    "    coords = np.empty((1,3))\n",
    "\n",
    "    for i in trainindex:\n",
    "        if(os.path.isfile(adjpath + f'{features[\"model_name\"][i]}.npz')):\n",
    "            point_cloud = np.load(cloudpath + f'{features[\"model_name\"][i]}.npy').reshape(-1,3)\n",
    "            coords = np.concatenate((coords,point_cloud))\n",
    "            \n",
    "\n",
    "    coords = np.delete(coords,0,0)\n",
    "    centroids = np.mean(coords,0)\n",
    "    coordscentered = coords - centroids\n",
    "    dev = np.max(np.sqrt(np.sum(coordscentered**2,axis=-1) / (trainindex.shape[0] - 1)))\n",
    "\n",
    "    train = MyDataset(centroids, dev, trainindex)\n",
    "    test = MyDataset(centroids, dev, testindex)\n",
    "    validation = MyDataset(centroids, dev, range(3478,3661))\n",
    "    train.apply(NormalizeAdj())\n",
    "    test.apply(NormalizeAdj())\n",
    "    validation.apply(NormalizeAdj())\n",
    "    train = DisjointLoader(train, batch_size=32)\n",
    "    test = DisjointLoader(test, batch_size=32)\n",
    "    validation = DisjointLoader(validation, batch_size=32)\n",
    "    \n",
    "    return train, test, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define callbacks\n",
    "def get_callbacks(weights_file, patience, lr_factor):\n",
    "  return [\n",
    "      # Only save the weights that correspond to the minimum mape.\n",
    "      ModelCheckpoint(filepath= weights_file,\n",
    "                      monitor=\"val_mape\", \n",
    "                      mode=\"min\",\n",
    "                      save_best_only=True, \n",
    "                      save_weights_only=False),\n",
    "      # If val_loss doesn't improve for a number of epochs set with 'patience' var \n",
    "      # training will stop to avoid overfitting.    \n",
    "      EarlyStopping(monitor=\"val_loss\",\n",
    "                    mode=\"min\",\n",
    "                    patience = patience,\n",
    "                    verbose=1),\n",
    "      # Learning rate is reduced by 'lr_factor' if val_loss stagnates\n",
    "      # for a number of epochs set with 'patience/2' var.     \n",
    "      ReduceLROnPlateau(monitor=\"val_loss\", mode=\"min\",\n",
    "                        factor=lr_factor, min_lr=1e-8, patience=patience//2, verbose=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#util\n",
    "def reset_model(model):\n",
    "    import keras.backend as K\n",
    "    session = K.get_session()\n",
    "    for layer in model.layers: \n",
    "        if hasattr(layer, 'kernel.initializer') and layer.trainable: \n",
    "            layer.kernel.initializer.run(session=session)\n",
    "        if hasattr(layer, 'bias.initializer') and layer.trainable:\n",
    "            layer.bias.initializer.run(session=session)\n",
    "\n",
    "def make_trainable(model):\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation\n",
    "def cross_validate(model, dir):\n",
    "    loss_per_fold = []\n",
    "    mae_per_fold = []\n",
    "    mape_per_fold = []\n",
    "    validation_per_fold = []\n",
    "    batch_size = 32\n",
    "    verbosity = 1\n",
    "    no_epochs = 1000\n",
    "    kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "\n",
    "    print('multi-input')\n",
    "    fold_no = 1\n",
    "    for train, test in kfold.split(df[0:3478], build_times[0:3478]):\n",
    "\n",
    "        reset_model(model)\n",
    "        trainloader, testloader, validationloader = get_split_graphs(train, test)\n",
    "\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "        model.compile(loss='mape',\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), #0.001\n",
    "            metrics=['mae','mape']\n",
    "        )\n",
    "        history = model.fit(trainloader.load(), validation_data=testloader.load(), validation_steps=testloader.steps_per_epoch, steps_per_epoch=trainloader.steps_per_epoch,\n",
    "            batch_size=batch_size,\n",
    "            epochs=no_epochs,\n",
    "            verbose=verbosity,\n",
    "            callbacks=get_callbacks(f'{dir}_{fold_no}',\n",
    "                                        patience=60,\n",
    "                                        lr_factor=0.3))\n",
    "        model = keras.models.load_model(f\"{dir}_{fold_no}\")\n",
    "    \n",
    "        # Generate generalization metrics\n",
    "        valscores = model.evaluate(validationloader.load(), steps=validationloader.steps_per_epoch, batch_size=32, verbose=0)\n",
    "        validation_per_fold.append(valscores)\n",
    "        scores = model.evaluate(testloader.load(), steps=testloader.steps_per_epoch, batch_size=32, verbose=0)\n",
    "        print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]}; {model.metrics_names[2]} of {scores[2]}%;')\n",
    "        loss_per_fold.append(scores[0])\n",
    "        mae_per_fold.append(scores[1])\n",
    "        mape_per_fold.append(scores[2])\n",
    "\n",
    "        # Increase fold number\n",
    "        fold_no = fold_no + 1\n",
    "\n",
    "    return loss_per_fold, mae_per_fold, mape_per_fold, validation_per_fold\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print scores\n",
    "def print_scores(loss_per_fold, mae_per_fold, mape_per_fold, validation_per_fold):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('Score per fold')\n",
    "    for i in range(0, len(loss_per_fold)):\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Mean average error: {mae_per_fold[i]}% - Mean percentage error: {mape_per_fold[i]}%')\n",
    "        print(f'    Score on unseen data: Loss: {validation_per_fold[i][0]} - Mean average error: {validation_per_fold[i][1]}% - Mean percentage error: {validation_per_fold[i][2]}%')\n",
    "        print('------------------------------------------------------------------------')\n",
    "    print('Average scores for all folds:')\n",
    "    print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "    print(f'> Mean average error: {np.mean(mae_per_fold)}')\n",
    "    print(f'> Mean percentage error: {np.mean(mape_per_fold)}')\n",
    "    print(f'> Unseen Loss: {np.mean(np.asarray(validation_per_fold)[:,0])}')\n",
    "    print(f'> Unseen Mean average error: {np.mean(np.asarray(validation_per_fold)[:,1])}')\n",
    "    print(f'> Unseen Mean percentage error: {np.mean(np.asarray(validation_per_fold)[:,2])}')\n",
    "    print('------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-input\n",
      "Getting split...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\spektral\\data\\utils.py:221: UserWarning: you are shuffling a 'MyDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - ETA: 0s - loss: 44.7116 - mae: 203.0311 - mape: 40.6507"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) args_0 with unsupported characters which will be renamed to args_0_4 in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: crossvalidationmodels\\GCN_1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: crossvalidationmodels\\GCN_1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 9s 87ms/step - loss: 44.7116 - mae: 203.0311 - mape: 40.6507 - val_loss: 43.7918 - val_mae: 169.8668 - val_mape: 39.7309 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      " 1/87 [..............................] - ETA: 7s - loss: 49.2595 - mae: 79.9885 - mape: 45.1985"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\spektral\\data\\utils.py:221: UserWarning: you are shuffling a 'MyDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - ETA: 0s - loss: 42.1963 - mae: 200.5422 - mape: 38.1405"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) args_0 with unsupported characters which will be renamed to args_0_4 in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: crossvalidationmodels\\GCN_1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: crossvalidationmodels\\GCN_1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 8s 88ms/step - loss: 42.1963 - mae: 200.5422 - mape: 38.1405 - val_loss: 40.9569 - val_mae: 172.0337 - val_mape: 36.8970 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      " 1/87 [..............................] - ETA: 6s - loss: 52.0761 - mae: 502.2348 - mape: 48.0162"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\spektral\\data\\utils.py:221: UserWarning: you are shuffling a 'MyDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 7s 76ms/step - loss: 42.3710 - mae: 201.0967 - mape: 38.3200 - val_loss: 43.1083 - val_mae: 168.8712 - val_mape: 39.0625 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "87/87 [==============================] - 7s 75ms/step - loss: 43.3697 - mae: 200.7721 - mape: 39.3239 - val_loss: 47.1695 - val_mae: 175.8879 - val_mape: 43.1213 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "87/87 [==============================] - 7s 75ms/step - loss: 43.4295 - mae: 201.3985 - mape: 39.3712 - val_loss: 42.1675 - val_mae: 172.4078 - val_mape: 38.1016 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 42.5319 - mae: 201.6146 - mape: 38.4635 - val_loss: 42.8498 - val_mae: 169.5627 - val_mape: 38.7719 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 41.8970 - mae: 201.1266 - mape: 37.8291 - val_loss: 42.7930 - val_mae: 170.8400 - val_mape: 38.7213 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 42.9744 - mae: 200.5765 - mape: 38.9012 - val_loss: 43.1174 - val_mae: 171.4279 - val_mape: 39.0432 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 41.8052 - mae: 199.8888 - mape: 37.7297 - val_loss: 43.1498 - val_mae: 172.5860 - val_mape: 39.0746 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "87/87 [==============================] - 6s 74ms/step - loss: 42.2857 - mae: 200.9951 - mape: 38.2055 - val_loss: 42.7012 - val_mae: 169.4078 - val_mape: 38.6150 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 42.8618 - mae: 200.8881 - mape: 38.7767 - val_loss: 42.9895 - val_mae: 173.7388 - val_mape: 38.9137 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "87/87 [==============================] - 7s 75ms/step - loss: 42.0545 - mae: 200.1936 - mape: 37.9756 - val_loss: 42.5562 - val_mae: 170.2603 - val_mape: 38.4740 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 42.3734 - mae: 201.3302 - mape: 38.2943 - val_loss: 43.4649 - val_mae: 169.8349 - val_mape: 39.3770 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "87/87 [==============================] - 7s 75ms/step - loss: 41.4808 - mae: 200.3681 - mape: 37.3909 - val_loss: 43.7654 - val_mae: 171.0088 - val_mape: 39.6742 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 41.9879 - mae: 201.8183 - mape: 37.8936 - val_loss: 43.6078 - val_mae: 171.8992 - val_mape: 39.5082 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "87/87 [==============================] - 7s 75ms/step - loss: 41.7153 - mae: 200.2769 - mape: 37.6109 - val_loss: 46.3680 - val_mae: 179.3732 - val_mape: 42.2696 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 41.9611 - mae: 199.7731 - mape: 37.8497 - val_loss: 47.9119 - val_mae: 169.8265 - val_mape: 43.7975 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "87/87 [==============================] - 7s 75ms/step - loss: 41.7460 - mae: 199.3763 - mape: 37.6201 - val_loss: 47.7843 - val_mae: 177.9684 - val_mape: 43.6680 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 42.4324 - mae: 200.4632 - mape: 38.3089 - val_loss: 43.6391 - val_mae: 169.2824 - val_mape: 39.5119 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "87/87 [==============================] - 7s 75ms/step - loss: 42.8701 - mae: 201.3071 - mape: 38.7412 - val_loss: 43.3110 - val_mae: 169.2486 - val_mape: 39.1806 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 41.9347 - mae: 199.5278 - mape: 37.8064 - val_loss: 44.0559 - val_mae: 174.5095 - val_mape: 39.9323 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "87/87 [==============================] - 7s 75ms/step - loss: 40.9961 - mae: 198.9164 - mape: 36.8706 - val_loss: 44.0740 - val_mae: 174.7871 - val_mape: 39.9517 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "87/87 [==============================] - 7s 75ms/step - loss: 41.6745 - mae: 199.6254 - mape: 37.5490 - val_loss: 43.7447 - val_mae: 172.6909 - val_mape: 39.6149 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 40.8160 - mae: 198.4047 - mape: 36.6870 - val_loss: 44.3356 - val_mae: 169.6183 - val_mape: 40.2057 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "87/87 [==============================] - 7s 75ms/step - loss: 41.0748 - mae: 198.8198 - mape: 36.9462 - val_loss: 42.9774 - val_mae: 171.4108 - val_mape: 38.8502 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "87/87 [==============================] - 7s 75ms/step - loss: 41.4149 - mae: 199.7317 - mape: 37.2756 - val_loss: 43.2449 - val_mae: 173.5148 - val_mape: 39.0983 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "87/87 [==============================] - 6s 75ms/step - loss: 41.4424 - mae: 200.1476 - mape: 37.2940 - val_loss: 46.3656 - val_mae: 170.9797 - val_mape: 42.2115 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "87/87 [==============================] - 6s 75ms/step - loss: 40.5933 - mae: 199.0313 - mape: 36.4461 - val_loss: 44.6696 - val_mae: 172.8358 - val_mape: 40.5183 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "87/87 [==============================] - 7s 75ms/step - loss: 42.0085 - mae: 199.7085 - mape: 37.8527 - val_loss: 45.6686 - val_mae: 169.6673 - val_mape: 41.5128 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 41.0338 - mae: 198.0222 - mape: 36.8805 - val_loss: 45.4813 - val_mae: 169.3368 - val_mape: 41.3189 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 41.0906 - mae: 198.1109 - mape: 36.9283 - val_loss: 44.0275 - val_mae: 170.5975 - val_mape: 39.8652 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "87/87 [==============================] - ETA: 0s - loss: 41.5652 - mae: 199.7950 - mape: 37.4044\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 41.5652 - mae: 199.7950 - mape: 37.4044 - val_loss: 44.2301 - val_mae: 172.1092 - val_mape: 40.0698 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "87/87 [==============================] - 6s 74ms/step - loss: 39.1575 - mae: 197.0064 - mape: 34.9956 - val_loss: 43.8854 - val_mae: 170.4283 - val_mape: 39.7242 - lr: 3.0000e-04\n",
      "Epoch 34/1000\n",
      "87/87 [==============================] - 7s 75ms/step - loss: 38.4882 - mae: 196.6094 - mape: 34.3259 - val_loss: 44.3048 - val_mae: 170.0283 - val_mape: 40.1432 - lr: 3.0000e-04\n",
      "Epoch 35/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 38.9060 - mae: 196.3064 - mape: 34.7454 - val_loss: 44.9649 - val_mae: 168.2467 - val_mape: 40.8044 - lr: 3.0000e-04\n",
      "Epoch 36/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 38.7429 - mae: 196.2979 - mape: 34.5832 - val_loss: 43.3975 - val_mae: 170.8379 - val_mape: 39.2410 - lr: 3.0000e-04\n",
      "Epoch 37/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 37.9449 - mae: 196.2696 - mape: 33.7862 - val_loss: 43.6050 - val_mae: 168.7287 - val_mape: 39.4484 - lr: 3.0000e-04\n",
      "Epoch 38/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 37.9436 - mae: 195.5930 - mape: 33.7869 - val_loss: 43.9567 - val_mae: 170.6596 - val_mape: 39.8017 - lr: 3.0000e-04\n",
      "Epoch 39/1000\n",
      "87/87 [==============================] - 7s 75ms/step - loss: 38.2403 - mae: 196.1837 - mape: 34.0850 - val_loss: 43.2670 - val_mae: 171.0157 - val_mape: 39.1151 - lr: 3.0000e-04\n",
      "Epoch 40/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 37.9571 - mae: 196.2810 - mape: 33.8031 - val_loss: 43.6323 - val_mae: 170.4879 - val_mape: 39.4782 - lr: 3.0000e-04\n",
      "Epoch 41/1000\n",
      "87/87 [==============================] - 7s 75ms/step - loss: 38.0446 - mae: 196.0877 - mape: 33.8919 - val_loss: 42.7967 - val_mae: 168.8365 - val_mape: 38.6460 - lr: 3.0000e-04\n",
      "Epoch 42/1000\n",
      "87/87 [==============================] - 7s 75ms/step - loss: 37.6334 - mae: 195.7889 - mape: 33.4818 - val_loss: 44.0975 - val_mae: 171.1746 - val_mape: 39.9505 - lr: 3.0000e-04\n",
      "Epoch 43/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 38.3308 - mae: 196.5022 - mape: 34.1816 - val_loss: 43.6838 - val_mae: 169.3838 - val_mape: 39.5333 - lr: 3.0000e-04\n",
      "Epoch 44/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 38.0462 - mae: 196.5498 - mape: 33.8960 - val_loss: 43.2158 - val_mae: 169.7458 - val_mape: 39.0656 - lr: 3.0000e-04\n",
      "Epoch 45/1000\n",
      "87/87 [==============================] - 7s 75ms/step - loss: 37.9668 - mae: 196.3813 - mape: 33.8153 - val_loss: 44.1489 - val_mae: 168.0917 - val_mape: 39.9971 - lr: 3.0000e-04\n",
      "Epoch 46/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 37.7676 - mae: 196.3060 - mape: 33.6192 - val_loss: 43.7190 - val_mae: 169.5171 - val_mape: 39.5688 - lr: 3.0000e-04\n",
      "Epoch 47/1000\n",
      "87/87 [==============================] - 7s 75ms/step - loss: 37.6520 - mae: 195.7690 - mape: 33.5022 - val_loss: 44.5135 - val_mae: 169.5821 - val_mape: 40.3667 - lr: 3.0000e-04\n",
      "Epoch 48/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 37.5162 - mae: 196.1441 - mape: 33.3683 - val_loss: 44.4880 - val_mae: 167.9597 - val_mape: 40.3394 - lr: 3.0000e-04\n",
      "Epoch 49/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 38.2274 - mae: 196.6645 - mape: 34.0784 - val_loss: 43.8635 - val_mae: 171.0155 - val_mape: 39.7168 - lr: 3.0000e-04\n",
      "Epoch 50/1000\n",
      "87/87 [==============================] - 6s 74ms/step - loss: 37.6482 - mae: 195.8417 - mape: 33.4996 - val_loss: 44.1163 - val_mae: 167.8018 - val_mape: 39.9660 - lr: 3.0000e-04\n",
      "Epoch 51/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 38.3075 - mae: 196.1706 - mape: 34.1616 - val_loss: 42.8365 - val_mae: 168.4740 - val_mape: 38.6916 - lr: 3.0000e-04\n",
      "Epoch 52/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 37.7384 - mae: 196.2937 - mape: 33.5936 - val_loss: 43.7265 - val_mae: 169.9124 - val_mape: 39.5837 - lr: 3.0000e-04\n",
      "Epoch 53/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 37.7368 - mae: 196.5299 - mape: 33.5932 - val_loss: 43.2910 - val_mae: 169.7657 - val_mape: 39.1488 - lr: 3.0000e-04\n",
      "Epoch 54/1000\n",
      "87/87 [==============================] - 7s 75ms/step - loss: 37.6517 - mae: 195.1964 - mape: 33.5083 - val_loss: 42.6922 - val_mae: 168.9434 - val_mape: 38.5454 - lr: 3.0000e-04\n",
      "Epoch 55/1000\n",
      "87/87 [==============================] - 7s 75ms/step - loss: 38.1336 - mae: 196.1163 - mape: 33.9888 - val_loss: 43.6228 - val_mae: 169.6299 - val_mape: 39.4834 - lr: 3.0000e-04\n",
      "Epoch 56/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 37.0532 - mae: 195.6918 - mape: 32.9141 - val_loss: 44.5471 - val_mae: 169.6395 - val_mape: 40.4035 - lr: 3.0000e-04\n",
      "Epoch 57/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 37.2613 - mae: 195.1845 - mape: 33.1185 - val_loss: 43.6610 - val_mae: 168.7396 - val_mape: 39.5198 - lr: 3.0000e-04\n",
      "Epoch 58/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 36.8244 - mae: 194.3177 - mape: 32.6824 - val_loss: 43.6033 - val_mae: 171.0455 - val_mape: 39.4604 - lr: 3.0000e-04\n",
      "Epoch 59/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 37.7822 - mae: 195.6109 - mape: 33.6387 - val_loss: 43.9711 - val_mae: 170.8490 - val_mape: 39.8300 - lr: 3.0000e-04\n",
      "Epoch 60/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 37.8166 - mae: 195.6501 - mape: 33.6712 - val_loss: 44.0439 - val_mae: 168.9617 - val_mape: 39.8940 - lr: 3.0000e-04\n",
      "Epoch 61/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 37.6799 - mae: 195.4115 - mape: 33.5268 - val_loss: 44.5097 - val_mae: 168.3091 - val_mape: 40.3577 - lr: 3.0000e-04\n",
      "Epoch 62/1000\n",
      "87/87 [==============================] - ETA: 0s - loss: 37.0252 - mae: 195.5135 - mape: 32.8770\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 37.0252 - mae: 195.5135 - mape: 32.8770 - val_loss: 44.5178 - val_mae: 167.9098 - val_mape: 40.3692 - lr: 3.0000e-04\n",
      "Epoch 62: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\spektral\\data\\utils.py:221: UserWarning: you are shuffling a 'MyDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 1: loss of 40.95688247680664; mae of 172.03375244140625; mape of 36.897010803222656%;\n",
      "Getting split...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\spektral\\data\\utils.py:221: UserWarning: you are shuffling a 'MyDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - ETA: 0s - loss: 44.4619 - mae: 202.2647 - mape: 40.4084"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) args_0 with unsupported characters which will be renamed to args_0_4 in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: crossvalidationmodels\\GCN_2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: crossvalidationmodels\\GCN_2\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 8s 87ms/step - loss: 44.4619 - mae: 202.2647 - mape: 40.4084 - val_loss: 43.6590 - val_mae: 168.2351 - val_mape: 39.6034 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      " 1/87 [..............................] - ETA: 7s - loss: 50.4378 - mae: 360.5072 - mape: 46.3821"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\spektral\\data\\utils.py:221: UserWarning: you are shuffling a 'MyDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - ETA: 0s - loss: 43.4560 - mae: 201.5322 - mape: 39.3905"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) args_0 with unsupported characters which will be renamed to args_0_4 in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: crossvalidationmodels\\GCN_2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: crossvalidationmodels\\GCN_2\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 8s 91ms/step - loss: 43.4560 - mae: 201.5322 - mape: 39.3905 - val_loss: 39.2916 - val_mae: 165.2457 - val_mape: 35.2344 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      " 1/87 [..............................] - ETA: 7s - loss: 43.8430 - mae: 69.7069 - mape: 39.7857"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\spektral\\data\\utils.py:221: UserWarning: you are shuffling a 'MyDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 7s 78ms/step - loss: 42.8472 - mae: 202.1337 - mape: 38.7865 - val_loss: 39.7288 - val_mae: 166.3784 - val_mape: 35.6641 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 42.7309 - mae: 202.2215 - mape: 38.6670 - val_loss: 41.9181 - val_mae: 170.1150 - val_mape: 37.8630 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 41.7406 - mae: 201.6278 - mape: 37.6733 - val_loss: 42.3455 - val_mae: 167.7996 - val_mape: 38.2780 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 43.5627 - mae: 201.8232 - mape: 39.4924 - val_loss: 41.3152 - val_mae: 165.2830 - val_mape: 37.2419 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 42.0693 - mae: 200.6381 - mape: 37.9909 - val_loss: 40.9222 - val_mae: 166.2682 - val_mape: 36.8428 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 42.2048 - mae: 201.2012 - mape: 38.1300 - val_loss: 41.0742 - val_mae: 166.0138 - val_mape: 36.9992 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 42.2422 - mae: 202.3616 - mape: 38.1662 - val_loss: 41.0076 - val_mae: 165.7184 - val_mape: 36.9316 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 42.6238 - mae: 200.9457 - mape: 38.5407 - val_loss: 41.4095 - val_mae: 165.9101 - val_mape: 37.3211 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 43.3306 - mae: 200.8240 - mape: 39.2302 - val_loss: 42.7667 - val_mae: 165.5452 - val_mape: 38.6644 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 42.7444 - mae: 201.1926 - mape: 38.6375 - val_loss: 40.9101 - val_mae: 165.0519 - val_mape: 36.7967 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 41.5588 - mae: 199.2973 - mape: 37.4381 - val_loss: 42.2600 - val_mae: 167.5456 - val_mape: 38.1350 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 41.7887 - mae: 200.7335 - mape: 37.6638 - val_loss: 41.7873 - val_mae: 164.8990 - val_mape: 37.6578 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 41.6966 - mae: 199.8120 - mape: 37.5760 - val_loss: 42.4275 - val_mae: 163.4022 - val_mape: 38.3010 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 41.9116 - mae: 199.2003 - mape: 37.7927 - val_loss: 42.1270 - val_mae: 167.3086 - val_mape: 38.0181 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 41.5045 - mae: 199.7308 - mape: 37.3829 - val_loss: 43.4940 - val_mae: 164.5912 - val_mape: 39.3681 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 41.7422 - mae: 198.2268 - mape: 37.6093 - val_loss: 43.2199 - val_mae: 164.4901 - val_mape: 39.0822 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 42.7651 - mae: 201.0006 - mape: 38.6325 - val_loss: 42.9040 - val_mae: 166.7860 - val_mape: 38.7761 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 41.6984 - mae: 199.4020 - mape: 37.5694 - val_loss: 42.3583 - val_mae: 165.8527 - val_mape: 38.2354 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 41.9742 - mae: 199.5781 - mape: 37.8442 - val_loss: 42.2884 - val_mae: 164.7609 - val_mape: 38.1486 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 41.7610 - mae: 199.7835 - mape: 37.6333 - val_loss: 43.7046 - val_mae: 163.1197 - val_mape: 39.5750 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 41.0028 - mae: 199.2110 - mape: 36.8706 - val_loss: 42.2011 - val_mae: 164.0320 - val_mape: 38.0625 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 42.0389 - mae: 199.1728 - mape: 37.9075 - val_loss: 42.0077 - val_mae: 164.3464 - val_mape: 37.8744 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 41.8741 - mae: 200.5398 - mape: 37.7387 - val_loss: 43.2522 - val_mae: 166.0681 - val_mape: 39.1180 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 42.8303 - mae: 199.7951 - mape: 38.6978 - val_loss: 44.2740 - val_mae: 171.2079 - val_mape: 40.1501 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 41.0079 - mae: 199.2039 - mape: 36.8748 - val_loss: 41.8451 - val_mae: 164.3200 - val_mape: 37.7105 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 41.2240 - mae: 198.1108 - mape: 37.0906 - val_loss: 44.2951 - val_mae: 169.5962 - val_mape: 40.1606 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 41.1624 - mae: 199.0603 - mape: 37.0227 - val_loss: 42.5689 - val_mae: 165.2583 - val_mape: 38.4267 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 41.0918 - mae: 199.2805 - mape: 36.9465 - val_loss: 43.8224 - val_mae: 164.2658 - val_mape: 39.6669 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 42.9582 - mae: 200.9336 - mape: 38.7964 - val_loss: 43.8234 - val_mae: 165.8713 - val_mape: 39.6575 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "87/87 [==============================] - ETA: 0s - loss: 41.0997 - mae: 198.5572 - mape: 36.9325\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 41.0997 - mae: 198.5572 - mape: 36.9325 - val_loss: 45.2647 - val_mae: 163.0133 - val_mape: 41.0842 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 40.0355 - mae: 197.1343 - mape: 35.8629 - val_loss: 41.8292 - val_mae: 164.1039 - val_mape: 37.6649 - lr: 3.0000e-04\n",
      "Epoch 34/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 39.3062 - mae: 196.7698 - mape: 35.1439 - val_loss: 43.2696 - val_mae: 162.7962 - val_mape: 39.1068 - lr: 3.0000e-04\n",
      "Epoch 35/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 39.0840 - mae: 196.7286 - mape: 34.9264 - val_loss: 41.7016 - val_mae: 163.8880 - val_mape: 37.5464 - lr: 3.0000e-04\n",
      "Epoch 36/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 39.2082 - mae: 196.3396 - mape: 35.0525 - val_loss: 42.0467 - val_mae: 164.7007 - val_mape: 37.8919 - lr: 3.0000e-04\n",
      "Epoch 37/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 38.8095 - mae: 197.0565 - mape: 34.6527 - val_loss: 42.1827 - val_mae: 163.5160 - val_mape: 38.0276 - lr: 3.0000e-04\n",
      "Epoch 38/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 39.0178 - mae: 196.4022 - mape: 34.8603 - val_loss: 42.5625 - val_mae: 163.3935 - val_mape: 38.4056 - lr: 3.0000e-04\n",
      "Epoch 39/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 39.1755 - mae: 196.4987 - mape: 35.0216 - val_loss: 42.6184 - val_mae: 162.9137 - val_mape: 38.4650 - lr: 3.0000e-04\n",
      "Epoch 40/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 38.4812 - mae: 195.8705 - mape: 34.3288 - val_loss: 42.7295 - val_mae: 162.5054 - val_mape: 38.5754 - lr: 3.0000e-04\n",
      "Epoch 41/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 38.1827 - mae: 196.0921 - mape: 34.0289 - val_loss: 41.4485 - val_mae: 163.1891 - val_mape: 37.2980 - lr: 3.0000e-04\n",
      "Epoch 42/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 38.4986 - mae: 196.1300 - mape: 34.3447 - val_loss: 44.4565 - val_mae: 161.7775 - val_mape: 40.2974 - lr: 3.0000e-04\n",
      "Epoch 43/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 38.1436 - mae: 195.3665 - mape: 33.9909 - val_loss: 42.3308 - val_mae: 161.8680 - val_mape: 38.1768 - lr: 3.0000e-04\n",
      "Epoch 44/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 38.8076 - mae: 197.1852 - mape: 34.6534 - val_loss: 42.6965 - val_mae: 165.0588 - val_mape: 38.5446 - lr: 3.0000e-04\n",
      "Epoch 45/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 38.3089 - mae: 195.9959 - mape: 34.1545 - val_loss: 42.8635 - val_mae: 161.7676 - val_mape: 38.7113 - lr: 3.0000e-04\n",
      "Epoch 46/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 38.1175 - mae: 196.1353 - mape: 33.9671 - val_loss: 43.2225 - val_mae: 162.0260 - val_mape: 39.0691 - lr: 3.0000e-04\n",
      "Epoch 47/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 38.3138 - mae: 196.3436 - mape: 34.1636 - val_loss: 41.3340 - val_mae: 163.2940 - val_mape: 37.1861 - lr: 3.0000e-04\n",
      "Epoch 48/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 38.0494 - mae: 195.8670 - mape: 33.8979 - val_loss: 41.7642 - val_mae: 162.7400 - val_mape: 37.6119 - lr: 3.0000e-04\n",
      "Epoch 49/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 37.8334 - mae: 195.7577 - mape: 33.6834 - val_loss: 41.5302 - val_mae: 163.5371 - val_mape: 37.3811 - lr: 3.0000e-04\n",
      "Epoch 50/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 38.2054 - mae: 195.7430 - mape: 34.0532 - val_loss: 42.2839 - val_mae: 163.1197 - val_mape: 38.1324 - lr: 3.0000e-04\n",
      "Epoch 51/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 38.6193 - mae: 195.8370 - mape: 34.4663 - val_loss: 42.4779 - val_mae: 162.5922 - val_mape: 38.3252 - lr: 3.0000e-04\n",
      "Epoch 52/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 38.1147 - mae: 196.1513 - mape: 33.9636 - val_loss: 42.1184 - val_mae: 162.1925 - val_mape: 37.9709 - lr: 3.0000e-04\n",
      "Epoch 53/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 37.7270 - mae: 194.2031 - mape: 33.5784 - val_loss: 41.8551 - val_mae: 162.9335 - val_mape: 37.7067 - lr: 3.0000e-04\n",
      "Epoch 54/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 38.5727 - mae: 196.4374 - mape: 34.4229 - val_loss: 42.0297 - val_mae: 163.8613 - val_mape: 37.8835 - lr: 3.0000e-04\n",
      "Epoch 55/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 38.0777 - mae: 195.0051 - mape: 33.9265 - val_loss: 42.3241 - val_mae: 161.3694 - val_mape: 38.1711 - lr: 3.0000e-04\n",
      "Epoch 56/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 37.9851 - mae: 195.6023 - mape: 33.8347 - val_loss: 41.8796 - val_mae: 161.4619 - val_mape: 37.7262 - lr: 3.0000e-04\n",
      "Epoch 57/1000\n",
      "87/87 [==============================] - 7s 76ms/step - loss: 37.7860 - mae: 195.5007 - mape: 33.6338 - val_loss: 41.9417 - val_mae: 161.7028 - val_mape: 37.7899 - lr: 3.0000e-04\n",
      "Epoch 58/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 37.6654 - mae: 195.8095 - mape: 33.5128 - val_loss: 41.7287 - val_mae: 162.8255 - val_mape: 37.5768 - lr: 3.0000e-04\n",
      "Epoch 59/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 37.9000 - mae: 195.6467 - mape: 33.7476 - val_loss: 41.8171 - val_mae: 162.3753 - val_mape: 37.6661 - lr: 3.0000e-04\n",
      "Epoch 60/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 37.7601 - mae: 194.8204 - mape: 33.6062 - val_loss: 42.4379 - val_mae: 163.7594 - val_mape: 38.2841 - lr: 3.0000e-04\n",
      "Epoch 61/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 38.0462 - mae: 195.4011 - mape: 33.8891 - val_loss: 41.8052 - val_mae: 162.2180 - val_mape: 37.6493 - lr: 3.0000e-04\n",
      "Epoch 62/1000\n",
      "87/87 [==============================] - ETA: 0s - loss: 37.9242 - mae: 195.2030 - mape: 33.7693\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 37.9242 - mae: 195.2030 - mape: 33.7693 - val_loss: 42.6055 - val_mae: 162.5346 - val_mape: 38.4487 - lr: 3.0000e-04\n",
      "Epoch 62: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\spektral\\data\\utils.py:221: UserWarning: you are shuffling a 'MyDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 2: loss of 39.2916374206543; mae of 165.24569702148438; mape of 35.234371185302734%;\n",
      "Getting split...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\spektral\\data\\utils.py:221: UserWarning: you are shuffling a 'MyDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - ETA: 0s - loss: 43.8057 - mae: 191.2368 - mape: 39.7699"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) args_0 with unsupported characters which will be renamed to args_0_4 in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: crossvalidationmodels\\GCN_3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: crossvalidationmodels\\GCN_3\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 9s 89ms/step - loss: 43.8057 - mae: 191.2368 - mape: 39.7699 - val_loss: 43.6489 - val_mae: 219.7684 - val_mape: 39.6225 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      " 1/87 [..............................] - ETA: 8s - loss: 47.0921 - mae: 77.3245 - mape: 43.0657"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\spektral\\data\\utils.py:221: UserWarning: you are shuffling a 'MyDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - ETA: 0s - loss: 42.8000 - mae: 190.8939 - mape: 38.7650"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) args_0 with unsupported characters which will be renamed to args_0_4 in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: crossvalidationmodels\\GCN_3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: crossvalidationmodels\\GCN_3\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 8s 95ms/step - loss: 42.8000 - mae: 190.8939 - mape: 38.7650 - val_loss: 40.3372 - val_mae: 213.3764 - val_mape: 36.3026 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      " 1/87 [..............................] - ETA: 5s - loss: 38.2404 - mae: 186.8742 - mape: 34.2058"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\spektral\\data\\utils.py:221: UserWarning: you are shuffling a 'MyDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 7s 79ms/step - loss: 42.4025 - mae: 190.5739 - mape: 38.3547 - val_loss: 40.8481 - val_mae: 211.9333 - val_mape: 36.7988 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 41.9783 - mae: 189.8250 - mape: 37.9263 - val_loss: 44.5553 - val_mae: 217.6388 - val_mape: 40.5010 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 42.8225 - mae: 190.9502 - mape: 38.7531 - val_loss: 42.7179 - val_mae: 213.1712 - val_mape: 38.6400 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 42.7750 - mae: 189.5205 - mape: 38.6960 - val_loss: 44.1427 - val_mae: 215.9239 - val_mape: 40.0628 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 43.1214 - mae: 191.2278 - mape: 39.0392 - val_loss: 42.6103 - val_mae: 212.9691 - val_mape: 38.5379 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 42.4685 - mae: 190.3951 - mape: 38.3989 - val_loss: 41.3084 - val_mae: 213.5627 - val_mape: 37.2461 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 42.2453 - mae: 190.4133 - mape: 38.1700 - val_loss: 42.4941 - val_mae: 208.8617 - val_mape: 38.4143 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 41.9840 - mae: 188.7711 - mape: 37.9123 - val_loss: 41.5849 - val_mae: 209.8102 - val_mape: 37.5129 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 41.5940 - mae: 189.8773 - mape: 37.5215 - val_loss: 41.2083 - val_mae: 210.8878 - val_mape: 37.1356 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 42.4801 - mae: 189.6606 - mape: 38.4102 - val_loss: 41.9942 - val_mae: 211.4753 - val_mape: 37.9266 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 41.8565 - mae: 188.9909 - mape: 37.7802 - val_loss: 41.2379 - val_mae: 211.7799 - val_mape: 37.1449 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 42.6755 - mae: 190.1441 - mape: 38.5711 - val_loss: 42.1330 - val_mae: 213.6147 - val_mape: 38.0380 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 43.5876 - mae: 190.6366 - mape: 39.4831 - val_loss: 41.2497 - val_mae: 212.3630 - val_mape: 37.1384 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 41.8249 - mae: 187.5035 - mape: 37.7090 - val_loss: 41.8485 - val_mae: 213.9743 - val_mape: 37.7398 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 42.5003 - mae: 189.9002 - mape: 38.3856 - val_loss: 42.7902 - val_mae: 211.8200 - val_mape: 38.6774 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 43.0521 - mae: 190.4350 - mape: 38.9355 - val_loss: 44.6704 - val_mae: 215.5884 - val_mape: 40.5579 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 42.4489 - mae: 189.4851 - mape: 38.3289 - val_loss: 42.2668 - val_mae: 210.6266 - val_mape: 38.1339 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 41.3723 - mae: 187.6358 - mape: 37.2415 - val_loss: 41.5077 - val_mae: 212.5675 - val_mape: 37.3719 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 42.0842 - mae: 188.4218 - mape: 37.9384 - val_loss: 41.2376 - val_mae: 209.4053 - val_mape: 37.0917 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 41.2012 - mae: 188.5466 - mape: 37.0595 - val_loss: 42.3121 - val_mae: 208.9643 - val_mape: 38.1660 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 42.2426 - mae: 188.7392 - mape: 38.0956 - val_loss: 41.8384 - val_mae: 210.7543 - val_mape: 37.6767 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 41.6140 - mae: 187.9555 - mape: 37.4536 - val_loss: 41.8795 - val_mae: 210.7653 - val_mape: 37.7145 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 41.8580 - mae: 187.9019 - mape: 37.6929 - val_loss: 42.4556 - val_mae: 209.0297 - val_mape: 38.2853 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 41.0346 - mae: 187.3379 - mape: 36.8723 - val_loss: 41.5017 - val_mae: 212.1606 - val_mape: 37.3322 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 40.9984 - mae: 187.9310 - mape: 36.8342 - val_loss: 42.3760 - val_mae: 212.4115 - val_mape: 38.2215 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 41.2514 - mae: 188.4340 - mape: 37.0902 - val_loss: 42.2576 - val_mae: 211.0939 - val_mape: 38.0937 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 41.4364 - mae: 187.4461 - mape: 37.2752 - val_loss: 41.7290 - val_mae: 210.2582 - val_mape: 37.5697 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 40.7118 - mae: 187.7171 - mape: 36.5461 - val_loss: 42.3330 - val_mae: 212.9950 - val_mape: 38.1687 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 40.8903 - mae: 188.4250 - mape: 36.7185 - val_loss: 42.9549 - val_mae: 210.3301 - val_mape: 38.7786 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "87/87 [==============================] - ETA: 0s - loss: 41.5550 - mae: 187.9026 - mape: 37.3789\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 41.5550 - mae: 187.9026 - mape: 37.3789 - val_loss: 42.3574 - val_mae: 210.6845 - val_mape: 38.1671 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 40.0604 - mae: 186.3142 - mape: 35.8708 - val_loss: 41.7420 - val_mae: 211.0712 - val_mape: 37.5563 - lr: 3.0000e-04\n",
      "Epoch 34/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 39.3445 - mae: 186.3459 - mape: 35.1614 - val_loss: 43.0980 - val_mae: 208.1311 - val_mape: 38.9166 - lr: 3.0000e-04\n",
      "Epoch 35/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 38.7216 - mae: 185.6773 - mape: 34.5446 - val_loss: 41.5177 - val_mae: 211.3717 - val_mape: 37.3438 - lr: 3.0000e-04\n",
      "Epoch 36/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 38.6848 - mae: 185.5841 - mape: 34.5116 - val_loss: 41.3066 - val_mae: 209.0602 - val_mape: 37.1335 - lr: 3.0000e-04\n",
      "Epoch 37/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 38.8460 - mae: 185.4207 - mape: 34.6718 - val_loss: 42.1726 - val_mae: 209.2380 - val_mape: 37.9991 - lr: 3.0000e-04\n",
      "Epoch 38/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 38.4290 - mae: 184.7711 - mape: 34.2577 - val_loss: 41.9754 - val_mae: 208.9238 - val_mape: 37.8018 - lr: 3.0000e-04\n",
      "Epoch 39/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 38.4902 - mae: 184.7965 - mape: 34.3181 - val_loss: 41.5278 - val_mae: 209.7224 - val_mape: 37.3549 - lr: 3.0000e-04\n",
      "Epoch 40/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 39.1894 - mae: 185.6018 - mape: 35.0194 - val_loss: 41.7521 - val_mae: 209.5398 - val_mape: 37.5827 - lr: 3.0000e-04\n",
      "Epoch 41/1000\n",
      "87/87 [==============================] - 7s 83ms/step - loss: 37.9776 - mae: 184.6094 - mape: 33.8113 - val_loss: 43.6652 - val_mae: 208.2854 - val_mape: 39.5007 - lr: 3.0000e-04\n",
      "Epoch 42/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 38.5802 - mae: 185.7911 - mape: 34.4162 - val_loss: 42.3303 - val_mae: 208.6754 - val_mape: 38.1609 - lr: 3.0000e-04\n",
      "Epoch 43/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 38.6183 - mae: 185.4689 - mape: 34.4504 - val_loss: 42.3498 - val_mae: 209.2215 - val_mape: 38.1833 - lr: 3.0000e-04\n",
      "Epoch 44/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 37.9738 - mae: 183.7053 - mape: 33.8076 - val_loss: 41.9481 - val_mae: 210.7865 - val_mape: 37.7863 - lr: 3.0000e-04\n",
      "Epoch 45/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 37.8983 - mae: 183.5848 - mape: 33.7315 - val_loss: 42.8923 - val_mae: 208.9551 - val_mape: 38.7224 - lr: 3.0000e-04\n",
      "Epoch 46/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 38.9123 - mae: 184.6561 - mape: 34.7467 - val_loss: 44.4842 - val_mae: 208.7818 - val_mape: 40.3179 - lr: 3.0000e-04\n",
      "Epoch 47/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 38.2712 - mae: 183.6595 - mape: 34.1057 - val_loss: 41.9942 - val_mae: 209.6356 - val_mape: 37.8275 - lr: 3.0000e-04\n",
      "Epoch 48/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 37.9155 - mae: 184.5293 - mape: 33.7533 - val_loss: 41.3449 - val_mae: 210.1127 - val_mape: 37.1851 - lr: 3.0000e-04\n",
      "Epoch 49/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 38.0557 - mae: 184.8801 - mape: 33.8964 - val_loss: 42.1044 - val_mae: 209.2412 - val_mape: 37.9457 - lr: 3.0000e-04\n",
      "Epoch 50/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 37.9217 - mae: 184.2701 - mape: 33.7623 - val_loss: 42.3921 - val_mae: 208.5705 - val_mape: 38.2308 - lr: 3.0000e-04\n",
      "Epoch 51/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 37.6040 - mae: 184.7990 - mape: 33.4431 - val_loss: 41.4564 - val_mae: 209.0360 - val_mape: 37.2982 - lr: 3.0000e-04\n",
      "Epoch 52/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 38.2602 - mae: 184.3366 - mape: 34.1012 - val_loss: 41.9260 - val_mae: 209.0665 - val_mape: 37.7667 - lr: 3.0000e-04\n",
      "Epoch 53/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 37.6743 - mae: 183.9004 - mape: 33.5168 - val_loss: 41.8627 - val_mae: 209.5200 - val_mape: 37.7059 - lr: 3.0000e-04\n",
      "Epoch 54/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 37.8223 - mae: 184.0198 - mape: 33.6635 - val_loss: 43.6353 - val_mae: 208.0063 - val_mape: 39.4718 - lr: 3.0000e-04\n",
      "Epoch 55/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 38.1209 - mae: 184.0676 - mape: 33.9608 - val_loss: 42.7437 - val_mae: 212.0826 - val_mape: 38.5889 - lr: 3.0000e-04\n",
      "Epoch 56/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 37.8556 - mae: 184.0635 - mape: 33.6980 - val_loss: 41.7777 - val_mae: 209.6137 - val_mape: 37.6227 - lr: 3.0000e-04\n",
      "Epoch 57/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 37.8575 - mae: 183.8776 - mape: 33.6986 - val_loss: 42.4311 - val_mae: 211.4609 - val_mape: 38.2726 - lr: 3.0000e-04\n",
      "Epoch 58/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 37.7900 - mae: 184.8764 - mape: 33.6310 - val_loss: 42.5146 - val_mae: 209.8493 - val_mape: 38.3563 - lr: 3.0000e-04\n",
      "Epoch 59/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 37.6369 - mae: 184.7761 - mape: 33.4758 - val_loss: 43.1147 - val_mae: 208.0827 - val_mape: 38.9523 - lr: 3.0000e-04\n",
      "Epoch 60/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 37.6041 - mae: 184.4208 - mape: 33.4429 - val_loss: 42.7477 - val_mae: 208.5442 - val_mape: 38.5857 - lr: 3.0000e-04\n",
      "Epoch 61/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 37.8255 - mae: 183.0791 - mape: 33.6657 - val_loss: 42.9049 - val_mae: 208.3841 - val_mape: 38.7439 - lr: 3.0000e-04\n",
      "Epoch 62/1000\n",
      "87/87 [==============================] - ETA: 0s - loss: 37.1758 - mae: 183.3981 - mape: 33.0175\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 37.1758 - mae: 183.3981 - mape: 33.0175 - val_loss: 42.4188 - val_mae: 208.3238 - val_mape: 38.2644 - lr: 3.0000e-04\n",
      "Epoch 62: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\spektral\\data\\utils.py:221: UserWarning: you are shuffling a 'MyDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 3: loss of 40.337196350097656; mae of 213.3764190673828; mape of 36.30261993408203%;\n",
      "Getting split...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\spektral\\data\\utils.py:221: UserWarning: you are shuffling a 'MyDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - ETA: 0s - loss: 43.6645 - mae: 191.2348 - mape: 39.6535"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) args_0 with unsupported characters which will be renamed to args_0_4 in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: crossvalidationmodels\\GCN_4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: crossvalidationmodels\\GCN_4\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 9s 89ms/step - loss: 43.6645 - mae: 191.2348 - mape: 39.6535 - val_loss: 40.9966 - val_mae: 216.2002 - val_mape: 36.9825 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      " 1/87 [..............................] - ETA: 8s - loss: 43.4459 - mae: 879.7827 - mape: 39.4318"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\spektral\\data\\utils.py:221: UserWarning: you are shuffling a 'MyDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 7s 79ms/step - loss: 42.2162 - mae: 189.2083 - mape: 38.2057 - val_loss: 44.7888 - val_mae: 223.5993 - val_mape: 40.7895 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "87/87 [==============================] - ETA: 0s - loss: 42.1463 - mae: 189.5932 - mape: 38.1378"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) args_0 with unsupported characters which will be renamed to args_0_4 in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: crossvalidationmodels\\GCN_4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: crossvalidationmodels\\GCN_4\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 8s 88ms/step - loss: 42.1463 - mae: 189.5932 - mape: 38.1378 - val_loss: 40.9197 - val_mae: 215.4016 - val_mape: 36.9016 - lr: 0.0010\n",
      "Epoch 4/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\spektral\\data\\utils.py:221: UserWarning: you are shuffling a 'MyDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 7s 80ms/step - loss: 41.6095 - mae: 189.4294 - mape: 37.5954 - val_loss: 41.8610 - val_mae: 219.3670 - val_mape: 37.8475 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 42.6141 - mae: 189.5941 - mape: 38.6021 - val_loss: 44.0939 - val_mae: 216.4114 - val_mape: 40.0848 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 42.2781 - mae: 189.8389 - mape: 38.2617 - val_loss: 43.1165 - val_mae: 220.0111 - val_mape: 39.0913 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 41.5168 - mae: 189.1750 - mape: 37.4798 - val_loss: 42.8077 - val_mae: 218.4096 - val_mape: 38.7735 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 42.2860 - mae: 189.9040 - mape: 38.2402 - val_loss: 41.7667 - val_mae: 215.7108 - val_mape: 37.7247 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 41.1107 - mae: 188.3936 - mape: 37.0802 - val_loss: 42.6904 - val_mae: 219.3620 - val_mape: 38.6634 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 41.9373 - mae: 188.5336 - mape: 37.9140 - val_loss: 43.1901 - val_mae: 213.4796 - val_mape: 39.1517 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 42.4513 - mae: 189.2690 - mape: 38.4167 - val_loss: 43.2999 - val_mae: 222.6428 - val_mape: 39.2640 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 41.7109 - mae: 188.9837 - mape: 37.6798 - val_loss: 42.4585 - val_mae: 218.2382 - val_mape: 38.4316 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 41.4772 - mae: 189.2634 - mape: 37.4405 - val_loss: 43.9187 - val_mae: 221.2753 - val_mape: 39.8808 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 41.3537 - mae: 188.8076 - mape: 37.3225 - val_loss: 42.7445 - val_mae: 217.8568 - val_mape: 38.7133 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 41.1951 - mae: 189.7420 - mape: 37.1651 - val_loss: 42.2940 - val_mae: 216.1421 - val_mape: 38.2680 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 41.6121 - mae: 189.7489 - mape: 37.5783 - val_loss: 42.9272 - val_mae: 215.3037 - val_mape: 38.8841 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 42.5823 - mae: 188.9093 - mape: 38.5345 - val_loss: 46.6068 - val_mae: 225.0135 - val_mape: 42.5586 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 42.2144 - mae: 190.0140 - mape: 38.1644 - val_loss: 46.3844 - val_mae: 212.1391 - val_mape: 42.3368 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 42.3627 - mae: 190.4892 - mape: 38.3205 - val_loss: 47.7337 - val_mae: 214.8400 - val_mape: 43.6909 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 42.0256 - mae: 190.0863 - mape: 37.9857 - val_loss: 43.7739 - val_mae: 220.9240 - val_mape: 39.7328 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 41.6487 - mae: 188.5970 - mape: 37.6039 - val_loss: 44.1898 - val_mae: 218.6394 - val_mape: 40.1455 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 42.0468 - mae: 189.0612 - mape: 37.9940 - val_loss: 44.6734 - val_mae: 212.7186 - val_mape: 40.6147 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 41.7496 - mae: 188.8357 - mape: 37.6903 - val_loss: 43.3382 - val_mae: 220.7857 - val_mape: 39.2823 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 41.4397 - mae: 189.0325 - mape: 37.3768 - val_loss: 44.2575 - val_mae: 218.2801 - val_mape: 40.1967 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 41.0826 - mae: 187.8840 - mape: 37.0178 - val_loss: 45.8495 - val_mae: 223.6158 - val_mape: 41.7734 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 41.8039 - mae: 189.3774 - mape: 37.7295 - val_loss: 45.1302 - val_mae: 223.1507 - val_mape: 41.0650 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 41.0249 - mae: 188.1509 - mape: 36.9532 - val_loss: 46.1230 - val_mae: 220.9081 - val_mape: 42.0553 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 41.3893 - mae: 188.4741 - mape: 37.3106 - val_loss: 43.7370 - val_mae: 215.9752 - val_mape: 39.6531 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 41.7761 - mae: 188.8625 - mape: 37.6851 - val_loss: 44.8758 - val_mae: 221.9908 - val_mape: 40.7844 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 41.5213 - mae: 188.3344 - mape: 37.4203 - val_loss: 44.0657 - val_mae: 222.0779 - val_mape: 39.9628 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 41.0991 - mae: 188.1374 - mape: 36.9964 - val_loss: 44.1316 - val_mae: 217.3382 - val_mape: 40.0273 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 40.9690 - mae: 188.0025 - mape: 36.8641 - val_loss: 44.9735 - val_mae: 219.5763 - val_mape: 40.8659 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "87/87 [==============================] - ETA: 0s - loss: 41.1721 - mae: 187.6060 - mape: 37.0619\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 41.1721 - mae: 187.6060 - mape: 37.0619 - val_loss: 45.0277 - val_mae: 219.2243 - val_mape: 40.9176 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 39.5109 - mae: 187.3147 - mape: 35.4022 - val_loss: 42.9477 - val_mae: 218.0054 - val_mape: 38.8415 - lr: 3.0000e-04\n",
      "Epoch 35/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 38.7590 - mae: 187.1548 - mape: 34.6509 - val_loss: 42.8756 - val_mae: 213.7729 - val_mape: 38.7674 - lr: 3.0000e-04\n",
      "Epoch 36/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 38.7683 - mae: 186.1867 - mape: 34.6618 - val_loss: 43.3749 - val_mae: 215.9635 - val_mape: 39.2710 - lr: 3.0000e-04\n",
      "Epoch 37/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 38.8909 - mae: 187.0362 - mape: 34.7870 - val_loss: 42.9973 - val_mae: 214.8475 - val_mape: 38.8927 - lr: 3.0000e-04\n",
      "Epoch 38/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 38.3661 - mae: 186.1020 - mape: 34.2626 - val_loss: 42.4263 - val_mae: 216.8075 - val_mape: 38.3244 - lr: 3.0000e-04\n",
      "Epoch 39/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 38.1858 - mae: 185.8935 - mape: 34.0842 - val_loss: 43.6148 - val_mae: 217.1637 - val_mape: 39.5137 - lr: 3.0000e-04\n",
      "Epoch 40/1000\n",
      "87/87 [==============================] - 7s 82ms/step - loss: 38.5126 - mae: 186.1439 - mape: 34.4121 - val_loss: 42.6680 - val_mae: 216.5333 - val_mape: 38.5727 - lr: 3.0000e-04\n",
      "Epoch 41/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 38.0382 - mae: 186.2384 - mape: 33.9430 - val_loss: 42.5957 - val_mae: 216.8604 - val_mape: 38.5055 - lr: 3.0000e-04\n",
      "Epoch 42/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 37.9546 - mae: 186.0176 - mape: 33.8634 - val_loss: 44.3623 - val_mae: 214.8950 - val_mape: 40.2689 - lr: 3.0000e-04\n",
      "Epoch 43/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 37.9082 - mae: 185.5710 - mape: 33.8163 - val_loss: 42.7833 - val_mae: 216.8384 - val_mape: 38.6934 - lr: 3.0000e-04\n",
      "Epoch 44/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 37.9807 - mae: 185.6516 - mape: 33.8876 - val_loss: 42.7450 - val_mae: 215.5235 - val_mape: 38.6526 - lr: 3.0000e-04\n",
      "Epoch 45/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 37.8999 - mae: 185.2629 - mape: 33.8105 - val_loss: 42.5760 - val_mae: 214.3475 - val_mape: 38.4863 - lr: 3.0000e-04\n",
      "Epoch 46/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 37.5249 - mae: 185.4709 - mape: 33.4351 - val_loss: 42.3715 - val_mae: 215.3343 - val_mape: 38.2855 - lr: 3.0000e-04\n",
      "Epoch 47/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 37.7331 - mae: 185.6162 - mape: 33.6479 - val_loss: 42.3173 - val_mae: 214.3947 - val_mape: 38.2313 - lr: 3.0000e-04\n",
      "Epoch 48/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 37.8074 - mae: 185.7793 - mape: 33.7200 - val_loss: 43.9080 - val_mae: 215.5363 - val_mape: 39.8216 - lr: 3.0000e-04\n",
      "Epoch 49/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 37.9795 - mae: 185.5840 - mape: 33.8932 - val_loss: 43.3684 - val_mae: 213.5564 - val_mape: 39.2806 - lr: 3.0000e-04\n",
      "Epoch 50/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 38.0007 - mae: 185.6826 - mape: 33.9115 - val_loss: 42.2077 - val_mae: 215.4064 - val_mape: 38.1210 - lr: 3.0000e-04\n",
      "Epoch 51/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 38.1086 - mae: 185.5547 - mape: 34.0184 - val_loss: 43.4593 - val_mae: 217.1363 - val_mape: 39.3699 - lr: 3.0000e-04\n",
      "Epoch 52/1000\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 37.2681 - mae: 184.7909 - mape: 33.1787 - val_loss: 42.8173 - val_mae: 214.4702 - val_mape: 38.7286 - lr: 3.0000e-04\n",
      "Epoch 53/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 37.3706 - mae: 185.2392 - mape: 33.2831 - val_loss: 42.9790 - val_mae: 215.4823 - val_mape: 38.8924 - lr: 3.0000e-04\n",
      "Epoch 54/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 37.7444 - mae: 185.3872 - mape: 33.6594 - val_loss: 44.4810 - val_mae: 214.0119 - val_mape: 40.3935 - lr: 3.0000e-04\n",
      "Epoch 55/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 37.5130 - mae: 185.1202 - mape: 33.4278 - val_loss: 42.2149 - val_mae: 216.0261 - val_mape: 38.1313 - lr: 3.0000e-04\n",
      "Epoch 56/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 37.8247 - mae: 184.9377 - mape: 33.7400 - val_loss: 44.1446 - val_mae: 213.7903 - val_mape: 40.0614 - lr: 3.0000e-04\n",
      "Epoch 57/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 37.7321 - mae: 185.6406 - mape: 33.6482 - val_loss: 43.4763 - val_mae: 218.7996 - val_mape: 39.3954 - lr: 3.0000e-04\n",
      "Epoch 58/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 37.9066 - mae: 185.3482 - mape: 33.8246 - val_loss: 42.6502 - val_mae: 215.7335 - val_mape: 38.5750 - lr: 3.0000e-04\n",
      "Epoch 59/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 37.6269 - mae: 185.7608 - mape: 33.5501 - val_loss: 42.9140 - val_mae: 215.7297 - val_mape: 38.8368 - lr: 3.0000e-04\n",
      "Epoch 60/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 37.1608 - mae: 185.1326 - mape: 33.0813 - val_loss: 44.1569 - val_mae: 212.9983 - val_mape: 40.0776 - lr: 3.0000e-04\n",
      "Epoch 61/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 37.8731 - mae: 185.4154 - mape: 33.7949 - val_loss: 44.3535 - val_mae: 214.8105 - val_mape: 40.2787 - lr: 3.0000e-04\n",
      "Epoch 62/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 37.6314 - mae: 185.0799 - mape: 33.5568 - val_loss: 44.0276 - val_mae: 213.2328 - val_mape: 39.9501 - lr: 3.0000e-04\n",
      "Epoch 63/1000\n",
      "86/87 [============================>.] - ETA: 0s - loss: 37.7415 - mae: 186.7056 - mape: 33.6653\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 37.5861 - mae: 184.7284 - mape: 33.5100 - val_loss: 43.7262 - val_mae: 216.1917 - val_mape: 39.6540 - lr: 3.0000e-04\n",
      "Epoch 63: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\spektral\\data\\utils.py:221: UserWarning: you are shuffling a 'MyDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 4: loss of 40.91968536376953; mae of 215.40164184570312; mape of 36.90154266357422%;\n",
      "Getting split...\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\spektral\\data\\utils.py:221: UserWarning: you are shuffling a 'MyDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/87 [============================>.] - ETA: 0s - loss: 44.2105 - mae: 191.9211 - mape: 40.2116"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) args_0 with unsupported characters which will be renamed to args_0_4 in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: crossvalidationmodels\\GCN_5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: crossvalidationmodels\\GCN_5\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 9s 88ms/step - loss: 44.1708 - mae: 193.0831 - mape: 40.1718 - val_loss: 39.0157 - val_mae: 200.7546 - val_mape: 35.0059 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      " 1/87 [..............................] - ETA: 6s - loss: 45.2871 - mae: 188.0762 - mape: 41.2773"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\spektral\\data\\utils.py:221: UserWarning: you are shuffling a 'MyDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 7s 80ms/step - loss: 44.0188 - mae: 192.5139 - mape: 40.0114 - val_loss: 40.3293 - val_mae: 207.0613 - val_mape: 36.3326 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "87/87 [==============================] - ETA: 0s - loss: 43.3428 - mae: 192.8396 - mape: 39.3311"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) args_0 with unsupported characters which will be renamed to args_0_4 in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: crossvalidationmodels\\GCN_5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: crossvalidationmodels\\GCN_5\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 8s 88ms/step - loss: 43.3428 - mae: 192.8396 - mape: 39.3311 - val_loss: 38.1645 - val_mae: 203.0934 - val_mape: 34.1468 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      " 1/87 [..............................] - ETA: 7s - loss: 38.3567 - mae: 177.3428 - mape: 34.3390"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\spektral\\data\\utils.py:221: UserWarning: you are shuffling a 'MyDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 7s 80ms/step - loss: 43.1315 - mae: 192.4503 - mape: 39.1091 - val_loss: 42.6088 - val_mae: 209.3272 - val_mape: 38.5856 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 43.8589 - mae: 193.6248 - mape: 39.8319 - val_loss: 38.7552 - val_mae: 201.9466 - val_mape: 34.7218 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 42.1804 - mae: 190.5826 - mape: 38.1498 - val_loss: 39.0236 - val_mae: 203.3042 - val_mape: 34.9964 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 43.0015 - mae: 192.3601 - mape: 38.9757 - val_loss: 40.3981 - val_mae: 205.6884 - val_mape: 36.3755 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 42.8114 - mae: 191.1437 - mape: 38.7783 - val_loss: 40.7010 - val_mae: 206.4584 - val_mape: 36.6629 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 43.5726 - mae: 193.5793 - mape: 39.5333 - val_loss: 41.0606 - val_mae: 200.8400 - val_mape: 37.0146 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 42.4242 - mae: 190.7706 - mape: 38.3742 - val_loss: 41.5465 - val_mae: 202.3526 - val_mape: 37.4876 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 43.1361 - mae: 192.1297 - mape: 39.0817 - val_loss: 40.5431 - val_mae: 201.3099 - val_mape: 36.4823 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 42.8109 - mae: 191.2551 - mape: 38.7510 - val_loss: 40.2692 - val_mae: 201.5607 - val_mape: 36.2038 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "87/87 [==============================] - 7s 77ms/step - loss: 42.6169 - mae: 190.4182 - mape: 38.5506 - val_loss: 39.7214 - val_mae: 202.5534 - val_mape: 35.6523 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 42.0662 - mae: 191.5865 - mape: 37.9884 - val_loss: 40.6145 - val_mae: 203.6207 - val_mape: 36.5319 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 43.7868 - mae: 192.5717 - mape: 39.7065 - val_loss: 40.4032 - val_mae: 202.9771 - val_mape: 36.3224 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 41.7627 - mae: 191.1988 - mape: 37.6787 - val_loss: 39.3058 - val_mae: 202.8457 - val_mape: 35.2240 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 42.9650 - mae: 191.6113 - mape: 38.8843 - val_loss: 41.4912 - val_mae: 203.8523 - val_mape: 37.4065 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 42.8881 - mae: 192.2735 - mape: 38.8035 - val_loss: 40.2192 - val_mae: 202.2022 - val_mape: 36.1329 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 42.0830 - mae: 190.9355 - mape: 37.9972 - val_loss: 40.7309 - val_mae: 201.4983 - val_mape: 36.6377 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 42.2413 - mae: 191.5595 - mape: 38.1367 - val_loss: 40.3399 - val_mae: 206.0422 - val_mape: 36.2254 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 41.8030 - mae: 190.4312 - mape: 37.6900 - val_loss: 41.5015 - val_mae: 203.5617 - val_mape: 37.4058 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 42.2291 - mae: 191.8606 - mape: 38.1285 - val_loss: 45.8088 - val_mae: 206.2732 - val_mape: 41.7002 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 43.1404 - mae: 191.5905 - mape: 39.0294 - val_loss: 40.2813 - val_mae: 202.7635 - val_mape: 36.1758 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 41.5302 - mae: 189.4436 - mape: 37.4086 - val_loss: 42.8843 - val_mae: 201.4877 - val_mape: 38.7611 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 41.6450 - mae: 189.5823 - mape: 37.5263 - val_loss: 44.1090 - val_mae: 207.3824 - val_mape: 39.9947 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 41.3502 - mae: 190.4063 - mape: 37.2250 - val_loss: 45.5538 - val_mae: 202.2883 - val_mape: 41.4251 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 42.6903 - mae: 191.5034 - mape: 38.5674 - val_loss: 42.7955 - val_mae: 204.2591 - val_mape: 38.6606 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 41.7323 - mae: 189.6042 - mape: 37.5975 - val_loss: 43.0100 - val_mae: 201.4117 - val_mape: 38.8684 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 42.6224 - mae: 190.5393 - mape: 38.4753 - val_loss: 42.3189 - val_mae: 204.1976 - val_mape: 38.1637 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 42.1763 - mae: 190.2301 - mape: 38.0230 - val_loss: 41.8342 - val_mae: 201.9252 - val_mape: 37.6690 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 41.8201 - mae: 188.8042 - mape: 37.6606 - val_loss: 44.7668 - val_mae: 202.8426 - val_mape: 40.6074 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 42.0702 - mae: 189.4379 - mape: 37.9030 - val_loss: 43.3154 - val_mae: 202.1736 - val_mape: 39.1457 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "87/87 [==============================] - ETA: 0s - loss: 42.5435 - mae: 189.6395 - mape: 38.3706\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 42.5435 - mae: 189.6395 - mape: 38.3706 - val_loss: 42.5792 - val_mae: 202.7206 - val_mape: 38.4150 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 40.0151 - mae: 187.7714 - mape: 35.8506 - val_loss: 42.9589 - val_mae: 201.2508 - val_mape: 38.7925 - lr: 3.0000e-04\n",
      "Epoch 35/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 39.7770 - mae: 188.2920 - mape: 35.6147 - val_loss: 42.0050 - val_mae: 202.6529 - val_mape: 37.8449 - lr: 3.0000e-04\n",
      "Epoch 36/1000\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 39.4853 - mae: 186.8276 - mape: 35.3242 - val_loss: 41.8032 - val_mae: 201.6994 - val_mape: 37.6445 - lr: 3.0000e-04\n",
      "Epoch 37/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 39.1113 - mae: 187.4253 - mape: 34.9523 - val_loss: 42.0376 - val_mae: 202.6541 - val_mape: 37.8808 - lr: 3.0000e-04\n",
      "Epoch 38/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 39.6958 - mae: 187.0155 - mape: 35.5423 - val_loss: 41.5399 - val_mae: 200.9530 - val_mape: 37.3871 - lr: 3.0000e-04\n",
      "Epoch 39/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 39.1130 - mae: 187.4403 - mape: 34.9613 - val_loss: 41.7859 - val_mae: 201.8974 - val_mape: 37.6343 - lr: 3.0000e-04\n",
      "Epoch 40/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 38.9624 - mae: 187.0052 - mape: 34.8132 - val_loss: 41.8908 - val_mae: 201.9903 - val_mape: 37.7440 - lr: 3.0000e-04\n",
      "Epoch 41/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 38.9818 - mae: 186.9296 - mape: 34.8335 - val_loss: 43.7141 - val_mae: 200.5280 - val_mape: 39.5612 - lr: 3.0000e-04\n",
      "Epoch 42/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 38.7541 - mae: 186.5672 - mape: 34.6055 - val_loss: 40.7634 - val_mae: 201.2784 - val_mape: 36.6180 - lr: 3.0000e-04\n",
      "Epoch 43/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 38.6362 - mae: 186.8630 - mape: 34.4905 - val_loss: 41.3437 - val_mae: 201.1087 - val_mape: 37.1966 - lr: 3.0000e-04\n",
      "Epoch 44/1000\n",
      "87/87 [==============================] - 7s 81ms/step - loss: 38.6702 - mae: 186.8829 - mape: 34.5211 - val_loss: 41.7581 - val_mae: 202.0336 - val_mape: 37.6113 - lr: 3.0000e-04\n",
      "Epoch 45/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 38.9317 - mae: 186.7279 - mape: 34.7840 - val_loss: 42.4421 - val_mae: 200.1541 - val_mape: 38.2938 - lr: 3.0000e-04\n",
      "Epoch 46/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 38.6949 - mae: 186.2321 - mape: 34.5493 - val_loss: 41.7794 - val_mae: 202.2612 - val_mape: 37.6339 - lr: 3.0000e-04\n",
      "Epoch 47/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 38.5730 - mae: 186.6395 - mape: 34.4266 - val_loss: 43.0736 - val_mae: 201.1866 - val_mape: 38.9271 - lr: 3.0000e-04\n",
      "Epoch 48/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 38.3536 - mae: 186.4752 - mape: 34.2069 - val_loss: 41.1003 - val_mae: 201.5263 - val_mape: 36.9530 - lr: 3.0000e-04\n",
      "Epoch 49/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 38.6926 - mae: 186.6687 - mape: 34.5449 - val_loss: 43.3881 - val_mae: 201.6997 - val_mape: 39.2409 - lr: 3.0000e-04\n",
      "Epoch 50/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 38.3369 - mae: 186.8204 - mape: 34.1907 - val_loss: 41.3344 - val_mae: 201.8020 - val_mape: 37.1883 - lr: 3.0000e-04\n",
      "Epoch 51/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 38.4022 - mae: 186.4315 - mape: 34.2576 - val_loss: 43.6986 - val_mae: 200.8979 - val_mape: 39.5510 - lr: 3.0000e-04\n",
      "Epoch 52/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 39.0190 - mae: 186.5441 - mape: 34.8709 - val_loss: 42.8211 - val_mae: 201.6241 - val_mape: 38.6734 - lr: 3.0000e-04\n",
      "Epoch 53/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 38.2790 - mae: 186.5843 - mape: 34.1305 - val_loss: 41.7474 - val_mae: 200.9238 - val_mape: 37.5955 - lr: 3.0000e-04\n",
      "Epoch 54/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 38.7690 - mae: 186.3150 - mape: 34.6158 - val_loss: 41.6793 - val_mae: 200.8830 - val_mape: 37.5266 - lr: 3.0000e-04\n",
      "Epoch 55/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 38.0997 - mae: 185.4543 - mape: 33.9472 - val_loss: 42.6691 - val_mae: 201.9812 - val_mape: 38.5222 - lr: 3.0000e-04\n",
      "Epoch 56/1000\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 38.2033 - mae: 186.0110 - mape: 34.0554 - val_loss: 41.7540 - val_mae: 203.1955 - val_mape: 37.6067 - lr: 3.0000e-04\n",
      "Epoch 57/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 37.9552 - mae: 185.7341 - mape: 33.8035 - val_loss: 43.4357 - val_mae: 200.8304 - val_mape: 39.2817 - lr: 3.0000e-04\n",
      "Epoch 58/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 38.6263 - mae: 186.7983 - mape: 34.4741 - val_loss: 41.7158 - val_mae: 204.0091 - val_mape: 37.5671 - lr: 3.0000e-04\n",
      "Epoch 59/1000\n",
      "87/87 [==============================] - 7s 79ms/step - loss: 38.6231 - mae: 186.0593 - mape: 34.4749 - val_loss: 43.0493 - val_mae: 201.5786 - val_mape: 38.9038 - lr: 3.0000e-04\n",
      "Epoch 60/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 38.4693 - mae: 185.5131 - mape: 34.3229 - val_loss: 41.5365 - val_mae: 200.7551 - val_mape: 37.3890 - lr: 3.0000e-04\n",
      "Epoch 61/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 38.9839 - mae: 187.0271 - mape: 34.8341 - val_loss: 43.4100 - val_mae: 202.4496 - val_mape: 39.2598 - lr: 3.0000e-04\n",
      "Epoch 62/1000\n",
      "87/87 [==============================] - 7s 80ms/step - loss: 38.2926 - mae: 184.4055 - mape: 34.1402 - val_loss: 43.2049 - val_mae: 201.4616 - val_mape: 39.0520 - lr: 3.0000e-04\n",
      "Epoch 63/1000\n",
      "87/87 [==============================] - ETA: 0s - loss: 37.9006 - mae: 184.8183 - mape: 33.7457\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "87/87 [==============================] - 7s 78ms/step - loss: 37.9006 - mae: 184.8183 - mape: 33.7457 - val_loss: 43.3919 - val_mae: 201.5869 - val_mape: 39.2374 - lr: 3.0000e-04\n",
      "Epoch 63: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robin\\.conda\\envs\\tfenvironment\\lib\\site-packages\\spektral\\data\\utils.py:221: UserWarning: you are shuffling a 'MyDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 5: loss of 38.16448211669922; mae of 203.0933837890625; mape of 34.14677429199219%;\n"
     ]
    }
   ],
   "source": [
    "#Hybrid Reset\n",
    "model = keras.models.load_model(\"savedmodels/GCN_simplified_normalized\")\n",
    "make_trainable(model)\n",
    "loss_per_fold, mae_per_fold, mape_per_fold, validation_per_fold = cross_validate(model, \"crossvalidationmodels/GCN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 40.95688247680664 - Mean average error: 172.03375244140625% - Mean percentage error: 36.897010803222656%\n",
      "    Score on unseen data: Loss: 46.48915100097656 - Mean average error: 365.64788818359375% - Mean percentage error: 42.42927932739258%\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 39.2916374206543 - Mean average error: 165.24569702148438% - Mean percentage error: 35.234371185302734%\n",
      "    Score on unseen data: Loss: 45.855106353759766 - Mean average error: 362.9642333984375% - Mean percentage error: 41.79783248901367%\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 40.337196350097656 - Mean average error: 213.3764190673828% - Mean percentage error: 36.30261993408203%\n",
      "    Score on unseen data: Loss: 46.665096282958984 - Mean average error: 367.31878662109375% - Mean percentage error: 42.63051223754883%\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 40.91968536376953 - Mean average error: 215.40164184570312% - Mean percentage error: 36.90154266357422%\n",
      "    Score on unseen data: Loss: 44.6888313293457 - Mean average error: 360.21136474609375% - Mean percentage error: 40.67068862915039%\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 38.16448211669922 - Mean average error: 203.0933837890625% - Mean percentage error: 34.14677429199219%\n",
      "    Score on unseen data: Loss: 46.16476821899414 - Mean average error: 361.0705261230469% - Mean percentage error: 42.147056579589844%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Loss: 39.93397674560547\n",
      "> Mean average error: 193.8301788330078\n",
      "> Mean percentage error: 35.896463775634764\n",
      "> Unseen Loss: 45.972590637207034\n",
      "> Unseen Mean average error: 363.44255981445315\n",
      "> Unseen Mean percentage error: 41.93507385253906\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_scores(loss_per_fold, mae_per_fold, mape_per_fold, validation_per_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot mape\n",
    "plt.plot(model.history.history[\"mape\"], label='Train')\n",
    "plt.plot(model.history.history[\"val_mape\"], label='Validation')\n",
    "plt.legend()\n",
    "plt.title('model mean absolute percentage error')\n",
    "plt.ylabel('mape')\n",
    "plt.xlabel('epoch')\n",
    "#plt.ylim([35, 150])\n",
    "#plt.xlim([0, 250])\n",
    "plt.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d217d37ed9247659b2812203162b84cdb779e33cccd1fe199abf14cba0e180e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
